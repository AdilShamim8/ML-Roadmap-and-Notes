{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373c9f42-7661-4741-a621-59a82cb5828f",
   "metadata": {},
   "source": [
    "# Scikit-Learn Pipelines [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2013%20Sklearn%20Pipelines)\n",
    "\n",
    "Scikit-Learn Pipelines are a powerful tool designed to streamline and standardize your machine learning workflow by chaining together preprocessing steps, feature engineering, and modeling. This ensures that every transformation is applied consistently during training and when making predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "A **Pipeline** in scikit-learn encapsulates a sequence of data processing steps into one single object. Each step (except the final estimator) must implement a `transform` method, and the final step must implement a `fit` method (or both `fit` and `predict`). This modular approach brings several advantages:\n",
    "\n",
    "- **Reproducibility:** Guarantees that the exact sequence of transformations is applied both during training and inference.\n",
    "- **Simplified Code:** Bundles complex workflows into a single, manageable object.\n",
    "- **Hyperparameter Tuning:** Facilitates end-to-end model selection using tools like `GridSearchCV` across all steps in the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "A pipeline can be thought of as a composition of functions. Suppose you have a series of transformations `T1,T2,,,,Tn` followed by an estimator `f`. Then the overall pipeline `P` can be expressed as:\n",
    "\n",
    "$$\n",
    "P(x) = f\\Big(T_n\\big(T_{n-1}(\\dots T_1(x)\\dots)\\big)\\Big)\n",
    "$$\n",
    "\n",
    "For instance, if `T1` is a standard scaling operation, it transforms a feature `x` as:\n",
    "\n",
    "$$\n",
    "x_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Here, μ is the mean and σ is the standard deviation of the feature. The transformed data then flows through subsequent steps until the final estimator produces the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## How Pipelines Work\n",
    "\n",
    "1. **Sequential Processing:**  \n",
    "   Each step in the pipeline processes the output of the previous step. During training, every transformer’s `fit` method is called sequentially to learn from the data, and during prediction, only the `transform` methods are applied in sequence.\n",
    "\n",
    "2. **Integration with Model Selection:**  \n",
    "   When using techniques like cross-validation or grid search, pipelines ensure that each fold of the data undergoes the exact same sequence of transformations, avoiding data leakage and ensuring valid model evaluation.\n",
    "\n",
    "3. **Modularity:**  \n",
    "   Pipelines allow you to easily swap or update individual steps without rewriting the entire workflow. This modularity supports experimentation and rapid prototyping.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Code Example\n",
    "\n",
    "Below is a Python code snippet demonstrating how to create a pipeline that includes data scaling, dimensionality reduction, and a classifier:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load example data\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define the pipeline steps\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),      # Step 1: Standardize features\n",
    "    ('pca', PCA(n_components=2)),        # Step 2: Reduce dimensionality\n",
    "    ('classifier', LogisticRegression()) # Step 3: Apply logistic regression\n",
    "])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(\"Model accuracy:\", score)\n",
    "\n",
    "# Optionally, use GridSearchCV to tune hyperparameters for all steps in the pipeline\n",
    "param_grid = {\n",
    "    'pca__n_components': [2, 3],\n",
    "    'classifier__C': [0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "- **StandardScaler:**  \n",
    "  Scales each feature to have zero mean and unit variance using the formula:  \n",
    "  $$ x_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "- **PCA (Principal Component Analysis):**  \n",
    "  Reduces the feature space while preserving as much variance as possible.\n",
    "\n",
    "- **LogisticRegression:**  \n",
    "  Serves as the final estimator in the pipeline to perform classification.\n",
    "\n",
    "- **GridSearchCV:**  \n",
    "  Tunes hyperparameters for both the PCA step and the logistic regression classifier simultaneously within the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits and Best Practices\n",
    "\n",
    "- **Consistency:**  \n",
    "  Using a pipeline ensures that the exact same transformations are applied during both training and prediction phases.\n",
    "\n",
    "- **Simplification:**  \n",
    "  Pipelines reduce the amount of boilerplate code, making your machine learning workflow cleaner and easier to manage.\n",
    "\n",
    "- **Error Reduction:**  \n",
    "  With all steps bundled, there's less risk of applying transformations out of order or forgetting to transform new data before making predictions.\n",
    "\n",
    "- **Scalability:**  \n",
    "  Pipelines seamlessly integrate with scikit-learn’s model selection and cross-validation tools, allowing for scalable and robust model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Scikit-Learn Pipelines are a foundational tool for building robust, maintainable, and reproducible machine learning workflows. By chaining preprocessing steps, feature engineering, and modeling into a single object, they provide a clear structure that minimizes errors and simplifies hyperparameter tuning. Experiment with different pipeline configurations to best suit your data processing and modeling needs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
