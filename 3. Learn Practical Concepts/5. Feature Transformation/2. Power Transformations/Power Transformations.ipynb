{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9743dbff-0519-4ee9-8401-b0c6e8ad7234",
   "metadata": {},
   "source": [
    "# Power Transformations in Feature Engineering | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2015%20Power%20Transformer)\n",
    "\n",
    "Power transformations help make data more Gaussian-like by stabilizing variance and reducing skewness. Two popular methods are the Box‑Cox transform and the Yeo‑Johnson transform.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Box‑Cox Transformation\n",
    "\n",
    "### Formula\n",
    "\n",
    "<p>For a feature <i>x</i> (where <i>x &gt; 0</i>), the Box-Cox transformation is defined as:</p>\n",
    "\n",
    "$$  \n",
    "x^{(\\lambda)} =   \n",
    "\\begin{cases}  \n",
    "\\frac{x^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\\n",
    "\\log(x) & \\text{if } \\lambda = 0.  \n",
    "\\end{cases}  \n",
    "$$  \n",
    "\n",
    "<ul>  \n",
    "    <li><strong>x</strong>: Original feature value (must be positive)</li>  \n",
    "    <li><strong>λ</strong>: Transformation parameter (learned from data)</li>  \n",
    "    <li><strong>x<sup>(λ)</sup></strong>: Transformed feature value</li>  \n",
    "</ul>  \n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Use Box‑Cox when all feature values are strictly positive.\n",
    "- It is especially useful when data exhibits strong right skewness.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Yeo‑Johnson Transformation\n",
    "\n",
    "### Formula\n",
    "\n",
    "The Yeo-Johnson transform extends the Box-Cox method to handle both positive and negative values:  \n",
    "\n",
    "For \\( x > 0 \\):  \n",
    "\n",
    "$$  \n",
    "x^{(\\lambda)} =   \n",
    "\\begin{cases}  \n",
    "\\frac{(x+1)^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\\n",
    "\\log(x+1) & \\text{if } \\lambda = 0,  \n",
    "\\end{cases}  \n",
    "$$  \n",
    "\n",
    "For \\( x < 0 \\):  \n",
    "\n",
    "$$  \n",
    "x^{(\\lambda)} =   \n",
    "\\begin{cases}  \n",
    "-\\frac{(-x+1)^{2-\\lambda} - 1}{2-\\lambda} & \\text{if } \\lambda \\neq 2, \\\\\n",
    "-\\log(-x+1) & \\text{if } \\lambda = 2.  \n",
    "\\end{cases}  \n",
    "$$  \n",
    "\n",
    "<ul>  \n",
    "    <li><strong>x</strong>: Original feature value (can be negative or positive)</li>  \n",
    "    <li><strong>λ</strong>: Transformation parameter (learned from data)</li>  \n",
    "    <li><strong>x<sup>(λ)</sup></strong>: Transformed feature value</li>  \n",
    "</ul>  \n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Use Yeo‑Johnson when your feature contains zero or negative values.\n",
    "- It provides similar variance stabilization benefits as Box‑Cox while accommodating a broader range of data.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Python Code Examples\n",
    "\n",
    "Below are Python code snippets using scikit‑learn’s `PowerTransformer` to apply both Box‑Cox and Yeo‑Johnson transformations.\n",
    "\n",
    "### 3.1 Box‑Cox Transformation Example\n",
    "\n",
    "> **Note:** Box‑Cox requires all values to be strictly positive.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample dataset (all values must be > 0)\n",
    "data_boxcox = pd.DataFrame({\n",
    "    'Feature': [1, 2, 3, 10, 20, 30, 50, 100]\n",
    "})\n",
    "\n",
    "# Initialize PowerTransformer with method='box-cox'\n",
    "pt_boxcox = PowerTransformer(method='box-cox', standardize=True)\n",
    "\n",
    "# Apply the Box-Cox transformation\n",
    "data_boxcox['BoxCox_Transformed'] = pt_boxcox.fit_transform(data_boxcox[['Feature']])\n",
    "\n",
    "print(\"Box-Cox Transformation:\")\n",
    "print(data_boxcox)\n",
    "\n",
    "# Plot original vs. Box-Cox transformed data\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(data_boxcox['Feature'], marker='o')\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(data_boxcox['BoxCox_Transformed'], marker='o', color='green')\n",
    "plt.title(\"Box-Cox Transformed Data\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Transformed Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Yeo‑Johnson Transformation Example\n",
    "\n",
    "> **Note:** Yeo‑Johnson can handle both positive and negative values.\n",
    "\n",
    "```python\n",
    "# Create a sample dataset with negative and positive values\n",
    "data_yeojohnson = pd.DataFrame({\n",
    "    'Feature': [-10, -5, -1, 0, 1, 5, 10, 20, 50]\n",
    "})\n",
    "\n",
    "# Initialize PowerTransformer with method='yeo-johnson'\n",
    "pt_yeojohnson = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "# Apply the Yeo-Johnson transformation\n",
    "data_yeojohnson['YeoJohnson_Transformed'] = pt_yeojohnson.fit_transform(data_yeojohnson[['Feature']])\n",
    "\n",
    "print(\"\\nYeo-Johnson Transformation:\")\n",
    "print(data_yeojohnson)\n",
    "\n",
    "# Plot original vs. Yeo-Johnson transformed data\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(data_yeojohnson['Feature'], marker='o')\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(data_yeojohnson['YeoJohnson_Transformed'], marker='o', color='orange')\n",
    "plt.title(\"Yeo-Johnson Transformed Data\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Transformed Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Power transformations—specifically the Box‑Cox and Yeo‑Johnson transforms—are powerful tools to normalize the distribution of your data and stabilize variance. Use Box‑Cox when your data is strictly positive and Yeo‑Johnson when your data includes zero or negative values. The scikit‑learn `PowerTransformer` makes it simple to integrate these transformations into your preprocessing pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
