{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865b9a02-0538-47be-ab8d-28c363823459",
   "metadata": {},
   "source": [
    "# Overview of Feature Selection\n",
    "\n",
    "**Feature Selection** is the process of identifying and selecting a subset of relevant features (or predictors) for use in model construction. Its benefits include:\n",
    "\n",
    "- **Improved Model Performance:** Reducing overfitting and improving generalization.\n",
    "- **Reduced Training Time:** Fewer features mean less complexity.\n",
    "- **Enhanced Interpretability:** Models become easier to understand and explain.\n",
    "\n",
    "Feature selection techniques generally fall into three categories:\n",
    "\n",
    "- **Filter Methods:** Evaluate features based on statistical measures.\n",
    "- **Wrapper Methods:** Use a predictive model to score feature subsets.\n",
    "- **Embedded Methods:** Perform feature selection as part of the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48bac9c-62ac-469d-9320-25a92706f68b",
   "metadata": {},
   "source": [
    " > # 1. Feature selection using SelectKBest and Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e0ada-33d0-4bb8-a0ad-8a589d58ebed",
   "metadata": {},
   "source": [
    "## II. SelectKBest\n",
    "\n",
    "### Overview\n",
    "**SelectKBest** is a filter method that selects the top \\( k \\) features based on a scoring function. It evaluates each feature independently and ranks them according to statistical tests. The most common scoring functions include:\n",
    "- **Chi-Square X^2** for classification tasks with non-negative features.\n",
    "- **ANOVA F-value** for regression or classification tasks.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Chi-Square Statistic\n",
    "\n",
    "<p>  \n",
    "  For a feature <strong>X</strong> and class <strong>Y</strong>, the chi-square statistic is computed as:  \n",
    "</p>  \n",
    "\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i} \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "<p>  \n",
    "  where:  \n",
    "  <ul>  \n",
    "    <li><strong>O<sub>i</sub></strong> is the observed frequency for category <strong>i</strong>.</li>  \n",
    "    <li><strong>E<sub>i</sub></strong> is the expected frequency under the null hypothesis (i.e., assuming no association between <strong>X</strong> and <strong>Y</strong>).</li>  \n",
    "  </ul>  \n",
    "</p>  \n",
    "\n",
    "#### ANOVA F-value\n",
    "For continuous features, the F-statistic measures the ratio of variance between groups to the variance within groups:\n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{variance between groups}}{\\text{variance within groups}}\n",
    "$$\n",
    "\n",
    "A higher F-value indicates a more significant difference among group means, suggesting that the feature is relevant.\n",
    "\n",
    "### Python Code Example\n",
    "\n",
    "Here’s how you can use **SelectKBest** with a chi-square test on a sample dataset:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample dataset (e.g., Iris)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# For chi-square, ensure features are non-negative.\n",
    "# Select the top 2 features based on the chi-square statistic\n",
    "chi2_selector = SelectKBest(score_func=chi2, k=2)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected feature indices (SelectKBest):\", chi2_selector.get_support(indices=True))\n",
    "print(\"Shape after selection:\", X_kbest.shape)\n",
    "```\n",
    "\n",
    "*Note:* When using other scoring functions (like ANOVA F-value), you can substitute `chi2` with `f_classif` (for classification) or `f_regression` (for regression).\n",
    "\n",
    "---\n",
    "\n",
    "## II . Recursive Feature Elimination (RFE)\n",
    "\n",
    "### Overview\n",
    "**Recursive Feature Elimination (RFE)** is a wrapper method that recursively removes the least important features based on the performance of an estimator (e.g., Logistic Regression, SVM). The process involves:\n",
    "1. Training a model on the current set of features.\n",
    "2. Ranking features by their importance (e.g., absolute coefficient values in linear models).\n",
    "3. Removing the least important feature(s).\n",
    "4. Repeating the process until the desired number of features is reached.\n",
    "\n",
    "### Mathematical Insight\n",
    "<p>  \n",
    "    Assume you have a linear model that provides coefficients   \n",
    "    <span style=\"font-weight: bold;\">w</span> = [<span style=\"font-weight: bold;\">w<sub>1</sub></span>,   \n",
    "    <span style=\"font-weight: bold;\">w<sub>2</sub></span>, ...,   \n",
    "    <span style=\"font-weight: bold;\">w<sub>d</sub></span>] for <span style=\"font-weight: bold;\">d</span> features.   \n",
    "    The importance of a feature can be measured by the magnitude <span style=\"font-weight: bold;\">|w<sub>i</sub>|</span>.   \n",
    "    At each step, RFE (Recursive Feature Elimination) eliminates the feature with the smallest   \n",
    "    <span style=\"font-weight: bold;\">|w<sub>i</sub>|</span> (or a set of features) and retrains the model on the remaining subset.  \n",
    "</p>  \n",
    "### Python Code Example\n",
    "\n",
    "Below is an example using RFE with Logistic Regression on the Iris dataset:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Initialize a Logistic Regression estimator (ensure enough iterations for convergence)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize RFE to select the top 2 features\n",
    "rfe_selector = RFE(estimator=model, n_features_to_select=2)\n",
    "X_rfe = rfe_selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected feature indices (RFE):\", rfe_selector.get_support(indices=True))\n",
    "print(\"Shape after selection:\", X_rfe.shape)\n",
    "```\n",
    "\n",
    "### How RFE Works Internally\n",
    "1. **Train the Model:** Initially, the model is trained using all features.\n",
    "2. **Rank Features:** The estimator’s coefficients (or feature importances) are used to rank features.\n",
    "3. **Eliminate Features:** The least important feature(s) are removed.\n",
    "4. **Iterate:** The process is repeated until the predefined number of features is achieved.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **SelectKBest** ranks features individually using statistical tests (like chi-square or ANOVA F-test). It is fast and computationally inexpensive but does not account for feature interactions.\n",
    "- **RFE** uses a model-based approach to eliminate features recursively. It often results in a better-performing feature subset but is computationally more intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673a994-5cb7-48ce-bc45-9ef152184fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5214840-0c64-492c-ac6c-1d6f26cb3a06",
   "metadata": {},
   "source": [
    "># 2. Chi-squared Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a4954-a4a3-4705-9f64-a0f75dc865fb",
   "metadata": {},
   "source": [
    "## Chi-Squared Feature Selection\n",
    "\n",
    "### Overview\n",
    "\n",
    "**Chi-squared feature selection** is a filter method that evaluates the independence between each feature and the target variable. It is commonly used in classification tasks where the features are categorical or non-negative (e.g., frequency counts). The core idea is to identify features that are statistically dependent on the target class.\n",
    "\n",
    "### The Chi-Squared Test\n",
    "\n",
    "The Chi-squared X^2 test measures how expectations compare to actual observed data. In the context of feature selection, it tests the null hypothesis that a feature and the target are independent.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For each feature, the Chi-squared statistic is computed as:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "<p>  \n",
    "  Where:  \n",
    "  <ul>  \n",
    "    <li><strong>O<sub>i</sub></strong> is the observed frequency for the <em>i<sup>th</sup></em> category.</li>  \n",
    "    <li><strong>E<sub>i</sub></strong> is the expected frequency under the null hypothesis.</li>  \n",
    "    <li><strong>k</strong> is the number of categories (or bins) for the feature.</li>  \n",
    "  </ul>  \n",
    "</p>  \n",
    "\n",
    "A higher X^2 value indicates a larger discrepancy between observed and expected frequencies, suggesting that the feature is more likely to be related to the target variable.\n",
    "\n",
    "### Using Chi-Squared for Feature Selection\n",
    "\n",
    "The process involves:\n",
    "1. **Binning Data (if necessary):** For continuous features, you may need to discretize them into bins since the chi-squared test is defined for categorical data.\n",
    "2. **Computing the Statistic:** Calculate the \\(\\chi^2\\) statistic for each feature.\n",
    "3. **Ranking Features:** Rank the features by their \\(\\chi^2\\) scores.\n",
    "4. **Selecting Top \\( k \\):** Choose the top \\( k \\) features with the highest scores.\n",
    "\n",
    "### Python Code Example with SelectKBest\n",
    "\n",
    "The following example demonstrates how to use the `SelectKBest` class from scikit-learn along with the `chi2` function to select the best features on the Iris dataset.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# For the chi-squared test, ensure that the feature values are non-negative.\n",
    "# Iris dataset features are positive, so it is suitable for chi-squared feature selection.\n",
    "\n",
    "# Initialize SelectKBest with chi2 as the scoring function and select top 2 features\n",
    "chi2_selector = SelectKBest(score_func=chi2, k=2)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "# Get indices of selected features\n",
    "selected_indices = chi2_selector.get_support(indices=True)\n",
    "\n",
    "print(\"Selected feature indices (Chi-squared):\", selected_indices)\n",
    "print(\"Shape of feature matrix after selection:\", X_kbest.shape)\n",
    "```\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- **Data Requirements:** The chi-squared test requires non-negative features. If your data includes negative values or is continuous, consider discretizing it before applying the test.\n",
    "- **Interpretability:** Higher X^2 values suggest a stronger association between a feature and the target variable. However, it does not capture interactions between features.\n",
    "- **Applicability:** Best suited for text data (e.g., word counts in document classification) or other scenarios where features represent frequencies or counts.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Chi-squared feature selection is a powerful and efficient method to evaluate the importance of individual features based on their statistical relationship with the target variable. By calculating and ranking X^2 scores, you can identify the features most likely to improve the performance of your classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225f9e7-06cf-46a0-b294-cd2ee1f44706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7697de34-aab8-40ca-be0d-c62aa876b868",
   "metadata": {},
   "source": [
    "># 3. Backward Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842cd03-404b-4c6b-b48d-53b4a9b86c09",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Backward Feature Elimination** is a wrapper-based feature selection method. It starts with all available features and iteratively removes the least significant feature—one that contributes the least to the model’s predictive power—until a stopping criterion is met (often when all remaining features are statistically significant).\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical and Algorithmic Insights\n",
    "\n",
    "### A. Statistical Significance & p-values\n",
    "\n",
    "When using regression models (e.g., Ordinary Least Squares), each feature is assigned a p-value that indicates the probability that its coefficient is zero (i.e., that the feature has no predictive power). In backward elimination, you typically set a significance level (e.g., α = 0.05):\n",
    "- **If a feature's p-value > α :** It is considered statistically insignificant and is removed.\n",
    "- **If all features have p-values <= α:** The algorithm stops.\n",
    "\n",
    "### B. Algorithm Steps\n",
    "\n",
    "1. **Fit the Full Model:** Use all features to fit your model.\n",
    "2. **Evaluate p-values:** For each feature, compute the p-value.\n",
    "3. **Remove the Least Significant Feature:** Identify the feature with the highest p-value (if above the significance level) and remove it.\n",
    "4. **Repeat:** Re-fit the model with the remaining features and repeat the elimination process until every feature has a p-value below the significance threshold.\n",
    "\n",
    "<p>  \n",
    "  Mathematically, if you have features   \n",
    "  <strong>X = {x<sub>1</sub>, x<sub>2</sub>, &hellip;, x<sub>d</sub>}</strong>   \n",
    "  and you fit a regression model:  \n",
    "</p>  \n",
    "<p>  \n",
    "  <strong>y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x<sub>1</sub> + &hellip; + &beta;<sub>d</sub>x<sub>d</sub> + &epsilon;</strong>  \n",
    "</p>  \n",
    "<p>  \n",
    "  then at each step, the feature <strong>x<sub>j</sub></strong> with the largest <strong>p<sub>j</sub></strong> (if <strong>p<sub>j</sub> > &alpha;</strong>) is removed from the model.  \n",
    "</p>  \n",
    "\n",
    "---\n",
    "\n",
    "##  Python Code Example\n",
    "\n",
    "Below is an example using the Boston Housing dataset (or any regression dataset) with the `statsmodels` package. This script demonstrates how to perform backward elimination based on p-values.\n",
    "\n",
    "> **Note:** Although the Boston dataset has been deprecated in newer versions of scikit-learn, it is still useful for illustration. You can substitute it with another dataset if needed.\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load dataset\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    # Start with all features\n",
    "    features = list(X.columns)\n",
    "    \n",
    "    while len(features) > 0:\n",
    "        X_opt = X[features]\n",
    "        # Fit the model using OLS regression\n",
    "        model = sm.OLS(y, X_opt).fit()\n",
    "        # Get the p-values for all features except the constant\n",
    "        p_values = model.pvalues.iloc[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        \n",
    "        # Check if the highest p-value exceeds the significance level\n",
    "        if max_p_value > significance_level:\n",
    "            # Identify the feature with the highest p-value\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "            print(f\"Removed {excluded_feature} with p-value {max_p_value:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return features, model\n",
    "\n",
    "# Perform backward elimination\n",
    "selected_features, final_model = backward_elimination(X, y, significance_level=0.05)\n",
    "\n",
    "print(\"\\nSelected features after backward elimination:\")\n",
    "print(selected_features)\n",
    "print(\"\\nFinal Model Summary:\")\n",
    "print(final_model.summary())\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "- **Data Preparation:**  \n",
    "  The dataset is loaded and converted into a DataFrame. A constant is added to model the intercept.\n",
    "  \n",
    "- **Backward Elimination Function:**  \n",
    "  The function `backward_elimination` repeatedly fits an Ordinary Least Squares (OLS) model, inspects p-values, and removes the feature with the highest p-value above the significance threshold.\n",
    "  \n",
    "- **Output:**  \n",
    "  The final set of features and model summary are printed, showing which predictors remained significant.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Backward Feature Elimination** starts with all features and removes the least significant ones based on p-values from regression models.\n",
    "- It is especially popular in regression problems where statistical significance is crucial.\n",
    "- The process continues iteratively until all features in the model are statistically significant at a predefined level (e.g., α = 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc98075-3486-4f4a-b7b7-621cf20cc628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4644b21a-8965-4532-9721-101d33438dae",
   "metadata": {},
   "source": [
    "> # 4. Dropping features using Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ea506-33c1-40f2-ac4b-131857051d12",
   "metadata": {},
   "source": [
    "##  Conceptual Overview\n",
    "\n",
    "The **Pearson correlation coefficient** measures the linear relationship between two continuous variables. Its value ranges from -1 to 1:\n",
    "- **1** indicates a perfect positive linear relationship.\n",
    "- **-1** indicates a perfect negative linear relationship.\n",
    "- **0** indicates no linear relationship.\n",
    "\n",
    "When features in your dataset are highly correlated with each other (i.e., they have a correlation coefficient above a set threshold, such as 0.9), they may be redundant. Dropping one feature from each highly correlated pair can help reduce multicollinearity, simplify your model, and potentially improve its interpretability and performance.\n",
    "\n",
    "\n",
    "##  Mathematical Formulation\n",
    "\n",
    "<p>  \n",
    "  The Pearson correlation coefficient <strong>r</strong> between two features <strong>X</strong> and <strong>Y</strong> is given by:  \n",
    "</p>  \n",
    "<p>  \n",
    "  <strong>r = \\(\\frac{\\operatorname{cov}(X, Y)}{\\sigma_X \\sigma_Y}\\)</strong>  \n",
    "</p>  \n",
    "<p>  \n",
    "  where:  \n",
    "  <ul>  \n",
    "    <li><strong>\\(\\operatorname{cov}(X, Y)\\)</strong> is the covariance between <strong>X</strong> and <strong>Y</strong>,</li>  \n",
    "    <li><strong>\\(\\sigma_X\\)</strong> and <strong>\\(\\sigma_Y\\)</strong> are the standard deviations of <strong>X</strong> and <strong>Y</strong> respectively.</li>  \n",
    "  </ul>  \n",
    "</p>  \n",
    "<p>  \n",
    "  A high absolute value of <strong>r</strong> (e.g., <strong>|r| > 0.9</strong>) suggests that the features are highly linearly correlated.  \n",
    "</p>  \n",
    "\n",
    "##  Python Code Example\n",
    "\n",
    "Below is a Python code snippet that demonstrates how to drop features based on a high Pearson correlation coefficient using a pandas DataFrame:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrame (df) with several features\n",
    "# For demonstration, we assume df is already loaded\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# Compute the correlation matrix (absolute values)\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Create an upper triangle matrix of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Define a threshold for correlation\n",
    "threshold = 0.9\n",
    "\n",
    "# Identify columns that have a correlation greater than the threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print(\"Features to drop due to high correlation:\")\n",
    "print(to_drop)\n",
    "\n",
    "# Drop the identified features from the DataFrame\n",
    "df_reduced = df.drop(columns=to_drop)\n",
    "\n",
    "print(\"Shape of original DataFrame:\", df.shape)\n",
    "print(\"Shape after dropping highly correlated features:\", df_reduced.shape)\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "- **Correlation Matrix:**  \n",
    "  The code calculates the absolute correlation matrix for the dataset.\n",
    "\n",
    "- **Upper Triangle Matrix:**  \n",
    "  We create an upper triangle matrix to avoid duplicating the correlation information (since the correlation matrix is symmetric).\n",
    "\n",
    "- **Thresholding:**  \n",
    "  By setting a threshold (e.g., 0.9), the code identifies which features are highly correlated with another feature.\n",
    "\n",
    "- **Feature Dropping:**  \n",
    "  Features meeting the criterion are dropped from the DataFrame, reducing redundancy.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Considerations\n",
    "\n",
    "- **Threshold Choice:**  \n",
    "  The threshold value is subjective; common choices are 0.8 or 0.9. You may adjust it based on your dataset and specific use case.\n",
    "\n",
    "- **Domain Knowledge:**  \n",
    "  Sometimes, even if two features are highly correlated, domain knowledge might suggest retaining both. Use caution and context when dropping features.\n",
    "\n",
    "- **Impact on Model Performance:**  \n",
    "  Always evaluate the impact of dropping features on your model’s performance, as eliminating too many features might lead to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcfb60-c828-4743-85b2-40767f7b6b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f3b445-2e2d-44cd-8581-12e79912c77f",
   "metadata": {},
   "source": [
    "> # 5. Feature Importance using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b355c-58e0-41ad-bc49-c18483fd636b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Random Forests are ensemble methods that build multiple decision trees and aggregate their predictions. One of the great benefits of Random Forests is their ability to provide an estimate of feature importance, which tells you how useful each feature is in making predictions.\n",
    "\n",
    "There are two common approaches to determine feature importance in Random Forests:\n",
    "\n",
    "- **Mean Decrease in Impurity (MDI):**  \n",
    "  Measures how much each feature contributes to reducing impurity (e.g., Gini impurity for classification or variance for regression) across all trees.\n",
    "  \n",
    "- **Permutation Importance:**  \n",
    "  Evaluates the drop in model performance when the feature's values are randomly shuffled, thereby breaking the relationship between the feature and the target.\n",
    "\n",
    "In this explanation, we’ll focus on the **Mean Decrease in Impurity (MDI)** method, as it is computed during the training of the Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "##  Mathematical Insight\n",
    "\n",
    "### Mean Decrease in Impurity (MDI)\n",
    "\n",
    "For a given tree in the forest, suppose a feature f is used in one or more splits. At each split, the reduction in impurity is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta i = i(\\text{parent}) - \\left( \\frac{N_{\\text{left}}}{N_{\\text{parent}}} \\times i(\\text{left}) + \\frac{N_{\\text{right}}}{N_{\\text{parent}}} \\times i(\\text{right}) \\right)\n",
    "$$\n",
    "\n",
    "<p>  \n",
    "  where:  \n",
    "  <ul>  \n",
    "    <li><strong>i(.)</strong> is the impurity (e.g., Gini impurity for classification),</li>  \n",
    "    <li><strong>N<sub>parent</sub></strong> is the number of samples at the parent node,</li>  \n",
    "    <li><strong>N<sub>left</sub></strong> and <strong>N<sub>right</sub></strong> are the numbers of samples in the left and right child nodes, respectively.</li>  \n",
    "  </ul>  \n",
    "</p>  \n",
    "\n",
    "For each tree t, the importance of feature f is computed by summing the impurity decrease over all nodes where f is used:\n",
    "\n",
    "$$\n",
    "I_{f}^{(t)} = \\sum_{\\text{node } n \\text{ using } f} \\frac{N_n}{N_{\\text{total}}} \\Delta i_n\n",
    "$$\n",
    "\n",
    "The overall importance of feature f across the forest (with T trees) is then the average:\n",
    "\n",
    "$$\n",
    "I_{f} = \\frac{1}{T} \\sum_{t=1}^{T} I_{f}^{(t)}\n",
    "$$\n",
    "\n",
    "This measure gives you a relative importance score for each feature based on how effectively it splits the data to reduce impurity.\n",
    "\n",
    "---\n",
    "\n",
    "##  Python Code Example\n",
    "\n",
    "Below is a Python example using the `RandomForestClassifier` from `scikit-learn` to compute and display feature importances.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset for demonstration\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Extract feature importances (Mean Decrease in Impurity)\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances using Random Forest (Mean Decrease in Impurity):\")\n",
    "print(feature_importance_df)\n",
    "```\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "- **Data Preparation:**  \n",
    "  The Iris dataset is loaded and split into features \\( X \\) and target \\( y \\). The feature names are also stored for reference.\n",
    "\n",
    "- **Training the Model:**  \n",
    "  A `RandomForestClassifier` is instantiated with 100 trees. After fitting the model, the attribute `.feature_importances_` provides the MDI-based importance scores for each feature.\n",
    "\n",
    "- **Displaying Results:**  \n",
    "  A pandas DataFrame is created to neatly display the features and their corresponding importance scores in descending order.\n",
    "\n",
    "---\n",
    "\n",
    "##  Summary\n",
    "\n",
    "- **Mean Decrease in Impurity (MDI):**  \n",
    "  Evaluates feature importance by measuring the reduction in impurity brought by splits on each feature across all trees in the forest.\n",
    "\n",
    "- **Interpretation:**  \n",
    "  Higher importance values indicate features that are more useful in reducing uncertainty and making accurate predictions.\n",
    "\n",
    "- **Practical Use:**  \n",
    "  Use the computed importances to understand your model better, select a subset of relevant features, or gain insights into your data's underlying structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5968f37-b767-4403-a249-91c4ebd81cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efc80f14-3a28-486e-ad71-b44990352b8b",
   "metadata": {},
   "source": [
    " > # 6. Feature Selection Advise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf0335-56be-44a8-b7b0-9f1b7a29cc02",
   "metadata": {},
   "source": [
    "## I. Understand Your Data and Domain\n",
    "\n",
    "- **Know Your Domain:**  \n",
    "  Use domain expertise to guide which features might be most relevant. Sometimes, even if two features are highly correlated, one may be more interpretable or actionable.\n",
    "  \n",
    "- **Data Exploration:**  \n",
    "  Conduct exploratory data analysis (EDA) to understand distributions, missing values, and potential outliers. Visualizing relationships can reveal redundancy or unexpected patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## II. Use Multiple Feature Selection Methods\n",
    "\n",
    "- **Filter Methods:**  \n",
    "  Use statistical tests (e.g., chi-squared, ANOVA, correlation coefficients) to rank features independently. These are fast and can be applied as a preliminary screening step.\n",
    "  \n",
    "- **Wrapper Methods:**  \n",
    "  Techniques like Recursive Feature Elimination (RFE) evaluate subsets of features using a predictive model. They can capture interactions between features but may be computationally intensive.\n",
    "  \n",
    "- **Embedded Methods:**  \n",
    "  Algorithms such as Lasso (with L1 regularization) or tree-based methods (e.g., Random Forest feature importance) integrate feature selection during model training. These are efficient and can handle large feature spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## III. Consider Model Complexity and Overfitting\n",
    "\n",
    "- **Regularization:**  \n",
    "  Incorporate regularization techniques (e.g., Lasso, Ridge) to prevent overfitting while simultaneously reducing the number of features.\n",
    "  \n",
    "- **Avoid Multicollinearity:**  \n",
    "  Use correlation analysis to drop or combine features that are highly correlated. This can simplify the model and improve its generalizability.\n",
    "\n",
    "---\n",
    "\n",
    "## IV. Evaluate Feature Selection Impact\n",
    "\n",
    "- **Cross-Validation:**  \n",
    "  Always validate your feature selection process using cross-validation. Compare model performance (e.g., accuracy, F1-score, RMSE) with and without the selected features.\n",
    "  \n",
    "- **Iterative Process:**  \n",
    "  Feature selection is rarely a one-shot task. Revisit and adjust your approach as you iterate over model tuning and as new data becomes available.\n",
    "\n",
    "---\n",
    "\n",
    "## V. Balancing Trade-offs\n",
    "\n",
    "- **Interpretability vs. Performance:**  \n",
    "  Sometimes a simpler model with fewer features may be preferred even if it has a marginally lower performance. Consider the interpretability of your model, especially in regulated industries.\n",
    "  \n",
    "- **Computational Efficiency:**  \n",
    "  In large datasets, reducing the number of features can significantly decrease training time and improve model scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## VI. Practical Tips\n",
    "\n",
    "- **Visualization:**  \n",
    "  Use heatmaps to inspect feature correlations, boxplots to understand distributions, and feature importance plots to see which features drive your model.\n",
    "  \n",
    "- **Automated Tools:**  \n",
    "  Explore tools like scikit-learn’s `SelectKBest`, `RFE`, or even AutoML solutions that integrate feature selection pipelines.\n",
    "  \n",
    "- **Domain-Specific Transformations:**  \n",
    "  Sometimes, feature engineering (e.g., combining or transforming features) is as important as feature selection. Think about creating new features that better capture underlying phenomena.\n",
    "\n",
    "- **Stay Updated:**  \n",
    "  The field of machine learning evolves rapidly. Follow reputable blogs, research papers, or courses that share the latest advancements in feature selection methodologies.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Feature selection is a critical step in building robust and interpretable machine learning models. By combining domain knowledge with a mix of statistical, wrapper, and embedded methods, you can effectively narrow down your feature set, reduce overfitting, and enhance model performance. Remember, the goal is to create a model that not only performs well but also generalizes to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
