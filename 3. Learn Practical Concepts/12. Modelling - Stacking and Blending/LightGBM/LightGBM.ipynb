{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6eeb66-2ffb-4455-a080-69641e81a5a5",
   "metadata": {},
   "source": [
    "# LightGBM: A Powerful Gradient Boosting Framework\n",
    "\n",
    "LightGBM is an open-source gradient boosting framework developed by Microsoft. It is designed for high efficiency, speed, and scalability by leveraging innovative techniques such as histogram-based algorithms, gradient-based one-side sampling (GOSS), and exclusive feature bundling (EFB).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Speed and Efficiency:** Uses a histogram-based algorithm to bucket continuous feature values, reducing computation and memory usage.\n",
    "- **Scalability:** Handles large-scale datasets and high-dimensional features effectively.\n",
    "- **Accuracy:** Achieves high predictive performance by combining many weak learners.\n",
    "- **GOSS (Gradient-Based One-Side Sampling):** Prioritizes instances with larger gradients to focus on the most informative samples.\n",
    "- **EFB (Exclusive Feature Bundling):** Reduces the number of features by bundling mutually exclusive ones.\n",
    "\n",
    "## How LightGBM Works\n",
    "\n",
    "### Histogram-based Splitting\n",
    "- **Concept:** Instead of evaluating all unique feature values, LightGBM bins continuous values into discrete bins.\n",
    "- **Advantage:** Reduces computation time and memory usage.\n",
    "\n",
    "### Gradient-Based One-Side Sampling (GOSS)\n",
    "- **Idea:** Retain instances with large gradients (those contributing most to the error) and randomly sample from those with smaller gradients.\n",
    "- **Result:** Fewer instances to process while preserving the quality of the gradient information.\n",
    "\n",
    "### Exclusive Feature Bundling (EFB)\n",
    "- **Idea:** Combine features that rarely take nonzero values simultaneously.\n",
    "- **Result:** Reduce the effective number of features without significant loss of information.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "<!-- Regression problem with samples -->  \n",
    "<p>Consider a regression problem with <span style=\"font-family: 'Courier New', Courier, monospace;\">N</span> samples <span style=\"font-family: 'Courier New', Courier, monospace;\">{(x<sub>i</sub>, y<sub>i</sub>)}_{i=1}^N</span>.</p>  \n",
    "\n",
    "<!-- Objective function in gradient boosting -->  \n",
    "<p>The objective function in gradient boosting is:</p>  \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "<!-- Loss function -->  \n",
    "<p>Loss function: <span style=\"font-family: 'Courier New', Courier, monospace;\">l(y<sub>i</sub>, ŷ<sub>i</sub>)</span> (e.g., mean squared error).</p>  \n",
    "\n",
    "<!-- Predicted output -->  \n",
    "<p>Predicted output: <span style=\"font-family: 'Courier New', Courier, monospace;\">ŷ<sub>i</sub></span>.</p>  \n",
    "\n",
    "<!-- Regularization term -->  \n",
    "<p>Regularization term for the <span style=\"font-family: 'Courier New', Courier, monospace;\">k</span>-th tree <span style=\"font-family: 'Courier New', Courier, monospace;\">f<sub>k</sub></span>: <span style=\"font-family: 'Courier New', Courier, monospace;\">Ω(f<sub>k</sub>)</span>.</p>  \n",
    "\n",
    "<!-- Model prediction update -->  \n",
    "<p>At iteration <span style=\"font-family: 'Courier New', Courier, monospace;\">t</span>, the model prediction is updated as:</p>  \n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "LightGBM uses a second-order Taylor expansion to approximate the loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{N} \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "<!-- Gradient and Hessian -->  \n",
    "<p>First derivative: <span style=\"font-family: 'Courier New', Courier, monospace;\">g<sub>i</sub> = &nbsp;∂l(y<sub>i</sub>, ŷ<sub>i</sub><sup>(t-1)</sup>) / ∂ŷ<sub>i</sub><sup>(t-1)</sup></span> (first derivative)</p>  \n",
    "<p>Second derivative: <span style=\"font-family: 'Courier New', Courier, monospace;\">h<sub>i</sub> = &nbsp;∂<sup>2</sup>l(y<sub>i</sub>, ŷ<sub>i</sub><sup>(t-1)</sup>) / ∂(ŷ<sub>i</sub><sup>(t-1)</sup>)<sup>2</sup></span> (second derivative)</p>  \n",
    "\n",
    "<!-- Regression problem description -->  \n",
    "<p>Consider a regression problem with <span style=\"font-family: 'Courier New', Courier, monospace;\">N</span> samples <span style=\"font-family: 'Courier New', Courier, monospace;\">{(x<sub>i</sub>, y<sub>i</sub>)}<sub>i=1</sub><sup>N</sup></span>.</p>  \n",
    "<p>The objective function in gradient boosting is:</p>  \n",
    "\n",
    "## Python Example: LightGBM for Classification\n",
    "\n",
    "Below is an example that demonstrates training a LightGBM model on the Iris dataset using Python.\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters for a multiclass classification problem\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train the LightGBM model\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data], early_stopping_rounds=10)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred_labels = [list(probs).index(max(probs)) for probs in y_pred]\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "LightGBM offers a robust, efficient, and scalable solution for gradient boosting. Its innovative techniques like histogram-based splitting, GOSS, and EFB make it a top choice for handling large datasets and high-dimensional data. Whether for regression or classification, LightGBM is a valuable tool in the machine learning toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
