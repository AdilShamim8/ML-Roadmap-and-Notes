{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9447570f-a66e-4556-aff5-5ea0bd689afa",
   "metadata": {},
   "source": [
    "# Stacking in Machine Learning |[Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2044%20Stacking%20and%20Blending)\n",
    "\n",
    "Stacking (or stacked generalization) is an ensemble learning technique that combines multiple base models (level-0 models) to improve predictive performance by training a meta-model (level-1 model) on the outputs (predictions) of these base models.\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "- **Goal:** To leverage the strengths of different models to achieve better performance than any single model.\n",
    "- **Process:** Train several base learners on the training data and then use their predictions as features to train a meta-learner.\n",
    "- **Applications:** Used in both classification and regression problems.\n",
    "\n",
    "## 2. How It Works\n",
    "\n",
    "### Base Models (Level-0)\n",
    "\n",
    "Assume you have \\(K\\) base models:\n",
    "$$\n",
    "( f_1(x), f_2(x), dots, f_K(x) )\n",
    "$$\n",
    "For a given input \\( x \\), each base model produces a prediction:\n",
    "$$\n",
    "\\hat{y}_k = f_k(x) \\quad \\text{for } k = 1, 2, \\dots, K\n",
    "$$\n",
    "\n",
    "### Meta-Model (Level-1)\n",
    "\n",
    "The meta-model \\( g \\) takes the base models' predictions as input to produce the final prediction:\n",
    "$$\n",
    "\\hat{y} = g\\left(\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_K\\right)\n",
    "$$\n",
    "\n",
    "In practice, the training of the meta-model is often performed using cross-validation:\n",
    "1. **Split the training data:** Use \\( k \\)-fold cross-validation to obtain out-of-fold predictions for each base model.\n",
    "2. **Train meta-model:** Use these out-of-fold predictions as features (often called the “level-1” data) along with the true target values to train the meta-learner.\n",
    "\n",
    "## 3. Algorithm Steps\n",
    "\n",
    "1. **Divide the Data:** Split your dataset into training and validation sets.\n",
    "2. **Train Base Models:** For each base model $$( f_k ): $$\n",
    "   - Use cross-validation to generate predictions on the validation set.\n",
    "   - Collect these predictions to form a new dataset.\n",
    "3. **Form Meta-Features:** Construct a new feature matrix where each column corresponds to the predictions from a base model.\n",
    "4. **Train the Meta-Model:** Train a model \\( g \\) on the new dataset (meta-features) with the true target values.\n",
    "5. **Make Final Predictions:** For a new input \\( x \\):\n",
    "   - Obtain base predictions $$ ( \\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_K ). $$\n",
    "   - Pass these to the meta-model ( g \\) to obtain the final prediction \\( \\hat{y} \\).\n",
    "\n",
    "## 4. Python Implementation Example\n",
    "\n",
    "Below is an example using scikit-learn's `StackingClassifier` for a classification task on the Iris dataset.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base learners (level-0 models)\n",
    "base_learners = [\n",
    "    ('lr', LogisticRegression(max_iter=200)),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svc', SVC(probability=True))\n",
    "]\n",
    "\n",
    "# Define meta-learner (level-1 model)\n",
    "meta_learner = LogisticRegression()\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5  # number of folds for generating out-of-fold predictions\n",
    ")\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "## 5. Advantages and Limitations\n",
    "\n",
    "### Advantages\n",
    "- **Improved Performance:** Combines strengths of diverse models to often outperform single models.\n",
    "- **Flexibility:** Can incorporate a wide range of model types.\n",
    "- **Robustness:** Reduces the risk of overfitting by balancing the biases of individual models.\n",
    "\n",
    "### Limitations\n",
    "- **Complexity:** More computationally intensive due to multiple training phases.\n",
    "- **Interpretability:** Harder to interpret than a single model.\n",
    "- **Risk of Overfitting:** If not properly validated, the meta-model might overfit the base predictions.\n",
    "\n",
    "## 6. Variations and Extensions\n",
    "\n",
    "- **Blending:** A variant of stacking where a hold-out set is used instead of cross-validation.\n",
    "- **Multi-level Stacking:** Stacking can be extended to more than two levels, although this increases complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
