{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "625511f0-d88c-49ec-b5d9-4242864eb9cc",
   "metadata": {},
   "source": [
    "# Blending in Machine Learning |[Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2044%20Stacking%20and%20Blending)\n",
    "\n",
    "Blending is an ensemble technique that combines predictions from multiple base models by using a hold-out set to train a meta-model. Unlike stacking, which typically relies on cross-validation to generate out-of-fold predictions, blending uses a dedicated subset of the training data to create meta-features.\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "- **Objective:** Improve predictive performance by combining multiple models.\n",
    "- **Process:**  \n",
    "  1. Split the dataset into two parts: a training subset (T) and a hold-out (blending) set (H).\n",
    "  2. Train several base models on the training subset (T).\n",
    "  3. Use these trained models to predict on the hold-out set (H) to generate meta-features.\n",
    "  4. Train a meta-model using these meta-features and the corresponding true labels from H.\n",
    "  5. For new data, base models predict first, and their outputs are fed into the meta-model to produce the final prediction.\n",
    "\n",
    "## 2. Key Formulas\n",
    "\n",
    "Let ( f_1, f_2, \\dots, f_K ) be the base models. For a given input ( x ):\n",
    "- **Base Predictions:**  \n",
    "$$\n",
    "  \\hat{y}_k = f_k(x) \\quad \\text{for } k = 1, 2, \\dots, K\n",
    "$$\n",
    "- **Meta-Model Prediction:**  \n",
    "  The meta-model \\( g \\) is trained on the predictions from the hold-out set \\( H \\):\n",
    "$$\n",
    "  \\hat{y} = g\\left(\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_K\\right)\n",
    "$$\n",
    "  where $$ (\\hat{y}_k) $$ are the outputs obtained from the base models on \\(H\\).\n",
    "\n",
    "## 3. Blending Algorithm Steps\n",
    "\n",
    "1. **Data Split:**\n",
    "   - Divide your training data into a training set \\( T \\) and a hold-out set \\( H \\).\n",
    "2. **Train Base Models:**\n",
    "   - Train each base model \\( f_k \\) on the training set \\( T \\).\n",
    "3. **Generate Meta-Features:**\n",
    "   - For each model \\( f_k \\), predict the outcomes on the hold-out set \\( H \\) to create a new feature matrix:\n",
    "     $$\n",
    "     X_{\\text{meta}} = \\left[ f_1(X_H), f_2(X_H), \\dots, f_K(X_H) \\right]\n",
    "     $$\n",
    "4. **Train Meta-Model:**\n",
    "   - Use \\( X_{\\text{meta}} \\) along with the true labels \\( y_H \\) from \\( H \\) to train the meta-model \\( g \\).\n",
    "5. **Final Prediction:**\n",
    "   - For new data \\( x_{\\text{new}} \\), obtain base predictions \\( f_k(x_{\\text{new}}) \\) and feed them into \\( g \\) to get the final output:\n",
    "     $$\n",
    "     \\hat{y}_{\\text{new}} = g\\left( f_1(x_{\\text{new}}), \\dots, f_K(x_{\\text{new}}) \\right)\n",
    "     $$\n",
    "\n",
    "\n",
    "## 4. Python Example: Blending\n",
    "\n",
    "Below is a Python example that demonstrates blending using scikit-learn. In this example, we use the Iris dataset, train three base models, and then blend their predictions with a meta-model.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Step 1: Split the dataset\n",
    "# Split into main training+validation and test sets\n",
    "X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Further split the main set into training set (T) and hold-out blending set (H)\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X_main, y_main, test_size=0.25, random_state=42)\n",
    "\n",
    "# Step 2: Train Base Models on T\n",
    "model_lr = LogisticRegression(max_iter=200).fit(X_train, y_train)\n",
    "model_dt = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "model_svc = SVC(probability=True).fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Generate Meta-Features on H\n",
    "# Obtain probability predictions for each model on the hold-out set\n",
    "pred_lr = model_lr.predict_proba(X_holdout)\n",
    "pred_dt = model_dt.predict_proba(X_holdout)\n",
    "pred_svc = model_svc.predict_proba(X_holdout)\n",
    "\n",
    "# Concatenate predictions to form meta-features\n",
    "X_meta = np.hstack([pred_lr, pred_dt, pred_svc])\n",
    "\n",
    "# Step 4: Train Meta-Model on the hold-out set's meta-features\n",
    "meta_model = LogisticRegression(max_iter=200).fit(X_meta, y_holdout)\n",
    "\n",
    "# Step 5: Final Prediction on Test Set\n",
    "# Get base model predictions on test data\n",
    "test_pred_lr = model_lr.predict_proba(X_test)\n",
    "test_pred_dt = model_dt.predict_proba(X_test)\n",
    "test_pred_svc = model_svc.predict_proba(X_test)\n",
    "X_test_meta = np.hstack([test_pred_lr, test_pred_dt, test_pred_svc])\n",
    "\n",
    "# Meta-model produces the final prediction\n",
    "y_pred = meta_model.predict(X_test_meta)\n",
    "\n",
    "# Evaluate blending performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Blending Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "## 5. Advantages and Limitations of Blending\n",
    "\n",
    "### Advantages\n",
    "- **Simplicity:** Easier to implement since it only requires a single split.\n",
    "- **Speed:** Generally faster than stacking due to the absence of cross-validation iterations.\n",
    "\n",
    "### Limitations\n",
    "- **Hold-Out Dependency:** The performance can be sensitive to how the hold-out set is chosen.\n",
    "- **Potential Overfitting:** If the hold-out set is small or not representative, the meta-model may overfit.\n",
    "- **Less Robust:** Typically, blending might be less robust than stacking if the hold-out set does not capture the full data distribution.\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "Blending is a practical ensemble method that leverages a simple train-holdout split to combine the strengths of various base models. While it is easier and faster to implement than stacking, careful attention must be paid to the hold-out set to ensure robust performance. Experiment with blending on your datasets to see if it provides a performance boost for your specific problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
