{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3294f3-c98f-49f8-a9ee-9b148008521b",
   "metadata": {},
   "source": [
    "# CatBoost: An Overview\n",
    "\n",
    "CatBoost is an open-source gradient boosting library developed by Yandex. It is designed to work well with categorical features without extensive preprocessing and offers robust performance on both classification and regression tasks.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Native Handling of Categorical Features:**  \n",
    "  CatBoost automatically converts categorical features into numerical values using techniques like target statistics and Bayesian smoothing.\n",
    "  \n",
    "- **Ordered Boosting:**  \n",
    "  Uses permutation-driven algorithms (ordered boosting) to reduce target leakage and mitigate overfitting.\n",
    "  \n",
    "- **Oblivious Decision Trees:**  \n",
    "  CatBoost builds symmetric trees (also known as oblivious trees) where the same splitting rule is applied at each level of the tree, leading to efficient and interpretable models.\n",
    "  \n",
    "- **Robust Performance:**  \n",
    "  It performs competitively with other boosting frameworks (like XGBoost and LightGBM) while often requiring less parameter tuning.\n",
    "\n",
    "## How CatBoost Works\n",
    "\n",
    "CatBoost builds an ensemble of decision trees using gradient boosting. The key differences compared to other methods include:\n",
    "\n",
    "1. **Handling Categorical Data:**  \n",
    "   Categorical features are converted using efficient algorithms that compute target statistics and reduce the risk of overfitting.\n",
    "\n",
    "2. **Ordered Boosting:**  \n",
    "   Instead of using the same dataset for computing gradients (which may lead to target leakage), CatBoost uses a permutation scheme. For each training instance, the algorithm computes the gradient using only data that appears before that instance in the permutation order.\n",
    "\n",
    "3. **Symmetric (Oblivious) Trees:**  \n",
    "   Each level of the tree uses the same split, resulting in a balanced tree structure that speeds up predictions and simplifies the model.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "The objective function in CatBoost is similar to other gradient boosting methods:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{n} l(y_i, F(x_i)) + \\Omega(F)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "<!-- Loss function description -->  \n",
    "<p>Loss function: <span style=\"font-family: 'Courier New', Courier, monospace;\">l(y<sub>i</sub>, F(x<sub>i</sub>))</span> (e.g., log-loss for classification or mean squared error for regression).</p>  \n",
    "\n",
    "<!-- Ensemble prediction description -->  \n",
    "<p>Ensemble prediction: <span style=\"font-family: 'Courier New', Courier, monospace;\">F(x<sub>i</sub>)</span>.</p>  \n",
    "\n",
    "<!-- Regularization term description -->  \n",
    "<p>Regularization term: <span style=\"font-family: 'Courier New', Courier, monospace;\">Ω(F)</span>.</p>  \n",
    "\n",
    "At each boosting iteration, CatBoost minimizes the loss by adding a new tree \\( f_t \\):\n",
    "\n",
    "$$\n",
    "F^{(t)}(x_i) = F^{(t-1)}(x_i) + f_t(x_i)\n",
    "$$\n",
    "\n",
    "The algorithm employs a second-order Taylor expansion to approximate the loss and uses the ordered boosting technique to compute gradients and second derivatives while avoiding target leakage.\n",
    "\n",
    "## Python Example: CatBoostClassifier\n",
    "\n",
    "Below is a Python example using CatBoost on the Iris dataset.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "# If there are categorical features, you can specify their indices in cat_features parameter.\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=0  # set verbose=1 to see training progress\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "## Advantages and Limitations\n",
    "\n",
    "### Advantages\n",
    "- **Automatic Categorical Handling:**  \n",
    "  Minimizes the need for manual encoding.\n",
    "- **Ordered Boosting:**  \n",
    "  Helps reduce overfitting and target leakage.\n",
    "- **Efficient and Scalable:**  \n",
    "  Optimized for speed and performance even on large datasets.\n",
    "- **User-Friendly:**  \n",
    "  Often requires less tuning compared to other gradient boosting libraries.\n",
    "\n",
    "### Limitations\n",
    "- **Memory Usage:**  \n",
    "  For extremely large datasets, memory consumption might be a concern.\n",
    "- **Less Flexibility in Some Hyperparameters:**  \n",
    "  While it works well out-of-the-box, fine-tuning for very specific applications might be less flexible compared to some other frameworks.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "CatBoost is a powerful and user-friendly gradient boosting tool, especially well-suited for datasets with categorical features. Its unique approaches—like ordered boosting and symmetric trees—help improve performance while reducing overfitting. Whether you're tackling a classification or regression problem, CatBoost provides a robust solution with minimal preprocessing effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
