{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a49aa7e-d152-4f06-bdf4-2715cb0d302d",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal combination of hyperparameters—the external configuration settings of a model—that results in the best performance on a given task. Unlike model parameters, which are learned during training, hyperparameters must be set before training and can greatly affect model behavior.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Hyperparameters vs. Model Parameters\n",
    "\n",
    "- **Model Parameters**:  \n",
    "  These are learned from data during training. For example, the weights in a neural network or the coefficients in linear regression.\n",
    "\n",
    "- **Hyperparameters**:  \n",
    "  These are set manually or via an automated tuning process. They control aspects of the training process (e.g., learning rate, regularization strength, number of layers, etc.) and can influence the model’s ability to generalize.\n",
    "\n",
    "### Why Tune Hyperparameters?\n",
    "\n",
    "- **Performance**: Optimal hyperparameters can improve accuracy, precision, recall, and other performance metrics.\n",
    "- **Generalization**: Proper tuning helps prevent overfitting by finding a good balance between model complexity and data representation.\n",
    "- **Efficiency**: Better-tuned hyperparameters can also reduce training time or resource usage.\n",
    "\n",
    "## Common Tuning Methods\n",
    "\n",
    "Several approaches exist for hyperparameter tuning:\n",
    "\n",
    "### 1. Grid Search\n",
    "\n",
    "Grid Search (implemented in scikit‑learn’s `GridSearchCV`) exhaustively evaluates every combination from a predefined grid of hyperparameter values.\n",
    "\n",
    "- **Pros**:  \n",
    "  Guarantees evaluation of all specified combinations.\n",
    "- **Cons**:  \n",
    "  Can be computationally expensive if the grid is large.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "<p>For hyperparameters <span style=\"font-family: 'Courier New', Courier, monospace;\">p<sub>1</sub>, p<sub>2</sub>, ....., p<sub>k</sub></span> with candidate sets:</p>  \n",
    "$$\n",
    "P_1, P_2, \\dots, P_k\n",
    "$$\n",
    "The grid is:\n",
    "$$\n",
    "\\mathcal{P} = \\{ (p_1, p_2, \\dots, p_k) \\mid p_1 \\in P_1,\\, p_2 \\in P_2,\\, \\dots,\\, p_k \\in P_k \\}\n",
    "$$\n",
    "Each combination is evaluated using cross-validation:\n",
    "$$\n",
    "\\text{CV Score}(\\mathbf{p}) = \\frac{1}{K} \\sum_{i=1}^{K} \\text{score}_i(\\mathbf{p})\n",
    "$$\n",
    "and the best combination is chosen:\n",
    "$$\n",
    "\\mathbf{p}^* = \\arg\\max_{\\mathbf{p} \\in \\mathcal{P}} \\text{CV Score}(\\mathbf{p})\n",
    "$$\n",
    "\n",
    "### 2. Random Search\n",
    "\n",
    "Random Search (available as `RandomizedSearchCV` in scikit‑learn) randomly samples a fixed number of combinations from a defined hyperparameter space.\n",
    "\n",
    "- **Pros**:  \n",
    "  More efficient when the hyperparameter space is large, since it does not evaluate all possible combinations.\n",
    "- **Cons**:  \n",
    "  Does not guarantee that the best combination will be found.\n",
    "\n",
    "**Sampling Formulation:**\n",
    "\n",
    "<p>For each hyperparameter, define a distribution <span style=\"font-family: 'Courier New', Courier, monospace;\">D<sub>i</sub></span>. Randomly sample <span style=\"font-family: 'Courier New', Courier, monospace;\">N</span> combinations:</p>  \n",
    "$$\n",
    "\\{ (p_1^{(i)}, p_2^{(i)}, \\dots, p_k^{(i)}) \\}_{i=1}^{N}\n",
    "$$\n",
    "\n",
    "### 3. Bayesian Optimization\n",
    "\n",
    "Bayesian optimization builds a probabilistic model of the function mapping hyperparameters to the objective metric. It then selects the next set of hyperparameters to evaluate by optimizing an acquisition function.\n",
    "\n",
    "- **Pros**:  \n",
    "  Efficiently navigates large or continuous spaces and often finds good solutions with fewer evaluations.\n",
    "- **Cons**:  \n",
    "  More complex to implement and may require additional libraries (e.g., Hyperopt, BayesianOptimization).\n",
    "\n",
    "### 4. Other Methods\n",
    "\n",
    "- **Hyperband**: Uses adaptive resource allocation to focus on promising hyperparameter configurations.\n",
    "- **Evolutionary Algorithms**: Uses genetic algorithms to evolve the hyperparameter population over time.\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- **Start Simple**: Begin with a coarse grid or fewer random samples, then refine the search around promising areas.\n",
    "- **Use Cross-Validation**: Reliable evaluation through k-fold cross-validation reduces the risk of overfitting.\n",
    "- **Balance Computational Cost**: Adjust the number of iterations and range of values based on available resources.\n",
    "- **Monitor and Iterate**: Analyze the tuning results and adjust the search space if necessary.\n",
    "\n",
    "## Python Code Examples\n",
    "\n",
    "### GridSearchCV Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object with 5-fold cross-validation\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV Accuracy: {:.2f}%\".format(grid.best_score_ * 100))\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### RandomizedSearchCV Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define distributions for hyperparameters\n",
    "param_distributions = {\n",
    "    'C': expon(scale=100),      # Exponential distribution for C\n",
    "    'gamma': expon(scale=0.1),    # Exponential distribution for gamma\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVC(),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best CV Accuracy: {:.2f}%\".format(random_search.best_score_ * 100))\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Hyperparameter tuning is crucial for optimizing model performance. Techniques such as Grid Search and Random Search provide systematic ways to explore the hyperparameter space, while advanced methods like Bayesian optimization can further enhance efficiency when dealing with large or continuous spaces.\n",
    "\n",
    "By applying these methods with cross-validation, you can reliably select the hyperparameters that yield the best generalization performance on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "*References:*\n",
    "- [scikit‑learn documentation on GridSearchCV.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- [Analytics Vidhya’s guide on hyperparameter tuning.](https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/)\n",
    "- [Great Learning’s article on hyperparameter tuning.](https://www.mygreatlearning.com/blog/gridsearchcv/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
