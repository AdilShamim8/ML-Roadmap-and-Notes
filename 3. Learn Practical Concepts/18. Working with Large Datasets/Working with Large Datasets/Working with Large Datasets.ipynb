{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e904aa7f-ea7d-4fa2-a3f7-faaf410c2996",
   "metadata": {},
   "source": [
    "# Out-of-Core Machine Learning with Vaex\n",
    "\n",
    "## 1. What is Out-of-Core Machine Learning?\n",
    "\n",
    "Out-of-core machine learning refers to techniques that allow you to train models on datasets that are too large to fit entirely in your system’s RAM. Instead of loading the whole dataset into memory, out-of-core methods process the data in smaller chunks or batches. This approach is especially useful for:\n",
    "\n",
    "- **Big Data:** When datasets exceed available memory.\n",
    "- **Streaming Data:** For real-time data that arrives continuously.\n",
    "- **Resource Efficiency:** Reducing memory footprint and enabling scalability.\n",
    "\n",
    "## 2. How Vaex Supports Out-of-Core ML\n",
    "\n",
    "[Vaex](https://vaex.io/) is a high-performance Python library designed for out-of-core DataFrame operations and big data analytics. It enables efficient data manipulation, transformation, and visualization on datasets that are larger than your machine's memory. Key features include:\n",
    "\n",
    "- **Memory Mapping:** Loads data from disk as needed without fully loading it into memory.\n",
    "- **Lazy Evaluation:** Operations are deferred until explicitly computed, saving resources.\n",
    "- **Fast Aggregations:** Optimized for computing statistical summaries, groupbys, and filtering on large datasets.\n",
    "\n",
    "## 3. Practical Implementation Using Vaex\n",
    "\n",
    "Below is an example that demonstrates how to use Vaex for out-of-core data processing. In this example, we simulate working with a large CSV file by loading a dataset with Vaex, performing some data transformations, and calculating aggregations without loading all data into RAM.\n",
    "\n",
    "### Example: Out-of-Core Data Processing with Vaex\n",
    "\n",
    "```python\n",
    "import vaex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For demonstration, we'll create a large synthetic CSV file.\n",
    "# In practice, you would have your own large CSV file.\n",
    "# Create a DataFrame with 10 million rows and 5 columns\n",
    "n_rows = 10_000_000\n",
    "data = {\n",
    "    'feature1': np.random.rand(n_rows),\n",
    "    'feature2': np.random.rand(n_rows),\n",
    "    'feature3': np.random.randint(0, 100, n_rows),\n",
    "    'feature4': np.random.normal(0, 1, n_rows),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n_rows)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file (simulate a large file)\n",
    "csv_path = 'large_dataset.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Load the large CSV file with Vaex using memory mapping\n",
    "# The 'convert' parameter creates an efficient binary version for faster access.\n",
    "df_vaex = vaex.from_csv(csv_path, convert=True, chunk_size=1_000_000)\n",
    "\n",
    "# Check the information about the dataset (this is a lazy operation)\n",
    "print(df_vaex.info())\n",
    "\n",
    "# Perform some out-of-core operations:\n",
    "# 1. Filter rows where feature3 is greater than 50\n",
    "df_filtered = df_vaex[df_vaex.feature3 > 50]\n",
    "\n",
    "# 2. Compute the mean of feature1 and feature4 for each category\n",
    "aggregated = df_filtered.groupby('category', agg={\n",
    "    'mean_feature1': vaex.agg.mean('feature1'),\n",
    "    'mean_feature4': vaex.agg.mean('feature4')\n",
    "})\n",
    "print(aggregated)\n",
    "\n",
    "# 3. Add a new computed column (e.g., ratio of feature1 to feature2)\n",
    "df_vaex['ratio'] = df_vaex.feature1 / (df_vaex.feature2 + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Compute the mean ratio (lazy evaluation, executed when needed)\n",
    "mean_ratio = df_vaex['ratio'].mean()\n",
    "print(\"Mean ratio of feature1/feature2:\", mean_ratio)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Creating & Saving Data:**  \n",
    "  We simulate a large dataset by generating a DataFrame with 10 million rows and saving it to CSV. In real scenarios, you might already have a large file.\n",
    "\n",
    "- **Loading Data with Vaex:**  \n",
    "  The `vaex.from_csv()` function loads the data using memory mapping. The `convert=True` parameter creates a binary format that speeds up subsequent loads.\n",
    "\n",
    "- **Filtering and Aggregation:**  \n",
    "  We filter the data and perform a group-by aggregation on a categorical column—all handled out-of-core without loading the entire dataset into memory.\n",
    "\n",
    "- **Lazy Computation:**  \n",
    "  Operations like calculating the mean of a computed column (`ratio`) are performed lazily, which means Vaex computes these results only when explicitly requested.\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "Out-of-core machine learning is crucial for working with large datasets that exceed memory limits. Vaex provides an efficient, scalable solution by combining memory mapping and lazy evaluation to process data in chunks. By using Vaex, you can perform complex transformations, filtering, and aggregations on big data without requiring extensive computational resources.\n",
    "\n",
    "> # reference:\n",
    ">   - NYC Cab Dataset Project - https://vaex.io/blog/ml-impossible-train-a-1-billion-sample-model-in-20-minutes-with-vaex-and-scikit-learn-on-your"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
