{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4ff71f-9620-4c76-8c64-ea6eaeebd5ce",
   "metadata": {},
   "source": [
    "# Multicollinearity in Regression Analysis\n",
    "\n",
    "Multicollinearity occurs when two or more predictor (independent) variables in a regression model are highly correlated. This can make it difficult to estimate the individual effect of each predictor on the target variable because their impacts on the outcome are intertwined.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Multicollinearity?\n",
    "\n",
    "- **Definition:**  \n",
    "  Multicollinearity is a situation in regression analysis where one predictor variable can be linearly predicted from the others with a substantial degree of accuracy. This high correlation among predictors can lead to:\n",
    "  - Unstable estimates of regression coefficients.\n",
    "  - Inflated standard errors, reducing the statistical power to detect significant predictors.\n",
    "  - Difficulty in assessing the relative importance of each predictor.\n",
    "\n",
    "- **Mathematical Insight:**  \n",
    "<p>When predictors are highly correlated, the design matrix <span style=\"font-family: 'Courier New', Courier, monospace;\">X</span> becomes nearly singular (or ill-conditioned), and solving for the coefficients <span style=\"font-family: 'Courier New', Courier, monospace;\">Î²</span> in the ordinary least squares (OLS) solution:</p>  \n",
    "\n",
    "  \n",
    "  $$\n",
    "  \\hat{\\beta} = (X^T X)^{-1} X^T y\n",
    "  $$\n",
    "  can result in large variances.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Practical Example\n",
    "\n",
    "<p>Consider a dataset with two predictors <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>1</sub></span> and <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>2</sub></span> that are highly correlated. For example, in a synthetic dataset, you might generate <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>2</sub></span> as a linear function of <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>1</sub></span> plus a little noise:</p>  \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X1 = np.random.normal(50, 10, n_samples)\n",
    "# X2 is highly correlated with X1 (e.g., X2 = X1 with some noise)\n",
    "X2 = X1 + np.random.normal(0, 2, n_samples)\n",
    "# Target variable depends on both X1 and X2\n",
    "y = 3*X1 + 2*X2 + np.random.normal(0, 5, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\n",
    "\n",
    "# Plot the relationship between X1 and X2\n",
    "plt.scatter(df['X1'], df['X2'])\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Scatter plot of X1 vs X2')\n",
    "plt.show()\n",
    "\n",
    "# Fit a regression model with both predictors\n",
    "X = df[['X1', 'X2']]\n",
    "X = sm.add_constant(X)  # Adds a constant term to the model\n",
    "model = sm.OLS(df['y'], X).fit()\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "**Interpretation:**  \n",
    "<p>In the regression summary, you might observe that the coefficients for <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>1</sub></span> and <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>2</sub></span> have large standard errors and possibly unexpected signs even though both variables are important. This is a symptom of multicollinearity.</p>  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Variance Inflation Factor (VIF) in Multicollinearity\n",
    "\n",
    "### What is VIF?\n",
    "\n",
    "- **Definition:**  \n",
    "<p>The Variance Inflation Factor quantifies how much the variance of an estimated regression coefficient increases due to multicollinearity. For a predictor <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>j</sub></span>, VIF is defined as:</p>  \n",
    "\n",
    "  $$\n",
    "  \\text{VIF}_j = \\frac{1}{1 - R_j^2}\n",
    "  $$\n",
    "  \n",
    "  <p>where <span style=\"font-family: 'Courier New', Courier, monospace;\">R<sub>j</sub><sup>2</sup></span> is the coefficient of determination obtained by regressing <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>j</sub></span> on all the other predictors.</p>  \n",
    "\n",
    "- **Interpretation:**  \n",
    "  - A VIF of 1 indicates no correlation with other variables.\n",
    "  - VIFs between 1 and 5 suggest moderate correlation.\n",
    "  - VIFs above 5 (or sometimes 10) indicate high multicollinearity and warrant further investigation.\n",
    "\n",
    "### Python Example: Computing VIF\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Prepare the feature matrix (excluding the target)\n",
    "features = df[['X1', 'X2']]\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = features.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n",
    "\n",
    "print(vif_data)\n",
    "```\n",
    "\n",
    "**Interpretation:**  \n",
    "<ul>  \n",
    "    <li>High VIF values (e.g., &gt;5) indicate that the corresponding predictor is highly correlated with other predictors in the model.</li>  \n",
    "    <li>In our synthetic example, expect both <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>1</sub></span> and <span style=\"font-family: 'Courier New', Courier, monospace;\">X<sub>2</sub></span> to show high VIF values due to their strong linear relationship.</li>  \n",
    "</ul>  \n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Multicollinearity can significantly affect the stability and interpretability of regression models. Understanding its implications, identifying it through diagnostic measures like VIF, and using techniques such as feature selection or regularization can help mitigate its effects. By examining practical examples and computing VIF, you can better diagnose and address multicollinearity in your datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
