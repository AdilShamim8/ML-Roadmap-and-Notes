{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805647bc-ffa4-45d9-aebb-cec340778d58",
   "metadata": {},
   "source": [
    "# Handling imbalanced dataset in machine learning\n",
    "\n",
    "Imbalanced datasets occur when the number of samples in one class significantly outnumber those in other classes. This imbalance can cause machine learning models to be biased toward the majority class. Below are four common methods to handle imbalanced data:\n",
    "\n",
    "---\n",
    "\n",
    "## Method 1: Undersampling\n",
    "\n",
    "### Overview\n",
    "- **Definition:** Reduces the size of the majority class by randomly removing samples.\n",
    "- **Advantage:** Simplifies the dataset and speeds up training.\n",
    "- **Disadvantage:** May discard useful information, leading to loss of potentially valuable data.\n",
    "\n",
    "### How It Works\n",
    "- Randomly select a subset of the majority class such that its size is closer to the minority class.\n",
    "\n",
    "### Python Example\n",
    "\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "\n",
    "# Apply random undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "print(\"Resampled class distribution:\", Counter(y_res))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Method 2: Oversampling\n",
    "\n",
    "### Overview\n",
    "- **Definition:** Increases the number of samples in the minority class by randomly duplicating them.\n",
    "- **Advantage:** Retains all information from the majority class.\n",
    "- **Disadvantage:** Can lead to overfitting since samples are repeated.\n",
    "\n",
    "### How It Works\n",
    "- Duplicate samples from the minority class until the classes are more balanced.\n",
    "\n",
    "### Python Example\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Apply random oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "print(\"Resampled class distribution (oversampling):\", Counter(y_res))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Method 3: SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "### Overview\n",
    "- **Definition:** Generates synthetic samples for the minority class instead of duplicating existing ones.\n",
    "- **Advantage:** Introduces new, slightly varied samples which can help reduce overfitting.\n",
    "- **Disadvantage:** May introduce noise if not tuned properly.\n",
    "\n",
    "### How It Works\n",
    "<p>For each minority sample <span style=\"font-family: 'Courier New', Courier, monospace;\">x<sub>i</sub></span>, SMOTE:</p>  \n",
    "<ol>  \n",
    "    <li>Finds <span style=\"font-family: 'Courier New', Courier, monospace;\">k</span>-nearest neighbors.</li>  \n",
    "    <li>Randomly selects one neighbor <span style=\"font-family: 'Courier New', Courier, monospace;\">x<sub>zi</sub></span>.</li>  \n",
    "</ol>  \n",
    "3. Generates a synthetic sample:\n",
    "   $$\n",
    "   x_{\\text{new}} = x_i + \\delta \\times (x_{zi} - x_i)\n",
    "   $$\n",
    "   \n",
    "   <p>where <span style=\"font-family: 'Courier New', Courier, monospace;\">ùõø</span> is a random number between 0 and 1.</p>  \n",
    "\n",
    "### Python Example\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "print(\"Resampled class distribution (SMOTE):\", Counter(y_res))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Method 4: Ensemble Methods with Undersampling\n",
    "\n",
    "### Overview\n",
    "- **Definition:** Combines the idea of undersampling with ensemble learning to reduce information loss.\n",
    "- **Advantage:** Mitigates the downsides of undersampling by training multiple models on different undersampled subsets.\n",
    "- **Disadvantage:** More computationally expensive since it involves training multiple models.\n",
    "\n",
    "### How It Works\n",
    "1. **Create Multiple Subsets:** Randomly undersample the majority class multiple times to create several balanced subsets.\n",
    "2. **Train Base Models:** Train a separate model on each balanced subset.\n",
    "3. **Aggregate Predictions:** Combine predictions from all base models (e.g., using voting or averaging) to make a final decision.\n",
    "\n",
    "### Python Example: Balanced Bagging Classifier\n",
    "\n",
    "```python\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define base estimator and ensemble classifier\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "ensemble = BalancedBaggingClassifier(base_estimator=base_estimator,\n",
    "                                     n_estimators=10,\n",
    "                                     random_state=42)\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble.fit(X, y)\n",
    "y_pred = ensemble.predict(X)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "print(classification_report(y, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "When working with imbalanced data, there is no one-size-fits-all solution. The choice of method depends on the dataset size, the importance of preserving the majority class data, and the risk of overfitting. Here‚Äôs a quick summary:\n",
    "- **Undersampling:** Reduces data size by removing majority samples.\n",
    "- **Oversampling:** Balances the dataset by duplicating minority samples.\n",
    "- **SMOTE:** Generates new, synthetic minority samples to enhance diversity.\n",
    "- **Ensemble with Undersampling:** Combines the robustness of ensembles with multiple undersampled subsets for improved performance.\n",
    "---\n",
    "> ## reference:\n",
    ">    - Resampling strategies for imbalanced datasets - https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    ">    - Kaggle Notebook - https://www.kaggle.com/kabure/credit-card-fraud-prediction-rf-smote\n",
    ">    - SMOTE on Quora Dataset - https://www.kaggle.com/theoviel/dealing-with-class-imbalance-with-smote"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
