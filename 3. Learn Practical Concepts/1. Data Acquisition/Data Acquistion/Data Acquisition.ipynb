{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97de1867-0468-4389-b5f9-46e47f868241",
   "metadata": {},
   "source": [
    "# Data Acquisition with Python  \n",
    "_Data acquisition_ refers to the process of gathering data from various sources. Two common techniques are **web scraping** and **fetching data from APIs**. In Python, you have several libraries at your disposal to simplify these tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Web Scraping | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2003%20Pandas%20DataFrame%20using%20Web%20Scraping)\n",
    "\n",
    "### What Is Web Scraping?\n",
    "Web scraping is the automated process of extracting information from websites. It involves downloading the HTML content of a page and parsing it to extract the desired data. Common use cases include:\n",
    "- Collecting product details from e-commerce sites.\n",
    "- Extracting news headlines.\n",
    "- Gathering research data.\n",
    "\n",
    "### Common Python Libraries\n",
    "- **`requests`**: For making HTTP requests to download web pages.\n",
    "- **`BeautifulSoup`** (from **`bs4`**): For parsing HTML and XML documents.\n",
    "- **`Scrapy`**: A powerful and scalable framework for large crawling projects.\n",
    "- **`Selenium`**: For scraping dynamic content rendered by JavaScript (via browser automation).\n",
    "\n",
    "### Example: Scraping a Web Page Using Requests and BeautifulSoup\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the target URL and headers (to mimic a browser)\n",
    "url = \"https://example.com\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract the page title as an example\n",
    "    title = soup.find(\"title\").get_text()\n",
    "    print(\"Page Title:\", title)\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Always include a proper **User-Agent** header.\n",
    "- Check for a successful response (HTTP status code 200).\n",
    "- Use BeautifulSoup to navigate the HTML DOM and extract information.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Fetching Data from APIs | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2000%20API%20To%20DataFrame)\n",
    "\n",
    "### What Is an API?\n",
    "An **API (Application Programming Interface)** allows you to interact with an external service in a structured way. Instead of scraping HTML, you make HTTP requests (GET, POST, etc.) and receive data (often in JSON format).\n",
    "\n",
    "### Why Use APIs?\n",
    "- APIs provide structured data that is easier to parse.\n",
    "- They often have clear documentation on how to query and retrieve data.\n",
    "- They can be more reliable than scraping websites that might change their layout.\n",
    "\n",
    "### Common Python Libraries\n",
    "- **`requests`**: For sending HTTP requests.\n",
    "- **`json`**: For parsing JSON responses (built into Python).\n",
    "- Additional libraries (e.g., **`pandas`**) can help transform API data into dataframes for analysis.\n",
    "\n",
    "### Example: Fetching Data from an API\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# Define the API endpoint and parameters\n",
    "api_url = \"https://api.example.com/data\"\n",
    "params = {\n",
    "    \"q\": \"search_term\",\n",
    "    \"api_key\": \"YOUR_API_KEY\"  # Replace with your actual API key if needed\n",
    "}\n",
    "\n",
    "# Make a GET request to the API\n",
    "response = requests.get(api_url, params=params)\n",
    "if response.status_code == 200:\n",
    "    # Parse JSON data\n",
    "    data = response.json()\n",
    "    print(\"API Data:\", data)\n",
    "else:\n",
    "    print(\"Error fetching data. Status code:\", response.status_code)\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Most APIs use JSON; use `response.json()` to easily parse the response.\n",
    "- Always refer to the API’s documentation for required parameters and authentication.\n",
    "- Handle errors by checking the response status code.\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices & Considerations\n",
    "\n",
    "- **Respect the Website's Terms:** Always check the website’s terms of service before scraping. Some sites prohibit automated scraping.\n",
    "- **Rate Limiting:** Introduce delays between requests (using Python’s `time.sleep()`) to avoid overloading servers or getting blocked.\n",
    "- **API Keys:** When working with APIs that require authentication, keep your keys secure (e.g., using environment variables or a `.env` file).\n",
    "- **Dynamic Content:** For pages rendered by JavaScript, consider using tools like Selenium or Playwright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe4c1b-21d8-48de-93b1-3319b6cf38b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
