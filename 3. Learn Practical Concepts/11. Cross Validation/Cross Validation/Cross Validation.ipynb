{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe8d4e4-4c4f-4b46-82fa-f61a274eb0fa",
   "metadata": {},
   "source": [
    "# Cross-Validation in Machine Learning\n",
    "\n",
    "## 1. What is Cross-Validation?\n",
    "Cross-validation is a resampling technique used to assess the performance of a machine learning model on unseen data. It helps in:\n",
    "- Avoiding overfitting\n",
    "- Selecting the best model\n",
    "- Tuning hyperparameters\n",
    "\n",
    "## 2. Holdout Method\n",
    "The simplest form of cross-validation, where the dataset is split into:\n",
    "- **Training Set**: Used to train the model.\n",
    "- **Test Set**: Used to evaluate model performance.\n",
    "\n",
    "### Example in Python:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Generating random data\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "# Splitting data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating on test set\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"Test R² Score:\", score)\n",
    "```\n",
    "\n",
    "## 3. K-Fold Cross-Validation\n",
    "Divides the dataset into *k* equal folds. The model is trained on *k-1* folds and tested on the remaining fold. The process is repeated *k* times, and results are averaged.\n",
    "\n",
    "### Formula:\n",
    "$$\n",
    "Error = \\frac{1}{k} \\sum_{i=1}^{k} Error_i\n",
    "$$\n",
    "\n",
    "### Example in Python:\n",
    "```python\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "print(\"K-Fold R² Scores:\", scores)\n",
    "print(\"Average R²:\", np.mean(scores))\n",
    "```\n",
    "\n",
    "## 4. Leave-One-Out Cross-Validation (LOOCV)\n",
    "A special case of K-Fold where *k* equals the number of data points. Each iteration trains on *n-1* samples and tests on 1 sample.\n",
    "\n",
    "### Example in Python:\n",
    "```python\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(model, X, y, cv=loo, scoring='r2')\n",
    "print(\"LOOCV Average R²:\", np.mean(scores))\n",
    "```\n",
    "\n",
    "## 5. Time Series Cross-Validation\n",
    "For time-dependent data, regular shuffling is not possible. Instead, the dataset is split sequentially, ensuring training always precedes testing.\n",
    "\n",
    "### Example in Python:\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')\n",
    "print(\"Time Series Cross-Validation Scores:\", scores)\n",
    "```\n",
    "\n",
    "### Summary:\n",
    "| Method | Best for |\n",
    "|--------|---------|\n",
    "| Holdout | Large datasets, fast evaluation |\n",
    "| K-Fold | Balanced performance estimation |\n",
    "| LOOCV | Small datasets, high variance |\n",
    "| Time Series | Temporal data |\n",
    "\n",
    "Cross-validation ensures robust model evaluation and selection, improving generalization to new data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
