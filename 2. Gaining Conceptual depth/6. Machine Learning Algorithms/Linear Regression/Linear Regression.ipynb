{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8784b7-eaed-4f2b-bb8d-90585876be09",
   "metadata": {},
   "source": [
    "# Regression Notes: Simple, Multiple, and Polynomial Regression\n",
    "\n",
    "## Table of Contents\n",
    "1. [Simple Linear Regression](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2030%20Simple%20Linear%20Regression)\n",
    "2. [Multiple Linear Regression](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2032%20Multiple%20Linear%20Regression)\n",
    "3. [Polynomial Regression](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2035%20Polynomial%20Regression)\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "### Model Formula\n",
    "\n",
    "<p>\n",
    "  y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x + &epsilon;\n",
    "</p>\n",
    "\n",
    "- **y**: Dependent variable  \n",
    "- **x**: Independent variable  \n",
    "- **&beta;<sub>0</sub>**: Intercept  \n",
    "- **&beta;<sub>1</sub>**: Slope  \n",
    "- **&epsilon;**: Error term  \n",
    "\n",
    "### Python Code Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data for simple linear regression\n",
    "np.random.seed(42)\n",
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * x + np.random.randn(100, 1)\n",
    "\n",
    "# Create the design matrix by adding a column of ones (for the intercept)\n",
    "X = np.c_[np.ones((100, 1)), x]\n",
    "\n",
    "# Compute the optimal coefficients using the normal equation:\n",
    "# theta = (X^T * X)^(-1) * X^T * y\n",
    "theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "print(\"Theta (Simple Linear Regression):\")\n",
    "print(theta_best)\n",
    "\n",
    "# Plot the data and the regression line\n",
    "plt.scatter(x, y, color='blue', label='Data Points')\n",
    "plt.plot(x, X.dot(theta_best), color='red', label='Regression Line')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "### Model Formula\n",
    "\n",
    "<p>\n",
    "  y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x<sub>1</sub> + &beta;<sub>2</sub>x<sub>2</sub> + ... + &beta;<sub>p</sub>x<sub>p</sub> + &epsilon;\n",
    "</p>\n",
    "\n",
    "- **x<sub>1</sub>, x<sub>2</sub>, ... , x<sub>p</sub>**: Multiple independent variables  \n",
    "- **&beta;<sub>i</sub>**: Coefficient for the i-th predictor  \n",
    "\n",
    "### Python Code Example\n",
    "\n",
    "```python\n",
    "# Generate synthetic data for multiple linear regression\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "x1 = 2 * np.random.rand(m, 1)\n",
    "x2 = 3 * np.random.rand(m, 1)\n",
    "\n",
    "# Create the design matrix: add a column of ones for the intercept\n",
    "X_multi = np.c_[np.ones((m, 1)), x1, x2]\n",
    "\n",
    "# Define true coefficients and generate y with some noise\n",
    "theta_true = np.array([[5], [4], [3]])\n",
    "y_multi = X_multi.dot(theta_true) + np.random.randn(m, 1)\n",
    "\n",
    "# Compute the optimal coefficients using the normal equation\n",
    "theta_best_multi = np.linalg.inv(X_multi.T.dot(X_multi)).dot(X_multi.T).dot(y_multi)\n",
    "print(\"Theta (Multiple Linear Regression):\")\n",
    "print(theta_best_multi)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "### Model Formula\n",
    "\n",
    "<p>\n",
    "  y = &beta;<sub>0</sub> + &beta;<sub>1</sub>x + &beta;<sub>2</sub>x<sup>2</sup> + ... + &beta;<sub>n</sub>x<sup>n</sup> + &epsilon;\n",
    "</p>\n",
    "\n",
    "- Here, the input **x** is transformed into a new feature space:\n",
    "  - 1, x, x<sup>2</sup>, ..., x<sup>n</sup>\n",
    "- The regression is still “linear” in the coefficients &beta; even though it models a nonlinear relationship.\n",
    "\n",
    "### Python Code Example using scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data for polynomial regression\n",
    "np.random.seed(42)\n",
    "x_poly = 2 * np.random.rand(100, 1) - 1  # x values in range [-1, 1]\n",
    "y_poly = 1 + 2 * x_poly + 3 * x_poly**2 + np.random.randn(100, 1)\n",
    "\n",
    "# Transform the input data to include polynomial features (degree 2)\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=True)\n",
    "X_poly_transformed = poly_features.fit_transform(x_poly)\n",
    "\n",
    "# Fit a linear regression model on the transformed polynomial features\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly_transformed, y_poly)\n",
    "y_poly_pred = lin_reg.predict(X_poly_transformed)\n",
    "\n",
    "print(\"Coefficients (Polynomial Regression):\")\n",
    "print(lin_reg.coef_)\n",
    "print(\"Intercept:\")\n",
    "print(lin_reg.intercept_)\n",
    "\n",
    "# Plot the polynomial fit\n",
    "plt.scatter(x_poly, y_poly, color='blue', label='Data Points')\n",
    "# For a smooth curve, sort x_poly and the corresponding predictions\n",
    "sorted_idx = x_poly.flatten().argsort()\n",
    "plt.plot(x_poly[sorted_idx], y_poly_pred[sorted_idx], color='red', label='Polynomial Fit')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression (Degree 2)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Simple Linear Regression:**  \n",
    "  Fits a straight line to the data with one predictor.\n",
    "  \n",
    "- **Multiple Linear Regression:**  \n",
    "  Extends the model to multiple predictors.\n",
    "  \n",
    "- **Polynomial Regression:**  \n",
    "  Transforms the input features (e.g., powers of x) and applies linear regression to capture non-linear relationships.\n",
    "  \n",
    "Each of these methods can be solved by the ordinary least squares (OLS) method, which minimizes the sum of squared residuals between the actual and predicted values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
