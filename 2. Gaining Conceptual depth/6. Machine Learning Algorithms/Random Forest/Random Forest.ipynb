{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b780ce5-e1e2-466c-8b71-3f58f35c66c5",
   "metadata": {},
   "source": [
    "# Random Forest | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2042%20Random%20Forest)\n",
    "\n",
    "Random Forest is an ensemble learning method used for classification and regression. It builds multiple decision trees and combines their outputs to make a more accurate and stable prediction.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Ensemble Method**: Random Forest is an ensemble method because it combines predictions from multiple models (the decision trees) to create a stronger model.\n",
    "\n",
    "2. **Bootstrapping**: Random Forest creates multiple decision trees by sampling subsets of the training data with replacement (bootstrapping). This means each tree is trained on a different subset of the data.\n",
    "\n",
    "3. **Feature Selection**: At each node, a random subset of features is selected to split the data, introducing diversity among the trees and helping to reduce overfitting.\n",
    "\n",
    "4. **Voting or Averaging**:\n",
    "   - **For Classification**: The final prediction is based on the majority vote from all the trees. \n",
    "   - **For Regression**: The final prediction is the average of the predictions from all the trees.\n",
    "\n",
    "## Formula for Random Forest\n",
    "\n",
    "For **classification**:\n",
    "```html\n",
    "P(y|X) = Majority Voting from all trees\n",
    "```\n",
    "\n",
    "For **regression**:\n",
    "```html\n",
    "P(y|X) = Average of predictions from all trees\n",
    "```\n",
    "\n",
    "## Random Forest Algorithm Steps\n",
    "\n",
    "1. **Create Bootstrapped Data**: Create multiple subsets of the original dataset using bootstrapping.\n",
    "2. **Build Decision Trees**: Train a decision tree on each of the bootstrapped datasets.\n",
    "3. **Combine Predictions**:\n",
    "   - For classification, use majority voting from all trees.\n",
    "   - For regression, use the average of all trees' predictions.\n",
    "\n",
    "## Python Code Example\n",
    "\n",
    "Here is an example of how to implement a Random Forest classifier in Python using the `sklearn` library:\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing and training the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Random Forest Classifier: {accuracy * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "## Advantages of Random Forest\n",
    "- **Robust**: Random Forest is resistant to overfitting due to its ensemble approach.\n",
    "- **Handles Missing Values**: It can handle missing data well by averaging the results across trees.\n",
    "- **Versatile**: It can be used for both classification and regression problems.\n",
    "\n",
    "## Disadvantages of Random Forest\n",
    "- **Complexity**: The model can become very complex and harder to interpret due to the large number of trees.\n",
    "- **Computationally Expensive**: Training many trees requires significant computational power, especially with large datasets.\n",
    "\n",
    "## Hyperparameters to Tune\n",
    "- `n_estimators`: Number of trees in the forest.\n",
    "- `max_depth`: Maximum depth of each tree.\n",
    "- `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "- `min_samples_leaf`: Minimum number of samples required to be at a leaf node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
