{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f6ed40-82e3-4fdc-bf77-22a0b4219f26",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title></title>\n",
    "    <style>\n",
    "      body { font-family: Arial, sans-serif; line-height: 1.6; }\n",
    "      code { background-color: #f4f4f4; padding: 2px 4px; }\n",
    "      pre { background-color: #f4f4f4; padding: 10px; overflow-x: auto; }\n",
    "      .formula { font-style: italic; color: #333; }\n",
    "      h1, h2, h3 { border-bottom: 1px solid #ddd; padding-bottom: 3px; }\n",
    "    </style>\n",
    "  </head>\n",
    "  <body>\n",
    "  \n",
    "  <h1>Logistic & Softmax Regression Study Notes</h1>\n",
    "  \n",
    "  <h2>1. Logistic Regression <a href=\"https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2039%20Logistic%20Regression\">Link</a></h2>\n",
    "  <p>\n",
    "    Logistic Regression is a supervised algorithm used for binary classification.\n",
    "    Instead of predicting a continuous value, it predicts the probability that an input\n",
    "    <em>x</em> belongs to a class.\n",
    "  </p>\n",
    "  \n",
    "  <h3>Sigmoid Function</h3>\n",
    "  <p>\n",
    "    The key element is the sigmoid (logistic) function, defined as:\n",
    "  </p>\n",
    "  <p class=\"formula\">\n",
    "    \\( \\sigma(z) = \\frac{1}{1+e^{-z}} \\)\n",
    "  </p>\n",
    "  <p>\n",
    "    where \\( z = \\mathbf{w}^T\\mathbf{x} + b \\).\n",
    "  </p>\n",
    "  \n",
    "  <h3>Cross-Entropy Loss</h3>\n",
    "  <p>\n",
    "    For a training example with true label \\( y \\) and prediction \\( \\hat{y}=\\sigma(z) \\), the loss is:\n",
    "  </p>\n",
    "  <p class=\"formula\">\n",
    "    \\( L(\\hat{y}, y) = -\\bigl[ y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\bigr] \\)\n",
    "  </p>\n",
    "  <p>\n",
    "    The goal during training is to minimize the average loss over the dataset.\n",
    "  </p>\n",
    "  \n",
    "  <h3>Python Example: Logistic Regression with scikit-learn</h3>\n",
    "  <pre><code class=\"python\">\n",
    "# Using scikit-learn to train logistic regression on a binary problem (Iris subset)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset and select two classes for binary classification\n",
    "data = load_iris()\n",
    "X = data.data[data.target != 2]\n",
    "y = data.target[data.target != 2]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create logistic regression model and train it\n",
    "model = LogisticRegression(solver='lbfgs', C=1.0, max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "  </code></pre>\n",
    "  \n",
    "  <h3>Python Example: Logistic Regression from Scratch (Gradient Descent)</h3>\n",
    "  <p>\n",
    "    The following Python code implements logistic regression manually using NumPy.\n",
    "  </p>\n",
    "  <pre><code class=\"python\">\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    z = np.dot(X, w) + b\n",
    "    a = sigmoid(z)\n",
    "    # Add a small value (epsilon) to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    cost = -np.mean(y * np.log(a + epsilon) + (1 - y) * np.log(1 - a + epsilon))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, w, b, learning_rate, num_iterations):\n",
    "    m = X.shape[0]\n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, w) + b\n",
    "        a = sigmoid(z)\n",
    "        dw = np.dot(X.T, (a - y)) / m\n",
    "        db = np.sum(a - y) / m\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            cost = compute_cost(X, y, w, b)\n",
    "            print(f\"Iteration {i}, Cost: {cost}\")\n",
    "    return w, b\n",
    "\n",
    "# Create a synthetic binary classification dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 2)\n",
    "# Define a simple rule: if sum of features > 0, class 1; else, class 0.\n",
    "y = (np.sum(X, axis=1) > 0).astype(int)\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.zeros(X.shape[1])\n",
    "b = 0\n",
    "\n",
    "# Train logistic regression using gradient descent\n",
    "w, b = gradient_descent(X, y, w, b, learning_rate=0.1, num_iterations=10000)\n",
    "print(\"Learned weights:\", w, \"Bias:\", b)\n",
    "\n",
    "# Predict on the training data\n",
    "z = np.dot(X, w) + b\n",
    "predictions = (sigmoid(z) >= 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(\"Training Accuracy (from scratch):\", accuracy)\n",
    "  </code></pre>\n",
    "  \n",
    "  <h2>2. Softmax Regression (Multinomial Logistic Regression) <a href=\"https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2041%20Logistic%20Regression%20(continued)\">Link</a> </h2>\n",
    "  <p>\n",
    "    Softmax regression generalizes logistic regression to classify inputs into\n",
    "    more than two classes.\n",
    "  </p>\n",
    "  \n",
    "  <h3>Softmax Function </h3>\n",
    "  <p>\n",
    "    For a vector of logits \\(\\mathbf{z} = [z_1, z_2, \\dots, z_K]\\), the softmax function is:\n",
    "  </p>\n",
    "  <p class=\"formula\">\n",
    "    \\( \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\)\n",
    "  </p>\n",
    "  \n",
    "  <h3>Python Example: Softmax Regression with scikit-learn</h3>\n",
    "  <pre><code class=\"python\">\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the full Iris dataset (3 classes)\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train multinomial logistic regression (softmax regression)\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict and calculate accuracy on training data\n",
    "y_pred = model.predict(X)\n",
    "print(\"Softmax Regression Accuracy:\", accuracy_score(y, y_pred))\n",
    "  </code></pre>\n",
    "  \n",
    "  <h2>3. Polynomial Features in Logistic Regression <a href=\"https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2041%20Logistic%20Regression%20(continued)\">Link</a> </h2>\n",
    "  <p>\n",
    "    To capture non-linear relationships, we can add polynomial features to the input data.\n",
    "    For example, for a single feature \\( x \\), you may use \\( x^2 \\), \\( x^3 \\), etc. For multiple\n",
    "    features, interaction terms such as \\( x_1x_2 \\) can be added.\n",
    "  </p>\n",
    "  <p>\n",
    "    <strong>Note:</strong> The number of features increases combinatorially with the polynomial degree.\n",
    "    With \\( N \\) original features and degree \\( k \\), the total number of terms is given by:\n",
    "  </p>\n",
    "  <p class=\"formula\">\n",
    "    \\( \\binom{N+k}{k} \\)\n",
    "  </p>\n",
    "  \n",
    "  <h3>Python Example: Using PolynomialFeatures with scikit-learn</h3>\n",
    "  <pre><code class=\"python\">\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline that transforms features and then applies logistic regression.\n",
    "degree = 2  # Change the degree as needed\n",
    "model_poly = make_pipeline(\n",
    "    PolynomialFeatures(degree=degree),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=200)\n",
    ")\n",
    "\n",
    "# Use the same binary subset of the Iris dataset\n",
    "model_poly.fit(X_train, y_train)\n",
    "y_pred_poly = model_poly.predict(X_test)\n",
    "print(\"Polynomial Logistic Regression Accuracy (degree={}):\".format(degree),\n",
    "      accuracy_score(y_test, y_pred_poly))\n",
    "  </code></pre>\n",
    "  \n",
    "  <h2>4. Hyperparameters in Logistic Regression <a href=\"https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2041%20Logistic%20Regression%20(continued)\">Link</a> </h2>\n",
    "  <p>\n",
    "    Common hyperparameters include:\n",
    "  </p>\n",
    "  <ul>\n",
    "    <li><strong>Regularization Strength (C or λ):</strong> Controls overfitting. Smaller <em>C</em> (or larger λ) implies stronger regularization.</li>\n",
    "    <li><strong>Penalty Type:</strong> L1 (promotes sparsity) or L2 (penalizes large weights).</li>\n",
    "    <li><strong>Solver:</strong> Algorithms such as <code>liblinear</code>, <code>lbfgs</code>, <code>sag</code>, or <code>saga</code>.</li>\n",
    "    <li><strong>max_iter:</strong> Maximum iterations for convergence.</li>\n",
    "    <li><strong>tol:</strong> Tolerance for stopping criteria.</li>\n",
    "  </ul>\n",
    "  <p>\n",
    "    These settings are tuned (using methods like grid search) to optimize performance.\n",
    "  </p>\n",
    "  \n",
    "  <h2>5. Summary</h2>\n",
    "  <p>\n",
    "    By combining theory with practical code examples, you can build a strong foundation in:\n",
    "  </p>\n",
    "  <ul>\n",
    "    <li>Binary Logistic Regression using the sigmoid function and cross-entropy loss.</li>\n",
    "    <li>Extending to multi-class problems with Softmax Regression.</li>\n",
    "    <li>Enhancing model capacity with Polynomial Features (noting the risk of overfitting and increased computation).</li>\n",
    "    <li>Tuning hyperparameters such as regularization, solvers, and iteration limits.</li>\n",
    "  </ul>\n",
    "  \n",
    "  <p>\n",
    "    The Python code examples illustrate both library-based implementations (using scikit-learn) and\n",
    "    a manual implementation using NumPy with gradient descent. Experiment with these examples to deepen your understanding.\n",
    "  </p>\n",
    "  \n",
    "  </body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
