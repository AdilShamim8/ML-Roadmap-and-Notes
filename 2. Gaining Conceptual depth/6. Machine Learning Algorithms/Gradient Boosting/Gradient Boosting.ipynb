{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb1842b-f236-41cc-b33f-507b18935e78",
   "metadata": {},
   "source": [
    "# Gradient Boosting: Note | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2045%20Gradient%20Boosting)\n",
    "\n",
    "Gradient Boosting is a powerful ensemble technique that builds a strong predictive model by sequentially combining multiple weak learners (often shallow decision trees). Each new model is trained to correct the errors of the ensemble built so far by approximating the negative gradient of a loss function.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Mathematical Background\n",
    "\n",
    "We want to find a model \\( F(x) \\) that minimizes a differentiable loss function \\( L(y, F(x)) \\) over a training set. The objective is to solve:\n",
    "\n",
    "$$\n",
    "F = \\underset{F}{\\operatorname{arg\\,min}} \\sum_{i=1}^{n} L(y_i, F(x_i))\n",
    "$$\n",
    "\n",
    "We represent \\( F(x) \\) as an additive model:\n",
    "\n",
    "$$\n",
    "F(x) = F_0(x) + \\sum_{m=1}^{M} \\gamma_m \\, h_m(x)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( F_0(x) \\) is the initial model (often a constant).\n",
    "- \\( h_m(x) \\) is the \\( m \\)-th weak learner.\n",
    "- \\( \\gamma_m \\) is the weight (step size) for the \\( m \\)-th learner.\n",
    "\n",
    "At each iteration \\( m \\), we compute the pseudo-residuals:\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left.\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F = F_{m-1}}\n",
    "$$\n",
    "\n",
    "Next, we fit a weak learner \\( h_m(x) \\) to these residuals and determine the optimal step size \\( \\gamma_m \\) via:\n",
    "\n",
    "$$\n",
    "\\gamma_m = \\underset{\\gamma}{\\operatorname{arg\\,min}} \\sum_{i=1}^{n} L\\left(y_i, F_{m-1}(x_i) + \\gamma \\, h_m(x_i)\\right)\n",
    "$$\n",
    "\n",
    "Finally, the model is updated:\n",
    "\n",
    "$$\n",
    "F_{m}(x) = F_{m-1}(x) + \\nu \\, \\gamma_m \\, h_m(x)\n",
    "$$\n",
    "\n",
    "Here, \\( \\nu \\) (nu) is the learning rate that scales the contribution of each new learner.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Python Implementation Step by Step\n",
    "\n",
    "Below is an example using scikit-learnâ€™s `GradientBoostingClassifier` on a synthetic dataset.\n",
    "\n",
    "### Step 1: Import Libraries and Create a Dataset\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a toy binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, \n",
    "                           n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### Step 2: Initialize and Train the Gradient Boosting Model\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                      max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "gb_clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Step 3: Make Predictions and Evaluate the Model\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "### Step 4: Conceptual Illustration of One Boosting Iteration (Regression Example)\n",
    "\n",
    "For regression using mean squared error (MSE), the process can be outlined as follows:\n",
    "\n",
    "1. **Initialization:**  \n",
    "   Set the initial prediction \\( F_0 \\) as the mean of the target values:\n",
    "   \n",
    "   $$\n",
    "   F_0 = \\frac{1}{n}\\sum_{i=1}^{n} y_i\n",
    "   $$\n",
    "\n",
    "2. **Compute Pseudo-Residuals:**  \n",
    "   For MSE, the pseudo-residuals are the actual residuals:\n",
    "   \n",
    "   $$\n",
    "   r_i = y_i - F_0\n",
    "   $$\n",
    "\n",
    "3. **Fit a Weak Learner:**  \n",
    "   Train a simple decision tree regressor on the residuals:\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.tree import DecisionTreeRegressor\n",
    "   \n",
    "   weak_learner = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "   weak_learner.fit(X_train, r)\n",
    "   predictions = weak_learner.predict(X_train)\n",
    "   ```\n",
    "\n",
    "4. **Determine Optimal Step Size (\\( \\gamma \\)):**  \n",
    "   For MSE loss in this simple case, the optimal \\( \\gamma \\) is 1.\n",
    "   \n",
    "   ```python\n",
    "   gamma = 1\n",
    "   ```\n",
    "\n",
    "5. **Update the Model:**\n",
    "   \n",
    "   $$\n",
    "   F_1 = F_0 + \\gamma \\cdot \\text{predictions}\n",
    "   $$\n",
    "\n",
    "6. **Repeat:**  \n",
    "   In a full gradient boosting model, repeat these steps for several iterations, updating \\( F_m(x) \\) at each step.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Conclusion\n",
    "\n",
    "This note has explained the key ideas behind Gradient Boosting:\n",
    "\n",
    "- **Model Representation:**  \n",
    "  $$ F(x) = F_0(x) + \\sum_{m=1}^{M} \\gamma_m \\, h_m(x) $$\n",
    "\n",
    "- **Pseudo-Residuals Computation:**  \n",
    "  $$ r_{im} = -\\left.\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F = F_{m-1}} $$\n",
    "\n",
    "- **Model Update Rule:**  \n",
    "  $$ F_{m}(x) = F_{m-1}(x) + \\nu \\, \\gamma_m \\, h_m(x) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
