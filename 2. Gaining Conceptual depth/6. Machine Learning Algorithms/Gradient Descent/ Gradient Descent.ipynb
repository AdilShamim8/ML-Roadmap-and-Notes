{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874f557e-9d1e-441c-9a4c-cd841c0859e7",
   "metadata": {},
   "source": [
    "# Notes on Gradient Descent and Its Variants\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm used to minimize a differentiable cost (or loss) function. The basic idea is to move in the direction opposite to the gradient of the function at the current point. This document covers:\n",
    "\n",
    "1. **Gradient Descent (GD) – The Basic Concept** | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2033%20Gradient%20Descent)\n",
    "2. **Batch Gradient Descent (BGD)** | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2034%20Types%20of%20Gradient%20Descent)\n",
    "3. **Stochastic Gradient Descent (SGD)** | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2034%20Types%20of%20Gradient%20Descent)\n",
    "4. **Mini-Batch Gradient Descent (MBGD)** | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2034%20Types%20of%20Gradient%20Descent)\n",
    "5. **Python Code Examples**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Gradient Descent (GD) – The Basic Concept\n",
    "\n",
    "At each iteration, the update for the parameters \\( w \\) is given by:\n",
    "\n",
    "$$\n",
    "w_{i+1} = w_i - \\alpha \\, \\nabla J(w_i)\n",
    "$$\n",
    "\n",
    "- **\\( w_i \\)**: The parameter vector at iteration \\( i \\).\n",
    "- **\\( \\alpha \\)**: The learning rate (step size).\n",
    "- **\\( \\nabla J(w_i) \\)**: The gradient of the cost function \\( J \\) at \\( w_i \\).\n",
    "\n",
    "This formula represents the idea of moving in the direction of steepest descent (i.e., the negative gradient).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Batch Gradient Descent (BGD)\n",
    "\n",
    "**Concept:**  \n",
    "In batch gradient descent, the gradient is calculated using the entire training dataset. That is, every update uses all \\( m \\) training examples.\n",
    "\n",
    "### Update Formula\n",
    "\n",
    "$$\n",
    "w_{i+1} = w_i - \\alpha \\, \\frac{1}{m} \\sum_{j=1}^{m} \\nabla J^{(j)}(w_i)\n",
    "$$\n",
    "\n",
    "- Here, \\( \\nabla J^{(j)}(w_i) \\) is the gradient computed from the \\( j \\)-th training example.\n",
    "- The cost function \\( J(w) \\) is often defined as:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2m} \\sum_{j=1}^{m} \\left( h_w(x^{(j)}) - y^{(j)} \\right)^2\n",
    "$$\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Smooth and stable convergence.\n",
    "- Precise gradient estimation.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Computationally expensive for large datasets (one update per epoch).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Concept:**  \n",
    "In stochastic gradient descent, the parameters are updated for each training example, rather than the entire dataset at once.\n",
    "\n",
    "### Update Formula\n",
    "\n",
    "$$\n",
    "w_{i+1} = w_i - \\alpha \\, \\nabla J^{(j)}(w_i)\n",
    "$$\n",
    "\n",
    "- \\( j \\) is randomly chosen from the training examples.\n",
    "- There are \\( m \\) updates in one epoch if there are \\( m \\) training samples.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Much faster per update.\n",
    "- Can help escape local minima due to noise.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- More noisy updates; the loss function may oscillate.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Mini-Batch Gradient Descent (MBGD)\n",
    "\n",
    "**Concept:**  \n",
    "Mini-batch gradient descent is a compromise between BGD and SGD. The dataset is divided into smaller batches, and the gradient is computed on each batch.\n",
    "\n",
    "### Update Formula\n",
    "\n",
    "$$\n",
    "w_{i+1} = w_i - \\alpha \\, \\frac{1}{b} \\sum_{j=1}^{b} \\nabla J^{(j)}(w_i)\n",
    "$$\n",
    "\n",
    "- **\\( b \\)**: Mini-batch size (where \\( 1 < b < m \\)).\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Benefits from vectorized operations (efficient on GPUs).\n",
    "- Reduces the variance of parameter updates compared to SGD.\n",
    "- More frequent updates than BGD, yet smoother than pure SGD.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- The mini-batch size \\( b \\) is a hyperparameter that must be chosen carefully.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Comparison Table\n",
    "\n",
    "Below is an HTML table summarizing the key differences:\n",
    "\n",
    "<table border=\"1\" cellspacing=\"0\" cellpadding=\"5\">\n",
    "  <tr>\n",
    "    <th>Variant</th>\n",
    "    <th>Data Used per Update</th>\n",
    "    <th>Updates per Epoch</th>\n",
    "    <th>Pros</th>\n",
    "    <th>Cons</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Batch Gradient Descent</td>\n",
    "    <td>Entire dataset</td>\n",
    "    <td>1</td>\n",
    "    <td>Stable; accurate gradient</td>\n",
    "    <td>Slow for large datasets</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Stochastic Gradient Descent</td>\n",
    "    <td>Single sample</td>\n",
    "    <td>\\( m \\)</td>\n",
    "    <td>Fast; can escape local minima</td>\n",
    "    <td>Noisy updates; more oscillations</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mini-Batch Gradient Descent</td>\n",
    "    <td>Subset of data</td>\n",
    "    <td>\\( \\frac{m}{b} \\)</td>\n",
    "    <td>Efficient; balanced noise and stability</td>\n",
    "    <td>Needs mini-batch size tuning</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Python Code Examples\n",
    "\n",
    "### Example: Batch Gradient Descent for Linear Regression\n",
    "\n",
    "Below is a simple Python implementation of batch gradient descent applied to linear regression:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Hypothesis: h_w(x) = w0 + w1 * x\n",
    "# Cost Function: J(w) = (1/(2*m)) * sum((h_w(x) - y)^2)\n",
    "\n",
    "def compute_cost(X, y, w):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(w)\n",
    "    cost = (1/(2*m)) * np.sum((predictions - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def batch_gradient_descent(X, y, w_init, alpha, num_iters):\n",
    "    w = w_init.copy()\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        predictions = X.dot(w)\n",
    "        gradient = (1/m) * X.T.dot(predictions - y)\n",
    "        w = w - alpha * gradient\n",
    "        \n",
    "        cost = compute_cost(X, y, w)\n",
    "        cost_history.append(cost)\n",
    "        print(f\"Iteration {i+1}: Cost {cost}, Weights {w}\")\n",
    "    \n",
    "    return w, cost_history\n",
    "\n",
    "# Example usage:\n",
    "# X should include a column of ones for the intercept term.\n",
    "X = np.array([\n",
    "    [1, 1.0],\n",
    "    [1, 2.0],\n",
    "    [1, 3.0],\n",
    "    [1, 4.0]\n",
    "])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "w_init = np.zeros(X.shape[1])\n",
    "alpha = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "w_final, cost_history = batch_gradient_descent(X, y, w_init, alpha, num_iters)\n",
    "print(\"Final weights:\", w_final)\n",
    "```\n",
    "\n",
    "### Example: Stochastic Gradient Descent for Linear Regression\n",
    "\n",
    "Here’s an implementation of SGD:\n",
    "\n",
    "```python\n",
    "def stochastic_gradient_descent(X, y, w_init, alpha, num_epochs):\n",
    "    w = w_init.copy()\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in indices:\n",
    "            xi = X[i, :].reshape(1, -1)  # Single training example\n",
    "            yi = y[i]\n",
    "            prediction = xi.dot(w)\n",
    "            gradient = xi.T * (prediction - yi)\n",
    "            w = w - alpha * gradient.flatten()\n",
    "        \n",
    "        cost = compute_cost(X, y, w)\n",
    "        cost_history.append(cost)\n",
    "        print(f\"Epoch {epoch+1}: Cost {cost}, Weights {w}\")\n",
    "    \n",
    "    return w, cost_history\n",
    "\n",
    "# Example usage:\n",
    "w_init = np.zeros(X.shape[1])\n",
    "alpha = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "w_final_sgd, cost_history_sgd = stochastic_gradient_descent(X, y, w_init, alpha, num_epochs)\n",
    "print(\"Final weights (SGD):\", w_final_sgd)\n",
    "```\n",
    "\n",
    "### Example: Mini-Batch Gradient Descent\n",
    "\n",
    "Below is an implementation of mini-batch gradient descent:\n",
    "\n",
    "```python\n",
    "def mini_batch_gradient_descent(X, y, w_init, alpha, num_epochs, batch_size):\n",
    "    w = w_init.copy()\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            predictions = X_batch.dot(w)\n",
    "            gradient = (1/len(y_batch)) * X_batch.T.dot(predictions - y_batch)\n",
    "            w = w - alpha * gradient\n",
    "        \n",
    "        cost = compute_cost(X, y, w)\n",
    "        cost_history.append(cost)\n",
    "        print(f\"Epoch {epoch+1}: Cost {cost}, Weights {w}\")\n",
    "    \n",
    "    return w, cost_history\n",
    "\n",
    "# Example usage:\n",
    "w_init = np.zeros(X.shape[1])\n",
    "alpha = 0.01\n",
    "num_epochs = 50\n",
    "batch_size = 2\n",
    "\n",
    "w_final_mbgd, cost_history_mbgd = mini_batch_gradient_descent(X, y, w_init, alpha, num_epochs, batch_size)\n",
    "print(\"Final weights (Mini-Batch):\", w_final_mbgd)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "- **Batch Gradient Descent:** Uses the full dataset to compute the gradient; stable but can be slow.\n",
    "- **Stochastic Gradient Descent:** Updates parameters using one sample at a time; faster but noisier.\n",
    "- **Mini-Batch Gradient Descent:** A compromise that uses a subset (mini-batch) of the data; efficient and often the best choice in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
