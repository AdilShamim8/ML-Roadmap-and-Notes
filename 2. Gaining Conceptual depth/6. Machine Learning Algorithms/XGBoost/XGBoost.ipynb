{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671e234c-d980-47dd-bc22-f4f602a148c7",
   "metadata": {},
   "source": [
    "# XGBoost Study Note\n",
    "\n",
    "This note provides an overview of XGBoost, its underlying formulas, and a step-by-step guide to implementing it in Python.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**XGBoost** (eXtreme Gradient Boosting) is a powerful, scalable ensemble method based on gradient boosting decision trees. It is widely used for classification and regression tasks due to its performance and speed.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Ensemble Learning:** Combines the outputs of multiple learners.\n",
    "- **Gradient Boosting:** Sequentially adds models to correct errors made by previous models.\n",
    "- **Decision Trees:** The building blocks used by XGBoost.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Theoretical Foundations\n",
    "\n",
    "### 2.1 Objective Function\n",
    "\n",
    "XGBoost aims to minimize a regularized objective function that balances training loss and model complexity.\n",
    "\n",
    "The general objective function is defined as:\n",
    "\n",
    "$$\n",
    "Obj(t) = \\sum_{i=1}^{n} l\\left(y_i, \\hat{y}_i^{(t)}\\right) + \\sum_{k=1}^{t} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( l(y_i, \\hat{y}_i^{(t)}) \\) is a differentiable loss function (e.g., logistic loss for classification).  \n",
    "- \\( \\Omega(f) \\) is the regularization term for the complexity of the model.\n",
    "\n",
    "### 2.2 Regularization Term\n",
    "\n",
    "The regularization for each tree \\( f \\) is given by:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( T \\) is the number of leaves in the tree.\n",
    "- \\( w_j \\) are the leaf weights.\n",
    "- \\( \\gamma \\) and \\( \\lambda \\) are regularization parameters that control overfitting.\n",
    "\n",
    "### 2.3 Second Order Taylor Approximation\n",
    "\n",
    "At each boosting iteration, the loss is approximated using a second order Taylor expansion:\n",
    "\n",
    "$$\n",
    "L^{(t)} \\approx \\sum_{i=1}^{n} \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial \\hat{y}_i^{(t-1)}} \\) is the first derivative (gradient).  \n",
    "- \\( h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{\\partial \\hat{y}_i^{(t-1) 2}} \\) is the second derivative (Hessian).\n",
    "\n",
    "### 2.4 Split Finding: Gain Calculation\n",
    "\n",
    "To decide the best split, XGBoost calculates the **gain** for a potential split as:\n",
    "\n",
    "$$\n",
    "Gain = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( G_L \\) and \\( H_L \\) are the sum of gradients and Hessians for the left node.  \n",
    "- \\( G_R \\) and \\( H_R \\) are the sum of gradients and Hessians for the right node.  \n",
    "- \\( \\lambda \\) and \\( \\gamma \\) are regularization parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step-by-Step Python Implementation\n",
    "\n",
    "### 3.1 Environment Setup\n",
    "\n",
    "Install XGBoost along with other required packages:\n",
    "\n",
    "```bash\n",
    "pip install xgboost pandas numpy scikit-learn matplotlib\n",
    "```\n",
    "\n",
    "### 3.2 Data Preparation\n",
    "\n",
    "Load and prepare your dataset using Pandas. Here’s a basic example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: Load your dataset (replace with your own file or data)\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Assume 'target' is the column to predict and the rest are features\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### 3.3 Building the XGBoost Model\n",
    "\n",
    "For a classification task, use `XGBClassifier`. For regression, use `XGBRegressor`.\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the classifier with chosen hyperparameters\n",
    "model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 3.4 Making Predictions and Evaluation\n",
    "\n",
    "After training, make predictions and evaluate the model’s performance:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### 3.5 Hyperparameter Tuning\n",
    "\n",
    "Use grid search to find the best hyperparameters:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the classifier\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBClassifier(objective='binary:logistic', random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "### 3.6 Feature Importance and Model Interpretation\n",
    "\n",
    "Plot feature importance to understand which features contribute most to the predictions:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Plot the feature importance\n",
    "plot_importance(model)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "For a deeper interpretation, consider using SHAP values:\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "# Create a TreeExplainer object for the model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advanced Topics\n",
    "\n",
    "- **Regularization Techniques:** Fine-tune the \\( \\lambda \\) and \\( \\gamma \\) parameters to prevent overfitting.\n",
    "- **Handling Imbalanced Data:** Adjust parameters like `scale_pos_weight` for imbalanced classes.\n",
    "- **Custom Objective Functions:** Define your own loss function if the built-in options don’t suit your problem.\n",
    "- **Distributed Training:** Explore distributed XGBoost for very large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "This note has covered the theoretical underpinnings of XGBoost, including its objective function and key formulas, and has provided a complete, step-by-step guide to implementing it in Python. Experiment with these examples and adjust parameters as needed for your specific dataset.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*References and further reading can be found in the official [XGBoost Documentation](https://xgboost.readthedocs.io/).*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
