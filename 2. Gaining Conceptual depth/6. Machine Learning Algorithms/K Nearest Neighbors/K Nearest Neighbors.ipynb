{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cbb978-04e3-4e42-81f7-457d2a44b3d4",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors (KNN) - Notes\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "K Nearest Neighbors (KNN) is a **non-parametric**, **lazy learning** algorithm used for both classification and regression tasks. Its simplicity lies in the idea that similar data points exist close to each other in the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Core Concepts\n",
    "\n",
    "### 2.1. How KNN Works\n",
    "\n",
    "1. **Distance Calculation:**  \n",
    "   KNN calculates the distance between a new data point and all points in the training set.  \n",
    "2. **Selecting Neighbors:**  \n",
    "   The algorithm picks the **k** closest points (neighbors) to make a prediction.  \n",
    "3. **Prediction:**  \n",
    "   - **Classification:** The class is determined by a majority vote among the **k** nearest neighbors.  \n",
    "   - **Regression:** The prediction is typically the average (or weighted average) of the values of the **k** neighbors.\n",
    "\n",
    "### 2.2. Distance Metrics\n",
    "\n",
    "The most commonly used distance metric in KNN is the **Euclidean distance**. It is defined using the following formula in HTML:\n",
    "\n",
    "<p>\n",
    "  Euclidean Distance: d(x, y) = &radic;(&sum;<sub>i=1</sub><sup>n</sup> (x<sub>i</sub> - y<sub>i</sub>)<sup>2</sup>)\n",
    "</p>\n",
    "\n",
    "Other distance metrics include Manhattan distance, Minkowski distance, etc.\n",
    "\n",
    "### 2.3. Choosing the Value of K\n",
    "\n",
    "- **Small k:**  \n",
    "  Can be sensitive to noise and outliers.\n",
    "- **Large k:**  \n",
    "  Leads to smoother decision boundaries but might underfit the data.\n",
    "\n",
    "Cross-validation is often used to select an optimal value for **k**.\n",
    "\n",
    "### 2.4. Data Preprocessing\n",
    "\n",
    "Since KNN uses distance measures, it's important to **normalize** or **standardize** features so that each feature contributes equally to the distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Python Implementation Example | [Sklearn KNN Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "Below is a simple Python implementation of the KNN algorithm from scratch.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two vectors.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def knn_predict(X_train, y_train, x_test, k=3):\n",
    "    \"\"\"\n",
    "    Predict the class of x_test based on the majority vote of k nearest neighbors.\n",
    "    \"\"\"\n",
    "    # Calculate distances from x_test to all training samples\n",
    "    distances = [euclidean_distance(x_test, x_train) for x_train in X_train]\n",
    "    \n",
    "    # Get the indices of the k nearest neighbors\n",
    "    k_indices = np.argsort(distances)[:k]\n",
    "    \n",
    "    # Extract the labels of the k nearest neighbors\n",
    "    k_nearest_labels = [y_train[i] for i in k_indices]\n",
    "    \n",
    "    # Determine the most common class label\n",
    "    most_common = Counter(k_nearest_labels).most_common(1)\n",
    "    return most_common[0][0]\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample training data: two features per sample\n",
    "    X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])\n",
    "    y_train = np.array([0, 0, 0, 1, 1])\n",
    "    \n",
    "    # New data point to classify\n",
    "    x_test = np.array([3, 3])\n",
    "    \n",
    "    # Predict the class using k=3\n",
    "    prediction = knn_predict(X_train, y_train, x_test, k=3)\n",
    "    print(\"Predicted class:\", prediction)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Advanced Considerations\n",
    "\n",
    "### 4.1. Optimization Techniques\n",
    "\n",
    "- **KD-Trees & Ball Trees:**  \n",
    "  These data structures help speed up the search for nearest neighbors, especially in higher-dimensional datasets.\n",
    "\n",
    "### 4.2. Dealing with High Dimensionality\n",
    "\n",
    "- **Curse of Dimensionality:**  \n",
    "  In high-dimensional spaces, the distance between data points can become less meaningful. Dimensionality reduction techniques such as Principal Component Analysis (PCA) can be useful.\n",
    "\n",
    "### 4.3. Handling Imbalanced Data\n",
    "\n",
    "- Consider weighting the votes of neighbors inversely by their distance to reduce the influence of noisy, faraway points.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "- **KNN** is easy to understand and implement but can be computationally expensive for large datasets.\n",
    "- Proper **preprocessing** (scaling and normalization) is crucial.\n",
    "- The choice of **k** and **distance metric** can significantly affect performance.\n",
    "- Advanced techniques like **KD-Trees** can optimize the neighbor search process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
