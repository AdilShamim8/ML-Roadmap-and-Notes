{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b06b203-2e5b-428a-a66a-697c7e5d9043",
   "metadata": {},
   "source": [
    "> # `Scalars`\n",
    "> What are scalars?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefd969-6c8f-4ec0-afe8-356e3ae4eb8d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "A **scalar** is the simplest object in linear algebra.\n",
    "\n",
    "* **Definition:** A scalar is just a single number.\n",
    "* **Examples:** (5), (-3.14), (\\sqrt{2}), or (0).\n",
    "* **Notation:** Usually written with lowercase letters like (a, b, c).\n",
    "\n",
    "### How scalars differ from other objects\n",
    "\n",
    "* **Scalar vs Vector:**\n",
    "  A vector has **magnitude + direction** (like an arrow in space). A scalar is just **magnitude** (no direction).\n",
    "* **Scalar vs Matrix:**\n",
    "  A matrix is a rectangular grid of numbers. A scalar is just one of those numbers by itself.\n",
    "\n",
    "### Why scalars matter in ML\n",
    "\n",
    "* Learning rate in gradient descent = scalar.\n",
    "* Weights of a neuron = often scaled (multiplied) by scalars.\n",
    "* Loss value = scalar (just one number representing error).\n",
    "\n",
    "Think of scalars as the **atoms** of linear algebra — the smallest building blocks.\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b28a8-1849-469e-9988-83520398212d",
   "metadata": {},
   "source": [
    "> # `Vectors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77048fca-bc31-4b47-9368-101be095bb7c",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
    "  <title></title>\n",
    "\n",
    "  <!-- MathJax config: allow $...$ and $$...$$ -->\n",
    "  <script>\n",
    "    window.MathJax = {\n",
    "      tex: {\n",
    "        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
    "        displayMath: [['$$','$$'], ['\\\\[','\\\\]']]\n",
    "      },\n",
    "      options: {\n",
    "        skipHtmlTags: ['script','noscript','style','textarea','pre','code']\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\" async></script>\n",
    "\n",
    "  <style>\n",
    "    :root{\n",
    "      --bg: #f7fbff;\n",
    "      --card: #ffffff;\n",
    "      --muted: #566274;\n",
    "      --accent: #0b63d6;\n",
    "      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, \"Roboto Mono\", \"Courier New\", monospace;\n",
    "    }\n",
    "    html,body{height:100%}\n",
    "    body{\n",
    "      margin:24px;\n",
    "      font-family: Inter, system-ui, -apple-system, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
    "      background: linear-gradient(180deg, #ffffff 0%, var(--bg) 100%);\n",
    "      color: #0b2030;\n",
    "      line-height:1.55;\n",
    "      -webkit-font-smoothing:antialiased;\n",
    "    }\n",
    "    .container{\n",
    "      max-width:980px;\n",
    "      margin:0 auto;\n",
    "      background:var(--card);\n",
    "      border-radius:12px;\n",
    "      padding:22px;\n",
    "      box-shadow:0 8px 30px rgba(11,30,45,0.06);\n",
    "      border:1px solid rgba(11,99,214,0.04);\n",
    "    }\n",
    "    header{\n",
    "      display:flex;\n",
    "      align-items:baseline;\n",
    "      justify-content:space-between;\n",
    "      gap:14px;\n",
    "      margin-bottom:14px;\n",
    "    }\n",
    "    header h1{margin:0; font-size:20px;}\n",
    "    header p{margin:0; color:var(--muted); font-size:13px;}\n",
    "    details{margin:6px 0;}\n",
    "    summary{\n",
    "      cursor:pointer;\n",
    "      font-weight:700;\n",
    "      font-size:15px;\n",
    "      list-style:none;\n",
    "      outline:none;\n",
    "    }\n",
    "    details > summary::-webkit-details-marker { display: none; }\n",
    "    details[open] > summary::after { content: \"▾\"; padding-left:8px; color:var(--muted); }\n",
    "    details > summary::after { content: \"▸\"; padding-left:8px; color:var(--muted); }\n",
    "    section{margin:12px 0;}\n",
    "    h2{font-size:16px; margin:8px 0 6px;}\n",
    "    p{margin:6px 0;}\n",
    "    .example{background:#f2f8ff; padding:10px; border-radius:8px; border:1px solid rgba(11,99,214,0.06);}\n",
    "    pre{background:#f7fbff; padding:10px; border-radius:8px; overflow:auto; border:1px solid rgba(11,30,45,0.03);}\n",
    "    code{font-family:var(--mono); background:#eef6ff; padding:2px 6px; border-radius:6px;}\n",
    "    ul, ol { margin:8px 0 8px 20px; }\n",
    "    .hint{font-size:13px; color:var(--muted);}\n",
    "    hr{border:0; border-top:1px solid rgba(11,30,45,0.06); margin:18px 0;}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <!-- <header>\n",
    "      <h1>Vector Notes</h1>\n",
    "      <p class=\"hint\">HTML + MathJax — display math uses <code>$$...$$</code></p>\n",
    "    </header> -->\n",
    "    <details open>\n",
    "      <summary>Click to expand</summary>\n",
    "      <section>\n",
    "        <h2>1. What are Vectors</h2>\n",
    "        <p><strong>Definition (short):</strong> an ordered list of numbers representing a point or arrow in $ \\mathbb{R}^n $. </p>\n",
    "        <p><strong>Math (stepwise):</strong></p>\n",
    "        <ol>\n",
    "          <li>A vector $ \\mathbf{v} \\in \\mathbb{R}^n $ written $ \\mathbf{v} = (v_1, v_2, \\dots, v_n) $.</li>\n",
    "          <li>Components $v_i$ are scalars.</li>\n",
    "          <li>Vector operations are defined componentwise (addition, scalar multiplication).</li>\n",
    "        </ol>\n",
    "        <p><strong>Geometry:</strong> think of $ \\mathbf{v} $ as an arrow from the origin to the point $(v_1, \\dots, v_n)$.</p>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>NumPy snippet</strong></p>\n",
    "          <pre><code>import numpy as np\n",
    "v = np.array([2.0, -1.0, 0.5])  # a vector in R^3\n",
    "</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise (try):</strong> Create two 3D vectors and print their components, shapes and types. <span class=\"hint\">Hint: use <code>np.array</code> and <code>.shape</code>.</span></p>\n",
    "        <p><strong>Pitfalls / summary:</strong> Vectors are not lists in math — their order matters and operations assume same dimension.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>2. Row Vector and Column Vector</h2>\n",
    "        <p><strong>Definition:</strong> same components but different orientation.</p>\n",
    "        <ul>\n",
    "          <li>Row: $ \\mathbf{r} = [r_1\\ r_2\\ \\dots\\ r_n] $ (1×n)</li>\n",
    "          <li>Column:\n",
    "            <div class=\"example\">\n",
    "              $$\n",
    "              \\mathbf{c} = \\begin{bmatrix}\n",
    "                c_1 \\\\\n",
    "                \\vdots \\\\\n",
    "                c_n\n",
    "              \\end{bmatrix}\n",
    "              \\quad (n\\times 1)\n",
    "              $$\n",
    "            </div>\n",
    "          </li>\n",
    "        </ul>\n",
    "        <p><strong>Why it matters:</strong> matrix multiplication rules require correct orientation.</p>\n",
    "        <p><strong>Stepwise:</strong></p>\n",
    "        <ol>\n",
    "          <li>Column vectors are the standard in linear algebra: treat $ \\mathbf{x}\\in\\mathbb{R}^n $ as $n\\times 1$.</li>\n",
    "          <li>A row vector is transpose: $ \\mathbf{r} = \\mathbf{c}^\\top $.</li>\n",
    "        </ol>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>NumPy snippet</strong></p>\n",
    "          <pre><code>x = np.array([1,2,3])        # 1D array (behaves like row/col depending on context)\n",
    "x_col = x.reshape((3,1))     # explicit column vector\n",
    "x_row = x.reshape((1,3))     # explicit row vector\n",
    "</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> reshape a vector into column and row and compute <code>x_col.T @ x_col</code>. <span class=\"hint\">Hint: <code>.reshape((n,1))</code> and <code>.T</code>.</span></p>\n",
    "        <p><strong>Pitfall:</strong> 1D <code>np.array</code> has no explicit row/col until you reshape — be careful with broadcasting.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>3. Distance from Origin</h2>\n",
    "        <p><strong>Definition:</strong> length (Euclidean norm) of vector $ \\mathbf{v} $:</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_i v_i^2}\n",
    "          $$\n",
    "        </div>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>NumPy</strong></p>\n",
    "          <pre><code>np.linalg.norm(v)           # default is L2 norm\n",
    "np.sqrt(np.dot(v,v))        # equivalent</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> compute norm for <code>v=[3,4]</code> and confirm it equals 5. <span class=\"hint\">Pythagorean theorem.</span></p>\n",
    "        <p><strong>Pitfall:</strong> Norm can be other <code>p</code>-norms (see later); specify when needed.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>4. Euclidean Distance between 2 vectors</h2>\n",
    "        <p><strong>Definition:</strong></p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          d(\\mathbf{u},\\mathbf{v}) = \\|\\mathbf{u}-\\mathbf{v}\\|_2\n",
    "          $$\n",
    "        </div>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>NumPy</strong></p>\n",
    "          <pre><code>dist = np.linalg.norm(u - v)</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> compute distance between <code>[1,0,0]</code> and <code>[0,1,0]</code>. <span class=\"hint\">Answer: $\\sqrt{2}$.</span></p>\n",
    "        <p><strong>Pitfall:</strong> Don’t forget to center or scale features before computing distances in ML (units matter).</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>5. Scalar + Vector (Shifting)</h2>\n",
    "        <p><strong>Definition:</strong> adding the same scalar to every component (less common); more commonly vector + vector (broadcasting).</p>\n",
    "        <p><strong>Math (broadcast):</strong> when allowed,\n",
    "          $$\n",
    "          \\mathbf{v} + c = (v_1 + c, v_2 + c, \\dots)\n",
    "          $$\n",
    "        </p>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>v + 3    # adds 3 to every component (broadcast)\n",
    "# But usually shift by vector:\n",
    "v + np.array([1,2,3])</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> center a dataset <code>X</code> by subtracting its column means. <span class=\"hint\">Use <code>X - X.mean(axis=0)</code>.</span></p>\n",
    "        <p><strong>Pitfall:</strong> Broadcasting can silently do unwanted shifts if shapes differ.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>6. Scalar × Vector (Scaling)</h2>\n",
    "        <p><strong>Definition:</strong> multiply each component by scalar $a$: $a\\mathbf{v}$.</p>\n",
    "        <p><strong>Math:</strong>\n",
    "          $$\n",
    "          a\\mathbf{v} = (a v_1, \\dots, a v_n)\n",
    "          $$\n",
    "        </p>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>2.5 * v</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> scale <code>v=[2, -1]</code> by -3; check new length equals 3× original. <span class=\"hint\">Use <code>np.linalg.norm</code>.</span></p>\n",
    "        <p><strong>Pitfall:</strong> Mistaking scalar multiplication for elementwise product of two vectors.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>7. Vector + Vector / − Vector</h2>\n",
    "        <p><strong>Definition:</strong> componentwise addition/subtraction.</p>\n",
    "        <p><strong>Math:</strong>\n",
    "          $$\n",
    "          \\mathbf{u}+\\mathbf{v} = (u_1+v_1, \\dots)\n",
    "          $$\n",
    "        </p>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>u + v\n",
    "u - v</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> draw (mentally or plot) <code>u=[1,0]</code>, <code>v=[0,2]</code>; compute <code>u+v</code> and verify head-to-tail geometry.</p>\n",
    "        <p><strong>Pitfall:</strong> Vectors must have same dimension.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>8. Dot Product of 2 vectors</h2>\n",
    "        <p><strong>Definition:</strong>\n",
    "          $$\n",
    "          \\mathbf{u}\\cdot\\mathbf{v} = \\sum_i u_i v_i\n",
    "          $$\n",
    "        </p>\n",
    "        <p>Result is a scalar. Equivalently, $\\mathbf{u}^\\top \\mathbf{v}$ (for column vectors).</p>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>np.dot(u, v)    # or u @ v</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> compute <code>dot([1,2,3],[4,0,-1])</code> by hand and check code. <span class=\"hint\">1*4 + 2*0 + 3*(-1) = 1.</span></p>\n",
    "        <p><strong>Pitfall:</strong> For multi-dimensional arrays, <code>np.dot</code>/<code>@</code> have broadcasting rules—use shapes carefully.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>9. Angle between 2 vectors</h2>\n",
    "        <p><strong>Formula:</strong></p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\cos\\theta = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\|\\mathbf{u}\\|\\,\\|\\mathbf{v}\\|}, \\qquad\n",
    "          \\theta = \\arccos\\Big(\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\|\\mathbf{u}\\|\\,\\|\\mathbf{v}\\|}\\Big)\n",
    "          $$\n",
    "        </div>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>cos_theta = np.dot(u,v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "theta = np.arccos(np.clip(cos_theta, -1, 1))</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> check angle between <code>[1,0]</code> and <code>[0,1]</code> is $\\pi/2$. <span class=\"hint\">dot=0 → cos=0 → angle=90°.</span></p>\n",
    "        <p><strong>Pitfall:</strong> Floating errors can push cos slightly out of [-1,1]; use <code>np.clip</code>.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>10. Unit Vectors</h2>\n",
    "        <p><strong>Definition:</strong> vector of length 1. Unit vector in direction $\\mathbf{v}$:</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n",
    "          $$\n",
    "        </div>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>v_hat = v / np.linalg.norm(v)</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> convert <code>[3,4]</code> to a unit vector and check norm = 1. <span class=\"hint\">norm is 5.</span></p>\n",
    "        <p><strong>Pitfall:</strong> dividing by zero if $\\mathbf{v}=\\mathbf{0}$.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>11. Projection of a Vector</h2>\n",
    "        <p><strong>Definitions:</strong></p>\n",
    "        <p>Scalar (length of projection):\n",
    "          $$\n",
    "          \\mathrm{comp}_{\\mathbf{v}}(\\mathbf{u}) = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\|\\mathbf{v}\\|}\n",
    "          $$\n",
    "        </p>\n",
    "        <p>Vector projection (shadow):\n",
    "          $$\n",
    "          \\mathrm{proj}_{\\mathbf{v}}(\\mathbf{u}) = \\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\|\\mathbf{v}\\|^2}\\,\\mathbf{v}\n",
    "          $$\n",
    "        </p>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>proj_u_on_v = (np.dot(u,v) / np.dot(v,v)) * v</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> project <code>u=[1,2,2]</code> onto <code>v=[2,0,1]</code> and confirm algebraically and with code.</p>\n",
    "        <p><strong>Pitfall:</strong> If <code>v</code> is zero or near-zero, projection is undefined / unstable.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>12. Basis Vectors</h2>\n",
    "        <p><strong>Definition:</strong> a set of vectors $\\{b_1,\\dots,b_k\\}$ that are linearly independent and span a space. If they span $\\mathbb{R}^n$ with $k=n$, they form a basis.</p>\n",
    "        <p><strong>Stepwise:</strong></p>\n",
    "        <ol>\n",
    "          <li>To express $\\mathbf{x}$ in basis $B$, find coordinates $\\mathbf{c}$ such that $\\mathbf{x} = B \\mathbf{c}$ where $B$ has basis vectors as columns.</li>\n",
    "          <li>If $B$ is invertible ($n\\times n$), $\\mathbf{c} = B^{-1}\\mathbf{x}$.</li>\n",
    "        </ol>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>B = np.column_stack([b1,b2,b3])    # basis as columns\n",
    "coords = np.linalg.solve(B, x)     # coordinates in basis B</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> Verify standard basis <code>e1,e2</code> spans $\\mathbb{R}^2$. Express <code>[3,4]</code> in that basis.</p>\n",
    "        <p><strong>Pitfall:</strong> Basis must be independent — redundant vectors are not a basis.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>13. Equation of a Line in n-D</h2>\n",
    "        <p><strong>Parametric form (most general):</strong></p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\mathbf{x}(t) = \\mathbf{p} + t\\mathbf{d},\\quad t\\in\\mathbb{R}\n",
    "          $$\n",
    "        </div>\n",
    "        <p>where $\\mathbf{p}$ is a point and $\\mathbf{d}$ a direction vector.</p>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>p = np.array([1,2,3])\n",
    "q = np.array([3,4,6])\n",
    "d = q - p\n",
    "# point on line for t=0.5\n",
    "x_t = p + 0.5 * d</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> parametric equation of line through (1,0,0) and (0,1,0).</p>\n",
    "        <p><strong>Pitfall:</strong> In dimensions &gt;2, lines are 1D subspaces + translation — don’t confuse with planes (2D).</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>14. Vector Norms (general)</h2>\n",
    "        <p><strong>Definition (p-norm):</strong></p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\|\\mathbf{v}\\|_p = \\left(\\sum_i |v_i|^p\\right)^{1/p}\n",
    "          $$\n",
    "        </div>\n",
    "        <p>Special cases: $p=2$ (Euclidean), $p=1$ (Manhattan), $p=\\infty$ (max).</p>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>np.linalg.norm(v, ord=2)   # L2\n",
    "np.linalg.norm(v, ord=1)   # L1\n",
    "np.linalg.norm(v, ord=np.inf)  # Linf</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> compute L1 and L2 norms for <code>[1,-2,2]</code>.</p>\n",
    "        <p><strong>Pitfall:</strong> Norms induce different geometry and hence different model behavior.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>15. Linear Independence</h2>\n",
    "        <p><strong>Definition:</strong> vectors $v_1,\\dots,v_k$ are independent if $c_1 v_1 + \\dots + c_k v_k = 0$ implies all $c_i=0$.</p>\n",
    "        <p><strong>Stepwise test:</strong></p>\n",
    "        <ol>\n",
    "          <li>Put vectors as columns into matrix $A$.</li>\n",
    "          <li>Compute rank: if <code>rank(A) = k</code>, independent; otherwise dependent.</li>\n",
    "        </ol>\n",
    "        <div class=\"example\">\n",
    "          <pre><code>A = np.column_stack([v1,v2,v3])\n",
    "np.linalg.matrix_rank(A)  # compare to number of vectors</code></pre>\n",
    "        </div>\n",
    "        <p><strong>Exercise:</strong> check independence of <code>v1=[1,0]</code>, <code>v2=[2,0]</code>. <span class=\"hint\">rank = 1 → dependent.</span></p>\n",
    "        <p><strong>Pitfall:</strong> Floating point noise may make near-dependent vectors show full rank — check condition numbers.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>16. Vector Spaces</h2>\n",
    "        <p><strong>Definition:</strong> a set $V$ with vector addition and scalar multiplication, closed under those operations and satisfying the vector space axioms.</p>\n",
    "        <p><strong>Common examples:</strong> $\\mathbb{R}^n$, subspaces (plane through origin), polynomial spaces.</p>\n",
    "        <p><strong>To verify a subspace $W \\subset V$:</strong></p>\n",
    "        <ol>\n",
    "          <li>Contains the zero vector.</li>\n",
    "          <li>Closed under addition.</li>\n",
    "          <li>Closed under scalar multiplication.</li>\n",
    "        </ol>\n",
    "        <p><strong>Exercise:</strong> show that the set of vectors orthogonal to a given vector $a$ forms a subspace. <span class=\"hint\">If $x\\cdot a = 0$ and $y\\cdot a = 0$ then $(x+y)\\cdot a = 0$ and $(\\lambda x)\\cdot a = 0$.</span></p>\n",
    "        <p><strong>Pitfall:</strong> an affine set (like a line not through the origin) is NOT a subspace.</p>\n",
    "      </section>\n",
    "    </details>\n",
    "    <hr/>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a532f49-ca1a-408d-917c-087c1cb9b12b",
   "metadata": {},
   "source": [
    "> # `Matrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498408e7-185e-4de7-941f-83e3ed88080d",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
    "  <title></title>\n",
    "\n",
    "  <!-- MathJax configuration: allow $...$ and $$...$$ -->\n",
    "  <script>\n",
    "    window.MathJax = {\n",
    "      tex: {\n",
    "        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
    "        displayMath: [['$$','$$'], ['\\\\[','\\\\]']]\n",
    "      },\n",
    "      options: {\n",
    "        skipHtmlTags: ['script','noscript','style','textarea','pre','code']\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\" async></script>\n",
    "\n",
    "  <style>\n",
    "    :root{\n",
    "      --bg:#fbfcfd;\n",
    "      --card:#ffffff;\n",
    "      --muted:#657786;\n",
    "      --accent:#0b63d6;\n",
    "      --mono: \"Courier New\", Courier, monospace;\n",
    "    }\n",
    "    html,body{height:100%}\n",
    "    body{\n",
    "      margin:24px;\n",
    "      font-family: Inter, ui-sans-serif, system-ui, -apple-system, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
    "      background:linear-gradient(180deg, #ffffff 0%, var(--bg) 100%);\n",
    "      color: #0b2030;\n",
    "      line-height:1.5;\n",
    "    }\n",
    "    .container{\n",
    "      max-width:900px;\n",
    "      margin:0 auto;\n",
    "      background:var(--card);\n",
    "      border-radius:12px;\n",
    "      padding:22px;\n",
    "      box-shadow:0 6px 20px rgba(13,30,45,0.08);\n",
    "      border:1px solid rgba(11,99,214,0.04);\n",
    "    }\n",
    "    header{\n",
    "      display:flex;\n",
    "      align-items:center;\n",
    "      gap:14px;\n",
    "      margin-bottom:18px;\n",
    "    }\n",
    "    header h1{\n",
    "      margin:0;\n",
    "      font-size:20px;\n",
    "      letter-spacing:-0.2px;\n",
    "    }\n",
    "    summary{cursor:pointer; font-weight:600}\n",
    "    details > summary::-webkit-details-marker { display: none; }\n",
    "    details[open] > summary::after { content: \"▾\"; padding-left:8px; color:var(--muted); }\n",
    "    details > summary::after { content: \"▸\"; padding-left:8px; color:var(--muted); }\n",
    "    section{margin:14px 0;}\n",
    "    h2{font-size:16px; margin:10px 0 8px;}\n",
    "    p{margin:6px 0;}\n",
    "    .example, .note { background:#f7fbff; padding:10px; border-radius:8px; border:1px solid rgba(11,99,214,0.06); font-size:14px; }\n",
    "    code{font-family:var(--mono); background:#f2f6fa; padding:2px 6px; border-radius:4px;}\n",
    "    hr{border:0; border-top:1px solid rgba(11,30,45,0.06); margin:18px 0;}\n",
    "    ol, ul { margin:8px 0 8px 20px; }\n",
    "    .small{font-size:13px; color:var(--muted);}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <!-- <header>\n",
    "      <h1>Matrix Notes — HTML + MathJax</h1>\n",
    "      <div class=\"small\">Rendered with MathJax (supports <code>$...$</code> and <code>$$...$$</code>)</div>\n",
    "    </header> -->\n",
    "    <details open>\n",
    "      <summary>Click to expand — Matrix Notes</summary>\n",
    "      <section>\n",
    "        <h2>1. <strong>What are Matrices?</strong></h2>\n",
    "        <p>A <strong>matrix</strong> is a rectangular array of numbers with rows and columns.</p>\n",
    "        <p>Notation: <code>A = [a_{ij}]</code>, where <code>a_{ij}</code> is the entry in row <code>i</code> and column <code>j</code>.</p>\n",
    "        <p>Dimensions: <code>m &times; n</code> (rows &times; columns).</p>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>Example:</strong></p>\n",
    "          <p>\n",
    "            $$\n",
    "            A =\n",
    "            \\begin{bmatrix}\n",
    "            1 & 2 & 3 \\\\\n",
    "            4 & 5 & 6\n",
    "            \\end{bmatrix}\n",
    "            \\quad\\text{is a }2\\times 3\\text{ matrix.}\n",
    "            $$\n",
    "          </p>\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>2. <strong>Types of Matrices</strong></h2>\n",
    "        <ul>\n",
    "          <li><strong>Square matrix:</strong> $n\\times n$ (same rows and columns).</li>\n",
    "          <li><strong>Row matrix:</strong> $1\\times n$ (only one row).</li>\n",
    "          <li><strong>Column matrix:</strong> $m\\times 1$ (only one column).</li>\n",
    "          <li><strong>Zero (null) matrix:</strong> all entries $=0$.</li>\n",
    "          <li><strong>Diagonal matrix:</strong> only diagonal entries may be nonzero.</li>\n",
    "          <li><strong>Identity matrix $I_n$:</strong> diagonal entries $=1$, others $=0$.</li>\n",
    "          <li><strong>Upper / Lower triangular:</strong> all entries below / above the main diagonal are zero.</li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>3. <strong>Orthogonal Matrices</strong></h2>\n",
    "        <p>A square matrix $Q$ is <strong>orthogonal</strong> if</p>\n",
    "        <p>\n",
    "          $$\n",
    "          Q^\\top Q = Q Q^\\top = I.\n",
    "          $$\n",
    "        </p>\n",
    "        <p>Equivalently, $Q^{-1} = Q^\\top$. Columns (and rows) of $Q$ form an orthonormal set. Orthogonal transforms preserve lengths and angles.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>4. <strong>Symmetric Matrices</strong></h2>\n",
    "        <p>A matrix $A$ is symmetric if</p>\n",
    "        <p>\n",
    "          $$\n",
    "          A = A^\\top.\n",
    "          $$\n",
    "        </p>\n",
    "        <p>Example:</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\begin{bmatrix}\n",
    "          1 & 2 \\\\\n",
    "          2 & 3\n",
    "          \\end{bmatrix}.\n",
    "          $$\n",
    "        </div>\n",
    "        <p>Symmetric matrices commonly appear as covariance matrices in ML and have real eigenvalues.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>5. <strong>Diagonal Matrices</strong></h2>\n",
    "        <p>A diagonal matrix has entries only on the main diagonal:</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          D =\n",
    "          \\begin{bmatrix}\n",
    "          2 & 0 & 0 \\\\\n",
    "          0 & 5 & 0 \\\\\n",
    "          0 & 0 & 7\n",
    "          \\end{bmatrix}.\n",
    "          $$\n",
    "        </div>\n",
    "        <p>Diagonal matrices are easy to work with (scaling each coordinate).</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>6. <strong>Matrix Equality</strong></h2>\n",
    "        <p>Two matrices $A$ and $B$ are equal if:</p>\n",
    "        <ol>\n",
    "          <li>They have the same dimensions, and</li>\n",
    "          <li>$a_{ij} = b_{ij}$ for every entry $(i,j)$.</li>\n",
    "        </ol>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>7. <strong>Scalar Operations on Matrices</strong></h2>\n",
    "        <p>For scalar $\\alpha$ and matrix $A=[a_{ij}]$:</p>\n",
    "        <ul>\n",
    "          <li>$\\alpha A = [\\alpha a_{ij}]$ (multiply each entry by $\\alpha$).</li>\n",
    "          <li>$A + B = [a_{ij}+b_{ij}]$ (entrywise) when $A$ and $B$ share dimensions.</li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>8. <strong>Matrix Addition and Subtraction</strong></h2>\n",
    "        <p>Addition/subtraction is element-wise and requires identical dimensions:</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          (A\\pm B)_{ij} = a_{ij}\\pm b_{ij}.\n",
    "          $$\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>9. <strong>Matrix Multiplication</strong></h2>\n",
    "        <p>If $A$ is $m\\times n$ and $B$ is $n\\times p$, then the product $AB$ is $m\\times p$ with entries</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          (AB)_{ij}=\\sum_{k=1}^{n} a_{ik}\\,b_{kj}.\n",
    "          $$\n",
    "        </div>\n",
    "        <p>Matrix multiplication is generally <strong>not</strong> commutative: $AB\\neq BA$ in general.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>10. <strong>Transpose</strong></h2>\n",
    "        <p>The transpose of $A$ is $A^\\top$ with</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          (A^\\top)_{ij} = A_{ji}.\n",
    "          $$\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>11. <strong>Determinant</strong></h2>\n",
    "        <p>The determinant is defined for square matrices. For a $2\\times 2$ matrix</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\det\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc.\n",
    "          $$\n",
    "        </div>\n",
    "        <p>A matrix $A$ is invertible exactly when $\\det(A)\\neq 0$.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>12. <strong>Minor and Cofactor</strong></h2>\n",
    "        <ul>\n",
    "          <li><strong>Minor</strong> $M_{ij}$: determinant of the submatrix obtained by deleting row $i$ and column $j$.</li>\n",
    "          <li><strong>Cofactor</strong> $C_{ij}$:\n",
    "            <div class=\"example\">\n",
    "              $$\n",
    "              C_{ij} = (-1)^{i+j} M_{ij}.\n",
    "              $$\n",
    "            </div>\n",
    "          </li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>13. <strong>Adjugate (Classical Adjoint)</strong></h2>\n",
    "        <p>The adjugate (classical adjoint) of $A$ is the transpose of the cofactor matrix:</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\operatorname{adj}(A) = \\big(C_{ij}\\big)^\\top.\n",
    "          $$\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>14. <strong>Inverse</strong></h2>\n",
    "        <p>If $A$ is invertible ($\\det(A)\\neq 0$), then</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          A^{-1}=\\frac{1}{\\det(A)}\\,\\operatorname{adj}(A).\n",
    "          $$\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>15. <strong>Rank</strong></h2>\n",
    "        <p>The <strong>rank</strong> of $A$ is the dimension of the column space (same as row space dimension). It equals the maximum number of linearly independent rows or columns.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>16. <strong>Column Space and Null Space</strong></h2>\n",
    "        <ul>\n",
    "          <li><strong>Column space (range):</strong> $\\operatorname{Col}(A)=\\operatorname{span}\\{\\text{columns of }A\\}$.</li>\n",
    "          <li><strong>Null space (kernel):</strong> $\\operatorname{Null}(A)=\\{\\mathbf{x}:A\\mathbf{x}=\\mathbf{0}\\}$.</li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>17. <strong>Change of Basis</strong></h2>\n",
    "        <p>Change-of-basis expresses coordinates of vectors in a different basis. Used in PCA and diagonalization when switching to eigenvector coordinates.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>18. <strong>Solving Linear Systems</strong></h2>\n",
    "        <p>Matrix form: $A\\mathbf{x}=\\mathbf{b}$. Common solution methods:</p>\n",
    "        <ul>\n",
    "          <li>Gaussian elimination (row reduction).</li>\n",
    "          <li>If $A$ is invertible: $\\mathbf{x}=A^{-1}\\mathbf{b}$.</li>\n",
    "          <li>LU decomposition for efficient solving.</li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>19. <strong>Linear Transformations</strong></h2>\n",
    "        <p>A matrix defines a linear map $T(\\mathbf{x})=A\\mathbf{x}$. Examples: rotations, scalings, reflections.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>20. <strong>3D Linear Transformations</strong></h2>\n",
    "        <p>Matrices in $\\mathbb{R}^3$ can rotate, scale, and reflect 3D objects — used in graphics and embeddings.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>21. <strong>Matrix Multiplication as Composition</strong></h2>\n",
    "        <p>Applying $AB$ to a vector $\\mathbf{x}$ means first apply $B$ then $A$:</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          (AB)\\mathbf{x}=A\\big(B\\mathbf{x}\\big).\n",
    "          $$\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>22. <strong>Non-square Linear Maps</strong></h2>\n",
    "        <p>A non-square matrix still maps between vector spaces of different dimensions. Example: a $3\\times 2$ matrix maps $\\mathbb{R}^2\\to\\mathbb{R}^3$.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>23. <strong>Dot Product (Matrix Form)</strong></h2>\n",
    "        <p>If $\\mathbf{u},\\mathbf{v}$ are column vectors, the dot product is</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          \\mathbf{u}\\cdot \\mathbf{v} = \\mathbf{u}^\\top \\mathbf{v}.\n",
    "          $$\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>24. <strong>Cross Product</strong></h2>\n",
    "        <p>Defined in $\\mathbb{R}^3$. For $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^3$:</p>\n",
    "        <ul>\n",
    "          <li>$\\mathbf{u}\\times\\mathbf{v}$ is orthogonal to both $\\mathbf{u}$ and $\\mathbf{v}$.</li>\n",
    "          <li>Magnitude:\n",
    "            <div class=\"example\">\n",
    "              $$\n",
    "              \\|\\mathbf{u}\\times\\mathbf{v}\\| = \\|\\mathbf{u}\\|\\,\\|\\mathbf{v}\\|\\,\\sin\\theta,\n",
    "              $$\n",
    "              where $\\theta$ is the angle between $\\mathbf{u}$ and $\\mathbf{v}$.\n",
    "            </div>\n",
    "          </li>\n",
    "          <li>Determinant (component) representation:\n",
    "            <div class=\"example\">\n",
    "              $$\n",
    "              \\mathbf{u}\\times\\mathbf{v} =\n",
    "              \\begin{vmatrix}\n",
    "              \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n",
    "              u_1 & u_2 & u_3 \\\\\n",
    "              v_1 & v_2 & v_3\n",
    "              \\end{vmatrix}.\n",
    "              $$\n",
    "            </div>\n",
    "          </li>\n",
    "        </ul>\n",
    "      </section>\n",
    "    </details>\n",
    "    <hr/>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac021f-df5e-4a7f-a47b-53c162ed5c83",
   "metadata": {},
   "source": [
    "> # `Tensors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abaf5f-6f1f-44eb-af47-217ea5d5c0f4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "## 1. What are Tensors?\n",
    "\n",
    "* A **Tensor** is a **mathematical object** that generalizes **scalars, vectors, and matrices** to higher dimensions.\n",
    "* Think of it as a **container for numbers**, arranged in a certain number of dimensions (also called **axes**).\n",
    "* The number of dimensions of a tensor is called its **rank** (or order).\n",
    "\n",
    "### Examples of Tensors by rank:\n",
    "\n",
    "* **Rank-0 Tensor** → Scalar (e.g., `7`)\n",
    "* **Rank-1 Tensor** → Vector (e.g., `[3, 5, 7]`)\n",
    "* **Rank-2 Tensor** → Matrix (e.g., `[[1,2],[3,4]]`)\n",
    "* **Rank-3 Tensor** → Cube of numbers (e.g., image with RGB channels)\n",
    "* **Rank-n Tensor** → Higher dimensional generalization\n",
    "\n",
    "**Visual:**\n",
    "\n",
    "```\n",
    "Scalar → 5\n",
    "Vector → [5, 2, 3]\n",
    "Matrix → [[1,2,3],\n",
    "          [4,5,6]]\n",
    "3D Tensor → [[[...]]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Importance of Tensors in Deep Learning\n",
    "\n",
    "* **Data Representation:**\n",
    "\n",
    "  * Images → 3D tensors (height × width × channels).\n",
    "  * Videos → 4D tensors (frames × height × width × channels).\n",
    "  * Text → tensors (sequence length × embedding dimensions).\n",
    "* **Model Parameters:** Neural network weights are stored as tensors.\n",
    "* **Operations:** All forward & backward computations (matrix multiplication, dot products, gradients) use tensor algebra.\n",
    "* **Hardware Acceleration:** GPUs & TPUs are optimized for tensor operations (fast parallel computation).\n",
    "\n",
    "**Key Point:** Without tensors, deep learning wouldn’t be possible—tensors are the **language of neural networks**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tensor Operations\n",
    "\n",
    "Just like vectors and matrices, tensors support operations:\n",
    "\n",
    "* **Element-wise Operations:** Add, subtract, multiply, divide (performed element by element).\n",
    "* **Reshaping:** Change the shape of a tensor without changing its data.\n",
    "\n",
    "  * Example: reshape a `2×3` tensor into a `3×2`.\n",
    "* **Transpose / Permutation:** Swap dimensions.\n",
    "\n",
    "  * Example: image tensor (height × width × channels) → (channels × height × width).\n",
    "* **Slicing / Indexing:** Extract sub-tensors (like cutting parts of a dataset).\n",
    "* **Broadcasting:** Automatically expand tensors to perform operations.\n",
    "\n",
    "  * Example: `[1,2,3] + 5 → [6,7,8]`\n",
    "* **Dot Products / Matrix Multiplication:** Extend naturally to tensors.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Representation using Tensors\n",
    "\n",
    "Different ML data types as tensors:\n",
    "\n",
    "* **Scalars:** Single measurement (e.g., temperature = 35°C).\n",
    "* **Vectors (1D tensor):** Features of one sample (e.g., `[height, weight, age]`).\n",
    "* **Matrices (2D tensor):** Batch of samples (e.g., 100 students × 3 features each).\n",
    "* **3D Tensor:** Images (batch × height × width × channels).\n",
    "* **4D/5D Tensor:** Videos, 3D medical scans, etc.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Black & white image: `28 × 28` (2D tensor).\n",
    "* Colored image: `28 × 28 × 3` (3D tensor).\n",
    "* Batch of 100 colored images: `100 × 28 × 28 × 3` (4D tensor).\n",
    "\n",
    "---\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084ae07-b75d-422f-ab82-53912b3585d1",
   "metadata": {},
   "source": [
    "> # `Eigen Values and Vectors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312e328-711f-4d31-b482-f62879a7274b",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
    "  <title></title>\n",
    "\n",
    "  <!-- MathJax config: allow $...$ and $$...$$ -->\n",
    "  <script>\n",
    "    window.MathJax = {\n",
    "      tex: {\n",
    "        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
    "        displayMath: [['$$','$$'], ['\\\\[','\\\\]']]\n",
    "      },\n",
    "      options: {\n",
    "        skipHtmlTags: ['script','noscript','style','textarea','pre','code']\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\" async></script>\n",
    "\n",
    "  <style>\n",
    "    :root{\n",
    "      --bg: #fbfdff;\n",
    "      --card: #ffffff;\n",
    "      --muted: #6b7a90;\n",
    "      --accent: #0b63d6;\n",
    "      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, \"Roboto Mono\", \"Courier New\", monospace;\n",
    "    }\n",
    "    html,body{height:100%}\n",
    "    body{\n",
    "      margin:20px;\n",
    "      font-family: Inter, system-ui, -apple-system, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
    "      background: linear-gradient(180deg, #ffffff 0%, var(--bg) 100%);\n",
    "      color:#0b2030;\n",
    "      -webkit-font-smoothing:antialiased;\n",
    "      line-height:1.55;\n",
    "    }\n",
    "    .wrap{max-width:920px; margin:0 auto; background:var(--card); border-radius:12px; padding:22px; box-shadow:0 8px 30px rgba(11,30,45,0.06); border:1px solid rgba(11,99,214,0.04);}\n",
    "    header{display:flex; justify-content:space-between; align-items:baseline; gap:10px; margin-bottom:12px;}\n",
    "    header h1{margin:0; font-size:20px;}\n",
    "    header p{margin:0; color:var(--muted); font-size:13px;}\n",
    "    details{margin:10px 0;}\n",
    "    summary{cursor:pointer; font-weight:700; font-size:15px;}\n",
    "    details>summary::-webkit-details-marker { display: none; }\n",
    "    details[open] > summary::after { content: \"▾\"; padding-left:8px; color:var(--muted); }\n",
    "    details > summary::after { content: \"▸\"; padding-left:8px; color:var(--muted); }\n",
    "    section{margin:12px 0;}\n",
    "    h2{font-size:16px; margin:8px 0 6px;}\n",
    "    p{margin:6px 0;}\n",
    "    .example{background:#f1f8ff; padding:10px; border-radius:8px; border:1px solid rgba(11,99,214,0.06);}\n",
    "    pre{background:#f7fbff; padding:10px; border-radius:6px; overflow:auto; border:1px solid rgba(11,30,45,0.03);}\n",
    "    code{font-family:var(--mono); background:#eef6ff; padding:2px 6px; border-radius:6px;}\n",
    "    ul, ol { margin:8px 0 8px 20px; }\n",
    "    .hint{font-size:13px; color:var(--muted);}\n",
    "    hr{border:0; border-top:1px solid rgba(11,30,45,0.06); margin:16px 0;}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"wrap\">\n",
    "    <!-- <header>\n",
    "      <h1>Eigenvalues & Eigenvectors — Notes</h1>\n",
    "      <p class=\"hint\">Math rendered with MathJax (use <code>$$...$$</code> for display math)</p>\n",
    "    </header> -->\n",
    "    <details open>\n",
    "      <summary>Click to expand</summary>\n",
    "      <section>\n",
    "        <h2>1. Eigenvalues & Eigenvectors</h2>\n",
    "        <ul>\n",
    "          <li><strong>Definition.</strong> For a square matrix $A$, a nonzero vector $v$ is an <em>eigenvector</em> if\n",
    "            $$\n",
    "            A v = \\lambda v,\n",
    "            $$\n",
    "            where $\\lambda$ is a scalar called the <em>eigenvalue</em>.</li>\n",
    "          <li><strong>Intuition.</strong> Applying $A$ to $v$ does not change $v$'s direction — only its magnitude (and possibly sign): it gets stretched/compressed (or flipped) by factor $\\lambda$.</li>\n",
    "        </ul>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>Quick check (worked):</strong> Let\n",
    "            $$\n",
    "            A = \\begin{bmatrix}2 & 0 \\\\[4pt] 0 & 3 \\end{bmatrix}.\n",
    "            $$\n",
    "            Test the standard basis vectors $e_1 = \\begin{bmatrix}1\\\\0\\end{bmatrix}$ and $e_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$:\n",
    "          </p>\n",
    "          <p>\n",
    "            $$\n",
    "            A e_1 = \\begin{bmatrix}2 & 0\\\\ 0 & 3\\end{bmatrix}\\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix} = 2 e_1,\n",
    "            $$\n",
    "            so $e_1$ is an eigenvector with eigenvalue $\\lambda=2$.\n",
    "          </p>\n",
    "          <p>\n",
    "            $$\n",
    "            A e_2 = \\begin{bmatrix}2 & 0\\\\ 0 & 3\\end{bmatrix}\\begin{bmatrix}0\\\\1\\end{bmatrix} = \\begin{bmatrix}0\\\\3\\end{bmatrix} = 3 e_2,\n",
    "            $$\n",
    "            so $e_2$ is an eigenvector with eigenvalue $\\lambda=3$.\n",
    "          </p>\n",
    "          <p class=\"hint\"><strong>Answer:</strong> eigenvalues $\\{2,3\\}$ with eigenvectors along the coordinate axes (any nonzero scalar multiples of $e_1$ and $e_2$).</p>\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>2. Why eigenvalues / eigenvectors matter</h2>\n",
    "        <ul>\n",
    "          <li>They reveal the <strong>principal directions</strong> of a linear transformation — the directions that are only scaled, not rotated.</li>\n",
    "          <li>In machine learning and applied math they appear in many places:\n",
    "            <ul>\n",
    "              <li>Stability analysis of dynamical systems (eigenvalues tell growth/decay rates).</li>\n",
    "              <li>Principal Component Analysis (PCA) — eigenvectors of covariance show main variation directions.</li>\n",
    "              <li>Dimensionality reduction, feature extraction, spectral clustering.</li>\n",
    "            </ul>\n",
    "          </li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>3. Eigenfaces (computer vision)</h2>\n",
    "        <p><strong>Idea:</strong> Treat each face image as a vector (flatten pixels). Compute the covariance matrix of training images and its eigenvectors — the top eigenvectors are <em>Eigenfaces</em>, i.e. the dominant modes of variation.</p>\n",
    "        <ul>\n",
    "          <li>Each face can be approximated by a linear combination of a few eigenfaces.</li>\n",
    "          <li>Applications: face recognition (project into eigenface subspace), compression.</li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>4. Principal Component Analysis (PCA)</h2>\n",
    "        <p><strong>Goal:</strong> reduce dimensionality while retaining most variance.</p>\n",
    "        <p><strong>Standard steps:</strong></p>\n",
    "        <ol>\n",
    "          <li>Center the data: subtract the column mean from each column (so each feature has zero mean).</li>\n",
    "          <li>Compute covariance matrix. One common unbiased (or population) form is\n",
    "            $$\n",
    "            C = \\frac{1}{n}\\, X^\\top X\n",
    "            $$\n",
    "            where $X$ is the data matrix (rows = observations, columns = features) after centering. (Note: some authors use $\\frac{1}{n-1}$ for sample covariance; be consistent with your convention.)\n",
    "          </li>\n",
    "          <li>Find eigenvectors and eigenvalues of $C$. Eigenvectors are principal components.</li>\n",
    "          <li>Project the data onto the top $k$ eigenvectors (those with largest eigenvalues).</li>\n",
    "        </ol>\n",
    "        <p><strong>Connection:</strong> eigenvalues measure the variance explained by each component; sorting eigenvalues descending gives principal directions of decreasing variance.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>Visualization tip</h2>\n",
    "        <p>Think of PCA as rotating the coordinate axes so that the new axes align with directions of maximum variance. Projecting onto the first $k$ axes keeps the most important variation.</p>\n",
    "      </section>\n",
    "    </details>\n",
    "    <hr/>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9f96a-da2b-4466-a52b-59132d78f4a7",
   "metadata": {},
   "source": [
    "> # `Matrix Factorization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fbd64d-36e2-4135-b910-b58d23a3c402",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
    "  <title></title>\n",
    "\n",
    "  <!-- MathJax: allow $...$ and $$...$$ -->\n",
    "  <script>\n",
    "    window.MathJax = {\n",
    "      tex: {\n",
    "        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
    "        displayMath: [['$$','$$'], ['\\\\[','\\\\]']]\n",
    "      },\n",
    "      options: {\n",
    "        skipHtmlTags: ['script','noscript','style','textarea','pre','code']\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\" async></script>\n",
    "\n",
    "  <style>\n",
    "    :root{\n",
    "      --bg:#fbfdff;\n",
    "      --card:#ffffff;\n",
    "      --muted:#6b7a90;\n",
    "      --accent:#0b63d6;\n",
    "      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, \"Roboto Mono\", \"Courier New\", monospace;\n",
    "    }\n",
    "    html,body{height:100%}\n",
    "    body{\n",
    "      margin:22px;\n",
    "      font-family: Inter, system-ui, -apple-system, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
    "      background: linear-gradient(180deg,#ffffff 0%,var(--bg) 100%);\n",
    "      color:#0b2030;\n",
    "      -webkit-font-smoothing:antialiased;\n",
    "      line-height:1.55;\n",
    "    }\n",
    "    .container{\n",
    "      max-width:980px;\n",
    "      margin:0 auto;\n",
    "      background:var(--card);\n",
    "      border-radius:12px;\n",
    "      padding:22px;\n",
    "      box-shadow:0 8px 28px rgba(11,30,45,0.06);\n",
    "      border:1px solid rgba(11,99,214,0.04);\n",
    "    }\n",
    "    header{display:flex; justify-content:space-between; align-items:baseline; gap:12px; margin-bottom:14px;}\n",
    "    header h1{margin:0; font-size:20px;}\n",
    "    header p{margin:0; color:var(--muted); font-size:13px;}\n",
    "    details{margin:8px 0;}\n",
    "    summary{cursor:pointer; font-weight:700; font-size:15px;}\n",
    "    details>summary::-webkit-details-marker{display:none;}\n",
    "    details[open] > summary::after { content: \"▾\"; padding-left:8px; color:var(--muted); }\n",
    "    details>summary::after { content: \"▸\"; padding-left:8px; color:var(--muted); }\n",
    "    section{margin:12px 0;}\n",
    "    h2{font-size:16px; margin:8px 0 6px;}\n",
    "    p{margin:6px 0;}\n",
    "    .example{background:#f1f8ff; padding:10px; border-radius:8px; border:1px solid rgba(11,99,214,0.06);}\n",
    "    pre{background:#f7fbff; padding:10px; border-radius:8px; overflow:auto; border:1px solid rgba(11,30,45,0.03);}\n",
    "    code{font-family:var(--mono); background:#eef6ff; padding:2px 6px; border-radius:6px;}\n",
    "    ul, ol { margin:8px 0 8px 20px; }\n",
    "    .hint{font-size:13px; color:var(--muted);}\n",
    "    hr{border:0; border-top:1px solid rgba(11,30,45,0.06); margin:18px 0;}\n",
    "    .answer{background:#fff8e1; border-left:4px solid #ffb020; padding:8px 12px; border-radius:6px; margin-top:8px;}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <!-- <header>\n",
    "      <h1>Matrix Factorizations — Notes</h1>\n",
    "      <p class=\"hint\">MathJax enabled — block math uses <code>$$...$$</code></p>\n",
    "    </header> -->\n",
    "    <details open>\n",
    "      <summary>Click to expand</summary>\n",
    "      <section>\n",
    "        <h2>1. LU Decomposition</h2>\n",
    "        <p><strong>What it is:</strong> Factor a square matrix $A$ into</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          A = L\\,U\n",
    "          $$\n",
    "          where $L$ is lower-triangular (often with 1s on the diagonal) and $U$ is upper-triangular.\n",
    "        </div>\n",
    "        <p><strong>Use:</strong> Efficiently solve linear systems, e.g. once you have $L$ and $U$ you solve $Ax=b$ by forward/back substitution:</p>\n",
    "        <ol>\n",
    "          <li>solve $L y = b$ (forward substitution)</li>\n",
    "          <li>solve $U x = y$ (back substitution)</li>\n",
    "        </ol>\n",
    "        <p><strong>ML connection:</strong> Speed up repeated solves (e.g. many right-hand sides) — useful in regression and numerical optimization.</p>\n",
    "        <div class=\"answer\"><strong>Quick check — why is LU easier?</strong>\n",
    "        Because triangular systems are cheap to solve (forward/back substitution cost $\\mathcal{O}(n^2)$ each), while direct solving with $A$ (e.g. Gaussian elimination each time) repeats work. Once you factorize $A=LU$, reusing $L$ and $U$ is much cheaper when solving multiple $b$'s.</div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>2. QR Decomposition</h2>\n",
    "        <p><strong>What it is:</strong> Factor a matrix $A$ as</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          A = Q R\n",
    "          $$\n",
    "          where $Q$ is orthogonal ($Q^\\top Q = I$) and $R$ is upper-triangular.\n",
    "        </div>\n",
    "        <p><strong>Use:</strong> Numerically stable solving of least squares. If $A$ is $m\\times n$ and full rank, the least-squares solution to $\\min_x\\|Ax-b\\|_2$ can be obtained from $R x = Q^\\top b$.</p>\n",
    "        <p><strong>ML connection:</strong> regression solutions, Gram–Schmidt orthogonalization, constructing orthonormal features.</p>\n",
    "        <div class=\"answer\"><strong>Check — what does “orthogonal” mean for columns of $Q$?</strong>\n",
    "        The columns are mutually orthonormal: their pairwise dot products are zero and each column has unit length. Formally, for columns $q_i, q_j$, $q_i^\\top q_j = 0$ if $i\\neq j$, and $q_i^\\top q_i = 1$.</div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>3. Eigen Decomposition</h2>\n",
    "        <p><strong>What it is:</strong> Factor a square matrix $A$ (diagonalizable case) as</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          A = V \\Lambda V^{-1}\n",
    "          $$\n",
    "          where columns of $V$ are eigenvectors and $\\Lambda$ is diagonal with eigenvalues.\n",
    "        </div>\n",
    "        <p><strong>Use:</strong> analyze linear transformations, solve systems in modal coordinates, exponentiate matrices (e.g., $e^{At}$ via eigen decomposition when possible).</p>\n",
    "        <p><strong>ML connection:</strong> PCA (covariance eigen-decomposition), spectral clustering, covariance/correlation analyses.</p>\n",
    "        <div class=\"answer\"><strong>Check — if an eigenvalue is very small (≈ 0), what does that say?</strong>\n",
    "        It indicates almost no variance (or negligible action) in that eigenvector direction — the data (or transformation) compresses that direction strongly, so it contributes little to variance and might be dropped for dimensionality reduction.</div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>4. Singular Value Decomposition (SVD)</h2>\n",
    "        <p><strong>What it is:</strong> Factor any matrix $A$ (square or rectangular) as</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          A = U \\Sigma V^\\top\n",
    "          $$\n",
    "          where $U$ (left singular vectors) and $V$ (right singular vectors) are orthogonal, and $\\Sigma$ is diagonal with non-negative singular values (usually ordered descending).\n",
    "        </div>\n",
    "        <p><strong>Use:</strong> dimension reduction, low-rank approximation (Eckart–Young theorem), denoising, pseudoinverse computation.</p>\n",
    "        <p><strong>ML connection:</strong> PCA (SVD on centered data), Latent Semantic Analysis (LSA) in NLP, collaborative filtering in recommender systems.</p>\n",
    "        <div class=\"answer\"><strong>Check — why is SVD more general than eigen decomposition?</strong>\n",
    "        SVD applies to any $m\\times n$ matrix (even non-square, non-symmetric). Eigen decomposition requires a square matrix (and diagonalizability for $A=V\\Lambda V^{-1}$). SVD gives orthonormal bases for domain and codomain simultaneously and always exists for real matrices.</div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>5. Non-Negative Matrix Factorization (NMF)</h2>\n",
    "        <p><strong>What it is:</strong> For a non-negative matrix $A$, find non-negative factors $W,H$ such that</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          A \\approx W H \\qquad\\text{with } W,H \\ge 0.\n",
    "          $$\n",
    "        </div>\n",
    "        <p><strong>Use:</strong> parts-based, interpretable decomposition where components and coefficients are non-negative (often sparser and easier to interpret).</p>\n",
    "        <p><strong>ML connection:</strong> topic modeling (topics × document weights), image decomposition (parts of an image), recommender systems with non-negative constraints.</p>\n",
    "        <div class=\"answer\"><strong>Check — why non-negativity helps interpretation?</strong>\n",
    "        Forcing non-negativity prevents canceling positive and negative components (no subtractive combinations). Components add up to form data, making factors more \"parts-based\" and often easier to map to real-world parts (e.g., topics, facial parts), unlike SVD where singular vectors can have positive and negative entries and require cancellations to form original data.</div>\n",
    "      </section>\n",
    "    </details>\n",
    "    <hr/>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be6b52-094f-4697-b90d-6d52818b2f96",
   "metadata": {},
   "source": [
    "> # `Advanced Topics`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15722116-500b-4f08-a286-7123b9b8c172",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
    "  <title></title>\n",
    "\n",
    "  <!-- MathJax configuration (supports $...$ and $$...$$) -->\n",
    "  <script>\n",
    "    window.MathJax = {\n",
    "      tex: {\n",
    "        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
    "        displayMath: [['$$','$$'], ['\\\\[','\\\\]']]\n",
    "      },\n",
    "      options: {\n",
    "        skipHtmlTags: ['script','noscript','style','textarea','pre','code']\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\" async></script>\n",
    "\n",
    "  <style>\n",
    "    :root{\n",
    "      --bg:#fbfdff;\n",
    "      --card:#ffffff;\n",
    "      --muted:#657786;\n",
    "      --accent:#0b63d6;\n",
    "      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, \"Roboto Mono\", \"Courier New\", monospace;\n",
    "    }\n",
    "    html,body{height:100%}\n",
    "    body{\n",
    "      margin:22px;\n",
    "      font-family:Inter, system-ui, -apple-system, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
    "      background: linear-gradient(180deg,#ffffff 0%,var(--bg) 100%);\n",
    "      color:#0b2030;\n",
    "      -webkit-font-smoothing:antialiased;\n",
    "      line-height:1.55;\n",
    "    }\n",
    "    .wrap{\n",
    "      max-width:980px;\n",
    "      margin:0 auto;\n",
    "      background:var(--card);\n",
    "      border-radius:12px;\n",
    "      padding:22px;\n",
    "      box-shadow:0 10px 30px rgba(11,30,45,0.06);\n",
    "      border:1px solid rgba(11,99,214,0.04);\n",
    "    }\n",
    "    header{display:flex; justify-content:space-between; align-items:baseline; gap:12px; margin-bottom:12px;}\n",
    "    header h1{margin:0; font-size:20px;}\n",
    "    header p{margin:0; color:var(--muted); font-size:13px;}\n",
    "    details{margin:10px 0;}\n",
    "    summary{cursor:pointer; font-weight:700; font-size:15px;}\n",
    "    details>summary::-webkit-details-marker { display: none; }\n",
    "    details[open] > summary::after { content: \"▾\"; padding-left:8px; color:var(--muted); }\n",
    "    details>summary::after { content: \"▸\"; padding-left:8px; color:var(--muted); }\n",
    "    section{margin:12px 0;}\n",
    "    h2{font-size:16px; margin:8px 0 6px;}\n",
    "    p{margin:6px 0;}\n",
    "    .example{background:#f1f8ff; padding:12px; border-radius:8px; border:1px solid rgba(11,99,214,0.06);}\n",
    "    pre{background:#f7fbff; padding:10px; border-radius:8px; overflow:auto; border:1px solid rgba(11,30,45,0.03);}\n",
    "    code{font-family:var(--mono); background:#eef6ff; padding:2px 6px; border-radius:6px;}\n",
    "    .answer{background:#fff8e6; border-left:4px solid #ffb146; padding:10px; border-radius:6px; margin-top:10px;}\n",
    "    ul, ol { margin:8px 0 8px 20px; }\n",
    "    .hint{font-size:13px; color:var(--muted);}\n",
    "    hr{border:0; border-top:1px solid rgba(11,30,45,0.06); margin:16px 0;}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"wrap\">\n",
    "    <details open>\n",
    "      <summary>Click to expand</summary>\n",
    "      <section>\n",
    "        <h2>1. Moore–Penrose Pseudoinverse</h2>\n",
    "        <p><strong>What it is:</strong> a generalization of the matrix inverse for matrices that are non-square or singular. For $A\\in\\mathbb{R}^{m\\times n}$, its pseudoinverse $A^+$ satisfies a set of Moore–Penrose conditions and gives best-fit/optimal solutions for least squares problems.</p>\n",
    "        <p><strong>SVD formula:</strong> if\n",
    "          $$\n",
    "          A = U \\Sigma V^\\top,\n",
    "          $$\n",
    "          then\n",
    "          $$\n",
    "          A^+ = V\\,\\Sigma^+\\,U^\\top,\n",
    "          $$\n",
    "          where $\\Sigma^+$ is formed by taking reciprocals of the nonzero singular values (and transposing the shape).</p>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>NumPy:</strong></p>\n",
    "          <pre><code>import numpy as np\n",
    "A = np.array([[1., 2., 3.],\n",
    "              [4., 5., 6.]])   # 2x3\n",
    "A_pinv = np.linalg.pinv(A)</code></pre>\n",
    "        </div>\n",
    "        <div class=\"answer\">\n",
    "          <strong>Check — why is pseudoinverse more flexible than a normal inverse?</strong>\n",
    "          <p>Because $A^+$ exists for any matrix (real matrices always have an SVD), whereas $A^{-1}$ exists only for square, full-rank matrices. The pseudoinverse gives the minimum-norm solution to under/overdetermined least squares problems and therefore handles non-square or singular cases gracefully.</p>\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>2. Quadratic Forms</h2>\n",
    "        <p><strong>Definition:</strong> A quadratic form is</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          Q(x) = x^\\top A x,\n",
    "          $$\n",
    "          where $x\\in\\mathbb{R}^n$ and typically $A$ is symmetric (we can always symmetrize $A$ without changing $Q$).</div>\n",
    "        <p><strong>Geometric meaning:</strong> Quadratic forms describe ellipsoids, hyperboloids, etc.; level sets $x^\\top A x = c$ are conic sections in general.</p>\n",
    "        <div class=\"answer\">\n",
    "          <strong>Check — if $A=I$, what is $Q(x)=x^\\top I x$?</strong>\n",
    "          <p>Then\n",
    "            $$\n",
    "            Q(x)=x^\\top x = \\|x\\|_2^2,\n",
    "            $$\n",
    "            the squared Euclidean norm. Its level sets are spheres (centered at the origin).</p>\n",
    "        </div>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>NumPy (compute quadratic form)</strong></p>\n",
    "          <pre><code>x = np.array([1.,2.,3.])\n",
    "A = np.eye(3)\n",
    "Q = x.T @ A @ x   # equals np.dot(x,x)</code></pre>\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>3. Positive Definite Matrices</h2>\n",
    "        <p><strong>Definition:</strong> Symmetric $A$ is <em>positive definite</em> if</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          x^\\top A x > 0 \\quad\\text{for all nonzero } x.\n",
    "          $$\n",
    "        </div>\n",
    "        <p><strong>Properties:</strong></p>\n",
    "        <ul>\n",
    "          <li>All eigenvalues are positive.</li>\n",
    "          <li>$A$ is invertible and well-behaved in optimization.</li>\n",
    "          <li>Cholesky decomposition exists ($A = R^\\top R$ for some upper-triangular $R$).</li>\n",
    "        </ul>\n",
    "        <div class=\"answer\">\n",
    "          <strong>Check — why do optimization algorithms “like” positive definite Hessians?</strong>\n",
    "          <p>Because a positive definite Hessian at a point implies the objective is locally strictly convex there — a unique local minimum. Algorithms (Newton, quasi-Newton) rely on inverting or factoring the Hessian; positive definiteness guarantees invertibility and well-conditioned quadratic approximations, giving reliable descent steps and fast (often quadratic) convergence near the minimum.</p>\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>4. Hadamard Product</h2>\n",
    "        <p><strong>Definition:</strong> The Hadamard product (element-wise product) of same-size matrices $A$ and $B$ is</p>\n",
    "        <div class=\"example\">\n",
    "          $$\n",
    "          (A\\circ B)_{ij} = A_{ij}\\,B_{ij}.\n",
    "          $$\n",
    "        </div>\n",
    "        <p><strong>Not the same as</strong> matrix multiplication — it multiplies corresponding entries.</p>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>Quick check:</strong> If</p>\n",
    "          $$\n",
    "          A = \\begin{bmatrix}1 & 2\\\\[4pt] 3 & 4\\end{bmatrix},\\quad\n",
    "          B = \\begin{bmatrix}5 & 6\\\\[4pt] 7 & 8\\end{bmatrix},\n",
    "          $$\n",
    "          then\n",
    "          $$\n",
    "          A\\circ B = \\begin{bmatrix}1\\cdot 5 & 2\\cdot 6\\\\[4pt] 3\\cdot 7 & 4\\cdot 8\\end{bmatrix}\n",
    "                    = \\begin{bmatrix}5 & 12\\\\[4pt] 21 & 32\\end{bmatrix}.\n",
    "          $$\n",
    "        </div>\n",
    "        <div class=\"example\">\n",
    "          <p><strong>NumPy:</strong></p>\n",
    "          <pre><code>A * B   # element-wise product in NumPy (Hadamard)</code></pre>\n",
    "        </div>\n",
    "      </section>\n",
    "    </details>\n",
    "    <hr/>\n",
    "</body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
