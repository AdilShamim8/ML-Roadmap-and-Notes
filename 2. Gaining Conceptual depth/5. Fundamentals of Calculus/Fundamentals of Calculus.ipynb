{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079a7f16-708f-4d99-909b-24a64ab69308",
   "metadata": {},
   "source": [
    "> # `Diffrentiation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cc75b-2fd9-41f8-8073-426077d4a63f",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\"/>\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/>\n",
    "  <title>Differentiation Notes</title>\n",
    "\n",
    "  <!-- MathJax for rendering LaTeX -->\n",
    "  <script>\n",
    "    window.MathJax = {\n",
    "      tex: {\n",
    "        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
    "        displayMath: [['$$','$$'], ['\\\\[','\\\\]']]\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\" async></script>\n",
    "\n",
    "  <style>\n",
    "    body {\n",
    "      font-family: Inter, system-ui, sans-serif;\n",
    "      margin: 24px;\n",
    "      background: #f9fcff;\n",
    "      color: #1a2a33;\n",
    "      line-height: 1.55;\n",
    "    }\n",
    "    .wrap {\n",
    "      max-width: 900px;\n",
    "      margin: auto;\n",
    "      background: #fff;\n",
    "      border-radius: 12px;\n",
    "      padding: 20px 28px;\n",
    "      box-shadow: 0 8px 24px rgba(0,0,0,0.08);\n",
    "    }\n",
    "    details { margin: 12px 0; }\n",
    "    summary { cursor: pointer; font-weight: 600; font-size: 16px; }\n",
    "    h2 { font-size: 18px; margin: 12px 0 6px; }\n",
    "    p { margin: 6px 0; }\n",
    "    .example {\n",
    "      background: #f2f9ff;\n",
    "      border: 1px solid #ddeaff;\n",
    "      border-radius: 8px;\n",
    "      padding: 10px 14px;\n",
    "      margin: 8px 0;\n",
    "    }\n",
    "    .answer {\n",
    "      background: #fff9eb;\n",
    "      border-left: 4px solid #ffb347;\n",
    "      padding: 10px 14px;\n",
    "      border-radius: 6px;\n",
    "      margin: 10px 0;\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"wrap\">\n",
    "\n",
    "<details open>\n",
    "<summary>Click to expand</summary>\n",
    "\n",
    "<section>\n",
    "  <h2>1. What is Differentiation?</h2>\n",
    "  <p><strong>Idea:</strong> Differentiation measures the <em>rate of change</em> of a function.</p>\n",
    "  <p>If $y=f(x)$, the derivative $f'(x)$ tells us how $y$ changes when $x$ changes a tiny bit.</p>\n",
    "  <p><strong>Geometric view:</strong> slope of the tangent line to the curve.</p>\n",
    "\n",
    "  <div class=\"answer\">\n",
    "    <strong>Quick check:</strong> If $f(x)=x^2$, slope at $x=2$?  \n",
    "    $f'(x)=2x \\;\\Rightarrow\\; f'(2)=4$.\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>2. Differentiation of a Constant</h2>\n",
    "  <p>Rule:</p>\n",
    "  <div class=\"example\">$$\\frac{d}{dx}(c) = 0$$</div>\n",
    "  <p>Constants don’t change, so slope = 0.</p>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>3. Power Rule</h2>\n",
    "  <p>If $f(x)=x^n$, then</p>\n",
    "  <div class=\"example\">$$\\frac{d}{dx}(x^n)=n x^{n-1}$$</div>\n",
    "  <p>Example: $\\tfrac{d}{dx}(x^3)=3x^2$.</p>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>4. Sum Rule</h2>\n",
    "  <div class=\"example\">\n",
    "    $$\\frac{d}{dx}[f(x)+g(x)] = f'(x)+g'(x)$$\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>5. Product Rule</h2>\n",
    "  <div class=\"example\">\n",
    "    $$\\frac{d}{dx}[f(x)g(x)] = f'(x)g(x) + f(x)g'(x)$$\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>6. Quotient Rule</h2>\n",
    "  <div class=\"example\">\n",
    "    $$\\frac{d}{dx}\\!\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{f'(x)g(x)-f(x)g'(x)}{(g(x))^2}$$\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>7. Chain Rule</h2>\n",
    "  <p>For composition $f(g(x))$:</p>\n",
    "  <div class=\"example\">\n",
    "    $$\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)$$\n",
    "  </div>\n",
    "  <p><strong>ML connection:</strong> Basis of <em>backpropagation</em> in neural networks.</p>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>8. Partial Differentiation</h2>\n",
    "  <p>If $f(x,y)$ depends on multiple variables:</p>\n",
    "  <ul>\n",
    "    <li>$\\tfrac{\\partial f}{\\partial x}$: treat $y$ constant, differentiate w.r.t $x$.</li>\n",
    "    <li>$\\tfrac{\\partial f}{\\partial y}$: treat $x$ constant, differentiate w.r.t $y$.</li>\n",
    "  </ul>\n",
    "\n",
    "  <div class=\"example\">\n",
    "    $f(x,y)=x^2 y + y^3$  \n",
    "    $$\\frac{\\partial f}{\\partial x} = 2xy, \\quad \n",
    "      \\frac{\\partial f}{\\partial y} = x^2 + 3y^2$$\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>9. Higher-Order Derivatives</h2>\n",
    "  <p>Differentiate multiple times:</p>\n",
    "  <ul>\n",
    "    <li>Second derivative: $f''(x)$ = derivative of derivative.</li>\n",
    "    <li>Tells about <em>curvature</em>.</li>\n",
    "  </ul>\n",
    "  <p><strong>ML connection:</strong> Hessians (matrix of second partial derivatives) appear in optimization.</p>\n",
    "</section>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<section>\n",
    "  <h2>10. Matrix Differentiation</h2>\n",
    "  <p>Extends to vector/matrix functions:</p>\n",
    "  <ul>\n",
    "    <li>If $f(x)=a^\\top x$, then $\\nabla_x f = a$.</li>\n",
    "    <li>If $f(x)=x^\\top A x$, then $\\nabla_x f = (A+A^\\top)x$.</li>\n",
    "  </ul>\n",
    "  <p><strong>ML connection:</strong> Gradients in cost functions, training neural networks, optimization.</p>\n",
    "</section>\n",
    "\n",
    "</details>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb9b95-7240-4b12-b1ae-add6bcef2e28",
   "metadata": {},
   "source": [
    "> # `Optimization Theory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0c4c5-9dde-433a-95f3-254a38367982",
   "metadata": {},
   "source": [
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"utf-8\"/>\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/>\n",
    "  <title>Functions & Optimization — Notes</title>\n",
    "\n",
    "  <!-- MathJax config (supports $...$ and $$...$$) -->\n",
    "  <script>\n",
    "    window.MathJax = {\n",
    "      tex: {\n",
    "        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],\n",
    "        displayMath: [['$$','$$'], ['\\\\[','\\\\]']]\n",
    "      },\n",
    "      options: {\n",
    "        skipHtmlTags: ['script','noscript','style','textarea','pre','code']\n",
    "      }\n",
    "    };\n",
    "  </script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\" async></script>\n",
    "\n",
    "  <style>\n",
    "    :root{\n",
    "      --bg:#f8fcff;\n",
    "      --card:#ffffff;\n",
    "      --muted:#64748b;\n",
    "      --accent:#0b63d6;\n",
    "      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, \"Roboto Mono\", \"Courier New\", monospace;\n",
    "    }\n",
    "    body{font-family:Inter, system-ui, -apple-system, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial; background:var(--bg); color:#0b2030; margin:22px;}\n",
    "    .card{max-width:980px; margin:0 auto; background:var(--card); padding:22px; border-radius:12px; box-shadow:0 10px 30px rgba(11,30,45,0.06); border:1px solid rgba(11,99,214,0.04);}\n",
    "    header{display:flex; align-items:baseline; justify-content:space-between; gap:12px; margin-bottom:12px;}\n",
    "    header h1{margin:0; font-size:20px;}\n",
    "    header p{margin:0; color:var(--muted); font-size:13px;}\n",
    "    details{margin:8px 0;}\n",
    "    summary{cursor:pointer; font-weight:700; font-size:15px;}\n",
    "    details>summary::-webkit-details-marker{display:none;}\n",
    "    details[open] > summary::after { content: \"▾\"; padding-left:8px; color:var(--muted); }\n",
    "    details>summary::after { content: \"▸\"; padding-left:8px; color:var(--muted); }\n",
    "    section{margin:12px 0;}\n",
    "    h2{font-size:16px; margin:8px 0 6px;}\n",
    "    p{margin:6px 0;}\n",
    "    .example{background:#f1f8ff; padding:10px; border-radius:8px; border:1px solid rgba(11,99,214,0.06);}\n",
    "    pre{background:#f7fbff; padding:10px; border-radius:8px; overflow:auto; border:1px solid rgba(11,30,45,0.03);}\n",
    "    code{font-family:var(--mono); background:#eef6ff; padding:2px 6px; border-radius:6px;}\n",
    "    .answer{background:#fffaf0; border-left:4px solid #ffb347; padding:10px 12px; border-radius:6px; margin-top:8px;}\n",
    "    ul, ol{margin:8px 0 8px 20px;}\n",
    "    .hint{font-size:13px; color:var(--muted);}\n",
    "    hr{border:0; border-top:1px solid rgba(11,30,45,0.06); margin:16px 0;}\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"card\">\n",
    "    <details open>\n",
    "      <summary>Click to expand</summary>\n",
    "      <section>\n",
    "        <h2>1. Function</h2>\n",
    "        <p>A <strong>function</strong> maps inputs to outputs:</p>\n",
    "        <div class=\"example\">\n",
    "          $$ y = f(x) $$\n",
    "        </div>\n",
    "        <p>In ML, $f$ often denotes a model (linear model, neural network, etc.).</p>\n",
    "        <div class=\"answer\">\n",
    "          <strong>Example (linear regression):</strong><br>\n",
    "          The common function form relating inputs $X$ and output $y$ is\n",
    "          $$\n",
    "          y = Xw + b \\quad\\text{or}\\quad \\hat{y} = w^\\top x + b,\n",
    "          $$\n",
    "          where $w$ are weights and $b$ is a bias/offset.\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>2. Multivariate Functions</h2>\n",
    "        <p>Functions of many variables:</p>\n",
    "        <div class=\"example\">\n",
    "          $$ f(x_1,x_2,\\dots,x_n) $$\n",
    "        </div>\n",
    "        <p>In ML inputs are usually high-dimensional vectors; cost functions depend on many parameters (weights).</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>3. Parameters of a Function</h2>\n",
    "        <p>Parameters are the adjustable knobs (weights, biases) we optimize. Example:</p>\n",
    "        <div class=\"example\">\n",
    "          $$ f(x) = w_1 x_1 + w_2 x_2 + b $$\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>4. Maxima & Minima</h2>\n",
    "        <p>Optimization aims to find minima (loss) or maxima (reward). First-order necessary condition:</p>\n",
    "        <div class=\"example\">\n",
    "          $$ \\nabla f(x) = 0 $$\n",
    "        </div>\n",
    "        <p>Second derivative (Hessian) indicates curvature: positive definite → local minimum.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>5. Loss Functions</h2>\n",
    "        <p>Loss measures error between predictions and targets. Common examples:</p>\n",
    "        <ul>\n",
    "          <li>MSE (Mean Squared Error): $$ L_{\\text{MSE}} = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2 $$</li>\n",
    "          <li>Cross-entropy (classification): $$ L_{\\text{CE}} = -\\sum_i y_i \\log \\hat{p}_i $$ (for one-hot $y$)</li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>6. How to Select a Good Loss Function</h2>\n",
    "        <p>Choice depends on the task:</p>\n",
    "        <ul>\n",
    "          <li>Regression → MSE, MAE.</li>\n",
    "          <li>Classification → Cross-entropy (softmax + log-loss).</li>\n",
    "          <li>Ranking → Hinge, pairwise losses.</li>\n",
    "        </ul>\n",
    "        <div class=\"answer\">\n",
    "          <strong>Why cross-entropy often beats MSE for classification:</strong>\n",
    "          <ul>\n",
    "            <li>Cross-entropy corresponds to the negative log-likelihood under a probabilistic model (softmax + categorical distribution), so optimizing it performs maximum likelihood estimation.</li>\n",
    "            <li>It produces larger gradients when predictions are confidently wrong, giving stronger corrective updates. MSE treats probabilities poorly and can produce vanishing gradients near 0/1 probabilities.</li>\n",
    "            <li>Cross-entropy's gradient structure aligns with the softmax output, leading to stable, faster convergence for classification tasks.</li>\n",
    "          </ul>\n",
    "        </div>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>7. Calculating Parameters of a Loss Function</h2>\n",
    "        <p>Find parameters $w$ that minimize $L(w)$. Usually done with gradient-based methods because closed-form solutions are rare for complex models.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>8. Convex & Concave Loss Functions</h2>\n",
    "        <p><strong>Convex:</strong> any line segment between two points on the graph lies above the graph → single global minimum. Convex losses are easier to optimize reliably.</p>\n",
    "        <p><strong>Concave:</strong> opposite.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>9. Gradient Descent</h2>\n",
    "        <p>Iterative update rule:</p>\n",
    "        <div class=\"example\">\n",
    "          $$ w \\leftarrow w - \\eta \\,\\nabla_w L(w) $$\n",
    "        </div>\n",
    "        <p>Where $\\eta$ is the learning rate. Variants: SGD, mini-batch, momentum, Adam, RMSProp, etc.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>10. Hessians</h2>\n",
    "        <p>Matrix of second partial derivatives:</p>\n",
    "        <div class=\"example\">\n",
    "          $$ H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} $$\n",
    "        </div>\n",
    "        <p>Hessians describe curvature and are central to Newton's method and second-order optimization; positive definite Hessian → local convexity.</p>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>11. Problems Faced in Optimization</h2>\n",
    "        <ul>\n",
    "          <li>Local minima (less of a problem in very-high-dimensional neural nets).</li>\n",
    "          <li>Saddle points (common and can slow training).</li>\n",
    "          <li>Vanishing / exploding gradients (especially in deep networks).</li>\n",
    "          <li>Poor learning rate choice (too small → slow; too large → divergence).</li>\n",
    "        </ul>\n",
    "      </section>\n",
    "      <hr/>\n",
    "      <section>\n",
    "        <h2>12. Constrained Optimization</h2>\n",
    "        <p>Sometimes we optimize subject to constraints (e.g., Lasso: $\\|w\\|_1 \\le \\alpha$ or equivalently add $\\lambda\\|w\\|_1$ penalty). Methods include Lagrange multipliers and KKT conditions.</p>\n",
    "      </section>\n",
    "    </details>\n",
    "    <hr/>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
