{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58caa16c-5b37-4a93-bf6a-3583fd11eab6",
   "metadata": {},
   "source": [
    "# Machine Learning Metrics: Formulas and Python Code | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2031%20Regression%20Metrics)\n",
    "\n",
    "## 1. Regression Metrics\n",
    "\n",
    "### 1.1.Mean Absolute Error (MAE)\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "**Python Code:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Example:\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "print(\"MAE:\", mae(y_true, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. Mean Squared Error (MSE)\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "**Python Code:**\n",
    "```python\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "print(\"MSE:\", mse(y_true, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3. Root Mean Squared Error (RMSE)\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2}\n",
    "$$\n",
    "\n",
    "**Python Code:**\n",
    "```python\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "print(\"RMSE:\", rmse(y_true, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4. R-squared (Coefficient of Determination)\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2}{\\sum_{i=1}^{n} \\left( y_i - \\bar{y} \\right)^2}\n",
    "$$\n",
    "where \\(\\bar{y}\\) is the mean of \\(y_i\\).\n",
    "\n",
    "**Python Code:**\n",
    "```python\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return 1 - (ss_res/ss_tot)\n",
    "\n",
    "print(\"R^2:\", r2_score(y_true, y_pred))\n",
    "```\n",
    "---\n",
    "### Mean Absolute Percentage Error (MAPE)\n",
    "$$\n",
    "\\text{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n",
    "$$\n",
    "---\n",
    "\n",
    "## 2. Classification Metrics | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2040%20Classification%20Metrics)\n",
    "\n",
    "Assume we have four outcomes from a binary classifier:\n",
    "- True Positives (TP)\n",
    "- False Positives (FP)\n",
    "- True Negatives (TN)\n",
    "- False Negatives (FN)\n",
    "\n",
    "### 2.1. Accuracy\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. Precision\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. Recall (Sensitivity)\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4. F1 Score\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5. Python Code Example for Classification Metrics\n",
    "\n",
    "Using scikit-learn, you can compute these metrics as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Example true labels and predicted labels\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6. ROC Curve and AUC\n",
    "\n",
    "For classifiers that output probabilities, you can evaluate performance using the ROC curve and compute the AUC:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Example: Probabilistic predictions for the positive class\n",
    "y_prob = [0.9, 0.1, 0.8, 0.4, 0.3, 0.85, 0.2, 0.7, 0.95, 0.05]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve (AUC = {:.2f})\".format(auc))\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 3. ROC and AUC Metrics | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2040%20Classification%20Metrics)\n",
    "\n",
    "### True Positive Rate (TPR) / Recall\n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "### False Positive Rate (FPR)\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "*Note:* The ROC curve plots TPR against FPR at various threshold settings. The AUC (Area Under the Curve) is a single scalar value representing the area under this curve.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Log Loss (Binary Cross-Entropy Loss) | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2040%20Classification%20Metrics)\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]\n",
    "$$\n",
    "\n",
    "<p><em>Here, &#x1D6E8; <span>(p<sub>i</sub>)</span> is the predicted probability for the positive class.</em></p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
