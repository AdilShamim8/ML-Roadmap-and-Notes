{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fca933-0009-4d94-b514-42ad4ac66d66",
   "metadata": {},
   "source": [
    "## 1. Introduction to Regularization\n",
    "\n",
    "In machine learning, one of the major challenges is overfitting—when a model learns the training data too well (including noise) and fails to generalize to unseen data. **Regularization** is a strategy to overcome this by adding a penalty term to the loss function. This penalty discourages overly complex models (with very large coefficient values), thereby improving generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Regularized Linear Models  | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2036%20Regularized%20Linear%20Models)\n",
    "\n",
    "Consider the standard linear regression model where we predict \\( y \\) from features \\( x \\) using coefficients \\(\\theta\\):  \n",
    "\n",
    "$$  \n",
    "\\hat{y}_i = \\theta^T x_i  \n",
    "$$  \n",
    "\n",
    "and the cost (loss) function is the mean squared error (MSE):  \n",
    "\n",
    "$$  \n",
    "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\theta^T x_i\\right)^2.  \n",
    "$$  \n",
    "\n",
    "**Regularization** adds a penalty term R(θ) to this loss:  \n",
    "\n",
    "$$  \n",
    "J_{\\text{reg}}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\theta^T x_i\\right)^2 + \\lambda \\, R(\\theta),  \n",
    "$$  \n",
    "\n",
    "where λ≥0  controls the strength of the penalty.  \n",
    "\n",
    "---  \n",
    "\n",
    "## 3. Types of Regularization  \n",
    "\n",
    "### Ridge Regression (L2 Regularization)  \n",
    "\n",
    "- **Penalty term:** $$ \\|\\theta\\|_2^2 = \\sum_{j=1}^p \\theta_j^2 $$  \n",
    "- **Cost function:**  \n",
    "\n",
    "$$  \n",
    "J_{\\text{ridge}}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\theta^T x_i\\right)^2 + \\lambda \\sum_{j=1}^p \\theta_j^2.  \n",
    "$$  \n",
    "\n",
    "- **Effect:** Shrinks coefficients toward zero but rarely exactly zero. It’s especially useful when predictors are highly correlated.  \n",
    "\n",
    "### Lasso Regression (L1 Regularization)  | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2037%20Lasso%20Regression)\n",
    "\n",
    "- **Penalty term:** $$ \\|\\theta\\|_1 = \\sum_{j=1}^p |\\theta_j| $$  \n",
    "- **Cost function:**  \n",
    "\n",
    "$$  \n",
    "J_{\\text{lasso}}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\theta^T x_i\\right)^2 + \\lambda \\sum_{j=1}^p |\\theta_j|.  \n",
    "$$  \n",
    "\n",
    "- **Effect:** Can force some coefficients exactly to zero, thus performing automatic feature selection.  \n",
    "\n",
    "### ElasticNet Regression  | [Link](https://github.com/AdilShamim8/50-Days-of-Machine-Learning/tree/main/Day%2038%20ElasticNet%20Regression)\n",
    "\n",
    "ElasticNet combines both L1 and L2 penalties. Its cost function is:  \n",
    "\n",
    "$$  \n",
    "J_{\\text{EN}}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\theta^T x_i\\right)^2 + \\lambda \\left(\\alpha \\|\\theta\\|_1 + (1-\\alpha)\\|\\theta\\|_2^2\\right),  \n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- λ controls the overall strength of regularization.  \n",
    "- α ∈ [0,1]\\) determines the mix:  \n",
    "  - α = 1 gives pure Lasso.  \n",
    "  - α = 0 gives pure Ridge.  \n",
    "\n",
    "ElasticNet is useful when you want a balance between coefficient shrinkage and variable selection, especially in high-dimensional settings.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Python Code Examples\n",
    "\n",
    "Below is a self-contained example using the Boston Housing dataset (available via scikit-learn) that shows how to apply ordinary linear regression, Ridge, Lasso, and ElasticNet. We also show how to use cross-validation to tune hyperparameters for ElasticNet.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston  # Note: Boston dataset is deprecated in recent versions; alternatives are available.\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ElasticNetCV\n",
    "\n",
    "# Load the dataset\n",
    "boston = load_boston()  \n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Standardize features (regularization is sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Ordinary Linear Regression\n",
    "# ---------------------------\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Linear Regression R^2:\", lr.score(X_test, y_test))\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Ridge Regression (L2)\n",
    "# ---------------------------\n",
    "ridge = Ridge(alpha=1.0)  # Try different alpha values to control penalty strength.\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Ridge Regression R^2:\", ridge.score(X_test, y_test))\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Lasso Regression (L1)\n",
    "# ---------------------------\n",
    "lasso = Lasso(alpha=0.1)  # The alpha here is the penalty term.\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"Lasso Regression R^2:\", lasso.score(X_test, y_test))\n",
    "\n",
    "# ---------------------------\n",
    "# 4. ElasticNet Regression\n",
    "# ---------------------------\n",
    "elastic = ElasticNet(alpha=1.0, l1_ratio=0.5)  # 50-50 mix of L1 and L2\n",
    "elastic.fit(X_train, y_train)\n",
    "print(\"ElasticNet Regression R^2:\", elastic.score(X_test, y_test))\n",
    "\n",
    "# ---------------------------\n",
    "# 5. ElasticNet with Cross-Validation\n",
    "# ---------------------------\n",
    "elastic_cv = ElasticNetCV(\n",
    "    alphas=[0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "    l1_ratio=[0.1, 0.5, 0.9],\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "elastic_cv.fit(X_train, y_train)\n",
    "print(\"ElasticNetCV Best alpha:\", elastic_cv.alpha_)\n",
    "print(\"ElasticNetCV Best l1_ratio:\", elastic_cv.l1_ratio_)\n",
    "print(\"ElasticNetCV R^2:\", elastic_cv.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **StandardScaler:** Regularization penalties are scale-dependent. Standardizing features ensures that each feature contributes equally.\n",
    "- **Ridge vs. Lasso:** Ridge shrinks all coefficients (helpful with multicollinearity), while Lasso may set some coefficients to zero (feature selection).\n",
    "- **ElasticNet:** Combines both effects; use ElasticNetCV to automatically select optimal hyperparameters (both alpha and l1_ratio).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "Regularization techniques are essential for building robust predictive models, especially in high-dimensional spaces. They help balance the bias–variance trade-off by penalizing large coefficients:\n",
    "\n",
    "- **Ridge Regression** (L2) controls coefficient magnitude.\n",
    "- **Lasso Regression** (L1) performs variable selection by enforcing sparsity.\n",
    "- **ElasticNet Regression** provides a tunable balance between L1 and L2 regularization.\n",
    "\n",
    "Using Python’s scikit-learn, you can quickly experiment with these methods and tune hyperparameters using cross-validation, thereby improving your model’s generalization performance.\n",
    "\n",
    "This overview—with formulas and code examples—should give you a strong foundation to explore regularized linear models further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
